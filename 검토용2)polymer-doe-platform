#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§¬ ë²”ìš© ê³ ë¶„ì ì‹¤í—˜ ì„¤ê³„ í”Œë«í¼ (Universal Polymer Design of Experiments Platform)
================================================================================

Enhanced Version 4.0.0
- ì™„ì „í•œ AI-ë°ì´í„°ë² ì´ìŠ¤ í†µí•©
- ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ
- ì´ˆë³´ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤
- 3D ë¶„ì ì‹œê°í™”
- ì‹¤ì‹œê°„ í˜‘ì—… ê¸°ëŠ¥

ê°œë°œ: Polymer DOE Research Team
ë¼ì´ì„ ìŠ¤: MIT
"""

# ==================== í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ====================
import os
import sys
import json
import time
import hashlib
import base64
import io
import re
import logging
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union, Callable, TypeVar, Generic
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from collections import defaultdict, OrderedDict, deque
from functools import lru_cache, wraps, partial
from pathlib import Path
import tempfile
import shutil
import traceback
import pickle
import sqlite3
import threading
import queue
import asyncio
import concurrent.futures
import uuid
import mimetypes
import zipfile
import tarfile
from contextlib import contextmanager
import subprocess
import platform

# ==================== ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ====================
import numpy as np
import pandas as pd
from scipy import stats, optimize, interpolate, signal
from scipy.stats import (
    f_oneway, ttest_ind, shapiro, levene, anderson,
    kruskal, mannwhitneyu, wilcoxon, friedmanchisquare
)
from scipy.optimize import minimize, differential_evolution, dual_annealing
from scipy.spatial.distance import cdist
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA, ICA, NMF
from sklearn.manifold import TSNE, MDS
from sklearn.model_selection import (
    train_test_split, cross_val_score, KFold, 
    GridSearchCV, RandomizedSearchCV, BayesSearchCV
)
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor,
    ExtraTreesRegressor, AdaBoostRegressor
)
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    explained_variance_score, max_error
)
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import (
    RBF, Matern, RationalQuadratic, ExpSineSquared,
    DotProduct, WhiteKernel, ConstantKernel
)
from sklearn.neural_network import MLPRegressor
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
import xgboost as xgb
import lightgbm as lgb

# ==================== ì‹œê°í™” ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.animation import FuncAnimation
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.io as pio
import altair as alt
import holoviews as hv
import bokeh.plotting as bk
from PIL import Image, ImageDraw, ImageFont
import cv2

# ==================== ì›¹ í”„ë ˆì„ì›Œí¬ ====================
import streamlit as st
from streamlit_option_menu import option_menu
from streamlit_drawable_canvas import st_canvas
from streamlit_ace import st_ace
from streamlit_aggrid import AgGrid, GridOptionsBuilder
from streamlit_elements import elements, mui, html
from streamlit_timeline import timeline
from streamlit_folium import folium_static
import streamlit.components.v1 as components

# ==================== 3D ì‹œê°í™” ====================
try:
    import py3Dmol
    from stmol import showmol
    import nglview
    PY3DMOL_AVAILABLE = True
except ImportError:
    PY3DMOL_AVAILABLE = False

# ==================== í™”í•™ ì •ë³´í•™ ====================
try:
    from rdkit import Chem
    from rdkit.Chem import Draw, Descriptors, AllChem
    from rdkit.Chem.Draw import IPythonConsole
    import pubchempy as pcp
    RDKIT_AVAILABLE = True
except ImportError:
    RDKIT_AVAILABLE = False

# ==================== API ë° ì™¸ë¶€ ì„œë¹„ìŠ¤ ====================
import requests
import aiohttp
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import httpx
import websocket
import socketio

# ==================== AI ì„œë¹„ìŠ¤ ====================
# OpenAI
try:
    import openai
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Google AI
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Anthropic Claude
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# ì¶”ê°€ AI ì„œë¹„ìŠ¤ë“¤
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

try:
    from huggingface_hub import InferenceClient, HfApi
    from transformers import pipeline
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False

# ==================== ë°ì´í„°ë² ì´ìŠ¤ ë° ì €ì¥ì†Œ ====================
try:
    import gspread
    from google.oauth2.service_account import Credentials
    from google.auth.transport.requests import Request
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaFileUpload
    GSPREAD_AVAILABLE = True
except ImportError:
    GSPREAD_AVAILABLE = False

try:
    from github import Github
    GITHUB_AVAILABLE = True
except ImportError:
    GITHUB_AVAILABLE = False

try:
    import pymongo
    from motor.motor_asyncio import AsyncIOMotorClient
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

# ==================== ë²ˆì—­ ë° ìì—°ì–´ ì²˜ë¦¬ ====================
try:
    from googletrans import Translator
    import langdetect
    TRANSLATION_AVAILABLE = True
except ImportError:
    TRANSLATION_AVAILABLE = False

try:
    import spacy
    import nltk
    NLP_AVAILABLE = True
except ImportError:
    NLP_AVAILABLE = False

# ==================== ì‹¤í—˜ ì„¤ê³„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ====================
try:
    import pyDOE2
    PYDOE_AVAILABLE = True
except ImportError:
    PYDOE_AVAILABLE = False

try:
    from smt.sampling_methods import LHS
    from smt.surrogate_models import KRG
    SMT_AVAILABLE = True
except ImportError:
    SMT_AVAILABLE = False

# ==================== ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ====================
try:
    import pdfkit
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph
    from reportlab.lib.styles import getSampleStyleSheet
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    import qrcode
    QRCODE_AVAILABLE = True
except ImportError:
    QRCODE_AVAILABLE = False

# ==================== ì„¤ì • ë° ìƒìˆ˜ ====================
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
logging.basicConfig(level=logging.INFO, format=log_format)

# íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
file_handler = logging.FileHandler('polymer_doe.log')
file_handler.setFormatter(logging.Formatter(log_format))
logger = logging.getLogger(__name__)
logger.addHandler(file_handler)

# ë²„ì „ ì •ë³´
VERSION = "4.0.0"
BUILD_DATE = "2024-01-20"
API_VERSION = "v1"

# ì§€ì› ì–¸ì–´ (í™•ì¥)
SUPPORTED_LANGUAGES = {
    'ko': 'í•œêµ­ì–´',
    'en': 'English',
    'ja': 'æ—¥æœ¬èª',
    'zh-cn': 'ç®€ä½“ä¸­æ–‡',
    'zh-tw': 'ç¹é«”ä¸­æ–‡',
    'de': 'Deutsch',
    'fr': 'FranÃ§ais',
    'es': 'EspaÃ±ol',
    'it': 'Italiano',
    'pt': 'PortuguÃªs',
    'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹',
    'ar': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©',
    'hi': 'à¤¹à¤¿à¤¨à¥à¤¦à¥€',
    'th': 'à¹„à¸—à¸¢',
    'vi': 'Tiáº¿ng Viá»‡t'
}

# ì‹¤í—˜ ì„¤ê³„ ë°©ë²• (í™•ì¥)
DESIGN_METHODS = {
    'full_factorial': {
        'name': 'ì™„ì „ ìš”ì¸ ì„¤ê³„ (Full Factorial)',
        'description': 'ëª¨ë“  ì¸ì ì¡°í•©ì„ ì‹œí—˜í•˜ëŠ” ê°€ì¥ ì™„ì „í•œ ì„¤ê³„',
        'pros': ['ì™„ì „í•œ ì •ë³´', 'ëª¨ë“  ìƒí˜¸ì‘ìš© íŒŒì•… ê°€ëŠ¥'],
        'cons': ['ì‹¤í—˜ ìˆ˜ê°€ ë§ìŒ', 'ë¹„ìš©ì´ ë†’ìŒ'],
        'suitable_for': 'ì¸ì ìˆ˜ê°€ ì ì€ ê²½ìš° (2-4ê°œ)',
        'min_factors': 2,
        'max_factors': 5
    },
    'fractional_factorial': {
        'name': 'ë¶€ë¶„ ìš”ì¸ ì„¤ê³„ (Fractional Factorial)',
        'description': 'ì£¼ìš” íš¨ê³¼ì™€ ì¼ë¶€ ìƒí˜¸ì‘ìš©ë§Œ ì¶”ì •í•˜ëŠ” íš¨ìœ¨ì  ì„¤ê³„',
        'pros': ['ì‹¤í—˜ ìˆ˜ ì ˆê°', 'íš¨ìœ¨ì '],
        'cons': ['ì¼ë¶€ ìƒí˜¸ì‘ìš© í˜¼ë™', 'í•´ìƒë„ ì œí•œ'],
        'suitable_for': 'ìŠ¤í¬ë¦¬ë‹ ì‹¤í—˜, ë§ì€ ì¸ì',
        'min_factors': 3,
        'max_factors': 15
    },
    'plackett_burman': {
        'name': 'Plackett-Burman ì„¤ê³„',
        'description': 'ì£¼íš¨ê³¼ë§Œ ì¶”ì •í•˜ëŠ” ìŠ¤í¬ë¦¬ë‹ ì„¤ê³„',
        'pros': ['ë§¤ìš° íš¨ìœ¨ì ', 'ë§ì€ ì¸ì ì²˜ë¦¬ ê°€ëŠ¥'],
        'cons': ['ìƒí˜¸ì‘ìš© ì¶”ì • ë¶ˆê°€', '2ìˆ˜ì¤€ë§Œ ê°€ëŠ¥'],
        'suitable_for': 'ì´ˆê¸° ìŠ¤í¬ë¦¬ë‹',
        'min_factors': 3,
        'max_factors': 47
    },
    'box_behnken': {
        'name': 'Box-Behnken ì„¤ê³„',
        'description': '2ì°¨ ëª¨ë¸ì„ ìœ„í•œ 3ìˆ˜ì¤€ ì„¤ê³„',
        'pros': ['ê·¹ê°’ ì¡°ê±´ íšŒí”¼', 'íš¨ìœ¨ì ì¸ 2ì°¨ ëª¨ë¸'],
        'cons': ['3ê°œ ì´ìƒ ì¸ì í•„ìš”', 'ì •ìœ¡ë©´ì²´ ì˜ì—­ë§Œ'],
        'suitable_for': 'ë°˜ì‘í‘œë©´ ëª¨ë¸ë§',
        'min_factors': 3,
        'max_factors': 7
    },
    'central_composite': {
        'name': 'ì¤‘ì‹¬ í•©ì„± ì„¤ê³„ (CCD)',
        'description': '2ì°¨ ëª¨ë¸ì„ ìœ„í•œ í‘œì¤€ ì„¤ê³„',
        'pros': ['íšŒì „ ê°€ëŠ¥', 'ìˆœì°¨ì  ì‹¤í—˜ ê°€ëŠ¥'],
        'cons': ['ì¶•ì ì´ ë²”ìœ„ ë°–ì¼ ìˆ˜ ìˆìŒ'],
        'suitable_for': 'ìµœì í™” ì‹¤í—˜',
        'min_factors': 2,
        'max_factors': 6
    },
    'latin_hypercube': {
        'name': 'ë¼í‹´ í•˜ì´í¼íë¸Œ ìƒ˜í”Œë§',
        'description': 'ê³µê°„ ì¶©ì§„ ì„¤ê³„',
        'pros': ['ê· ë“±í•œ ê³µê°„ íƒìƒ‰', 'ëª¨ë¸ ë¬´ê´€'],
        'cons': ['í†µê³„ì  íŠ¹ì„± ë¶€ì¡±'],
        'suitable_for': 'ì»´í“¨í„° ì‹¤í—˜, ì‹œë®¬ë ˆì´ì…˜',
        'min_factors': 1,
        'max_factors': 100
    },
    'taguchi': {
        'name': 'ë‹¤êµ¬ì¹˜ ì„¤ê³„',
        'description': 'ê°•ê±´ ì„¤ê³„ë¥¼ ìœ„í•œ ì§êµ ë°°ì—´',
        'pros': ['ì¡ìŒ ì¸ì ê³ ë ¤', 'ê°•ê±´ì„±'],
        'cons': ['ìƒí˜¸ì‘ìš© ì œí•œì '],
        'suitable_for': 'í’ˆì§ˆ ê°œì„ , ê°•ê±´ ì„¤ê³„',
        'min_factors': 2,
        'max_factors': 50
    },
    'mixture': {
        'name': 'í˜¼í•©ë¬¼ ì„¤ê³„',
        'description': 'ì„±ë¶„ í•©ì´ ì¼ì •í•œ ì‹¤í—˜',
        'pros': ['í˜¼í•©ë¬¼ íŠ¹í™”', 'ì œì•½ ì¡°ê±´ ì²˜ë¦¬'],
        'cons': ['íŠ¹ìˆ˜í•œ ë¶„ì„ í•„ìš”'],
        'suitable_for': 'ì¡°ì„± ìµœì í™”',
        'min_factors': 3,
        'max_factors': 10
    },
    'optimal': {
        'name': 'ìµœì  ì„¤ê³„ (D-optimal)',
        'description': 'ì •ë³´ëŸ‰ì„ ìµœëŒ€í™”í•˜ëŠ” ì„¤ê³„',
        'pros': ['ìœ ì—°í•œ ì„¤ê³„', 'ì œì•½ ì¡°ê±´ ì²˜ë¦¬'],
        'cons': ['ê³„ì‚° ì§‘ì•½ì '],
        'suitable_for': 'ë¹„í‘œì¤€ ìƒí™©',
        'min_factors': 1,
        'max_factors': 20
    },
    'adaptive': {
        'name': 'ì ì‘í˜• ì„¤ê³„',
        'description': 'ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ì‹¤í—˜ì  ê²°ì •',
        'pros': ['íš¨ìœ¨ì  íƒìƒ‰', 'ì‹¤ì‹œê°„ ìµœì í™”'],
        'cons': ['ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜'],
        'suitable_for': 'ê³ ê°€ ì‹¤í—˜, ì‹¤ì‹œê°„ ìµœì í™”',
        'min_factors': 1,
        'max_factors': 10
    },
    'sequential': {
        'name': 'ìˆœì°¨ì  ì„¤ê³„',
        'description': 'ë‹¨ê³„ë³„ë¡œ ì •ë°€ë„ë¥¼ ë†’ì´ëŠ” ì„¤ê³„',
        'pros': ['ë¦¬ìŠ¤í¬ ê°ì†Œ', 'ë‹¨ê³„ì  ê°œì„ '],
        'cons': ['ì‹œê°„ ì†Œìš”'],
        'suitable_for': 'ì¥ê¸° í”„ë¡œì íŠ¸',
        'min_factors': 1,
        'max_factors': 20
    },
    'space_filling': {
        'name': 'ê³µê°„ ì¶©ì§„ ì„¤ê³„',
        'description': 'ì‹¤í—˜ ì˜ì—­ì„ ê· ë“±í•˜ê²Œ ì»¤ë²„',
        'pros': ['ëª¨ë¸ ë…ë¦½ì ', 'íƒìƒ‰ì '],
        'cons': ['í†µê³„ì  ìµœì ì„± ë¶€ì¡±'],
        'suitable_for': 'ë¯¸ì§€ ì‹œìŠ¤í…œ íƒìƒ‰',
        'min_factors': 1,
        'max_factors': 50
    },
    'bayesian': {
        'name': 'ë² ì´ì§€ì•ˆ ìµœì í™”',
        'description': 'í™•ë¥  ëª¨ë¸ ê¸°ë°˜ ìˆœì°¨ì  ì„¤ê³„',
        'pros': ['ë§¤ìš° íš¨ìœ¨ì ', 'ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”'],
        'cons': ['ê³„ì‚° ë³µì¡', 'ì´ˆê¸° ë°ì´í„° í•„ìš”'],
        'suitable_for': 'ê³ ë¹„ìš© ì‹¤í—˜',
        'min_factors': 1,
        'max_factors': 20
    }
}

# ê³ ë¶„ì ìœ í˜• (í™•ì¥ ë° ìœ ì—°í™”)
POLYMER_CATEGORIES = {
    'base_types': {
        'thermoplastic': {
            'name': 'ì—´ê°€ì†Œì„± ê³ ë¶„ì',
            'description': 'ê°€ì—´ ì‹œ ì—°í™”ë˜ê³  ëƒ‰ê° ì‹œ ê²½í™”ë˜ëŠ” ê³ ë¶„ì',
            'subcategories': ['ë²”ìš©', 'ì—”ì§€ë‹ˆì–´ë§', 'ìŠˆí¼ ì—”ì§€ë‹ˆì–´ë§'],
            'examples': ['PE', 'PP', 'PS', 'PVC', 'PET', 'PA', 'PC', 'PMMA', 'POM', 'PEEK', 'PPS', 'PSU'],
            'typical_properties': ['ë…¹ëŠ”ì ', 'ìœ ë¦¬ì „ì´ì˜¨ë„', 'ìš©ìœµì§€ìˆ˜', 'ì¸ì¥ê°•ë„', 'ì‹ ìœ¨', 'ì¶©ê²©ê°•ë„', 'ê²½ë„'],
            'processing_methods': ['ì‚¬ì¶œì„±í˜•', 'ì••ì¶œ', 'ë¸”ë¡œìš°ì„±í˜•', 'ì—´ì„±í˜•', '3Dí”„ë¦°íŒ…']
        },
        'thermosetting': {
            'name': 'ì—´ê²½í™”ì„± ê³ ë¶„ì',
            'description': 'ê°€ì—´ ì‹œ í™”í•™ë°˜ì‘ìœ¼ë¡œ ê²½í™”ë˜ëŠ” ê³ ë¶„ì',
            'subcategories': ['ì—í­ì‹œ', 'í´ë¦¬ì—ìŠ¤í„°', 'í˜ë†€', 'í´ë¦¬ìš°ë ˆíƒ„'],
            'examples': ['Epoxy', 'UP', 'VE', 'Phenolic', 'PU', 'Silicone', 'BMI', 'PI'],
            'typical_properties': ['ê²½í™”ì‹œê°„', 'ê²½í™”ì˜¨ë„', 'ê°€êµë°€ë„', 'ê²½ë„', 'ë‚´ì—´ì„±', 'ì ‘ì°©ê°•ë„', 'ìˆ˜ì¶•ë¥ '],
            'processing_methods': ['RTM', 'SMC', 'BMC', 'í•¸ë“œë ˆì´ì—…', 'í•„ë¼ë©˜íŠ¸ì™€ì¸ë”©', 'ì˜¤í† í´ë ˆì´ë¸Œ']
        },
        'elastomer': {
            'name': 'íƒ„ì„±ì²´',
            'description': 'ê³ ë¬´ì™€ ê°™ì€ íƒ„ì„±ì„ ê°€ì§„ ê³ ë¶„ì',
            'subcategories': ['ì²œì—°ê³ ë¬´', 'í•©ì„±ê³ ë¬´', 'ì—´ê°€ì†Œì„± íƒ„ì„±ì²´'],
            'examples': ['NR', 'SBR', 'NBR', 'EPDM', 'Silicone', 'TPE', 'TPU', 'TPV', 'FKM'],
            'typical_properties': ['ê²½ë„', 'ì¸ì¥ê°•ë„', 'ì‹ ìœ¨', 'ë°˜ë°œíƒ„ì„±', 'ì••ì¶•ì˜êµ¬ë³€í˜•', 'ì¸ì—´ê°•ë„', 'ë‚´ë§ˆëª¨ì„±'],
            'processing_methods': ['ì»´íŒŒìš´ë”©', 'ìº˜ë¦°ë”ë§', 'ì••ì¶œ', 'ì‚¬ì¶œì„±í˜•', 'ê°€í™©']
        },
        'biopolymer': {
            'name': 'ë°”ì´ì˜¤ ê³ ë¶„ì',
            'description': 'ìƒë¬¼ ìœ ë˜ ë˜ëŠ” ìƒë¶„í•´ì„± ê³ ë¶„ì',
            'subcategories': ['ì²œì—° ê³ ë¶„ì', 'ë°”ì´ì˜¤ ê¸°ë°˜', 'ìƒë¶„í•´ì„±'],
            'examples': ['PLA', 'PHA', 'PBS', 'Starch', 'Cellulose', 'Chitosan', 'Alginate', 'Collagen'],
            'typical_properties': ['ìƒë¶„í•´ì„±', 'ìƒì²´ì í•©ì„±', 'ê¸°ê³„ì ê°•ë„', 'ê°€ê³µì„±', 'ì•ˆì •ì„±', 'ìˆ˜ë¶„í¡ìˆ˜ìœ¨', 'ê²°ì •í™”ë„'],
            'processing_methods': ['ìš©ì•¡ìºìŠ¤íŒ…', 'ì „ê¸°ë°©ì‚¬', '3Dë°”ì´ì˜¤í”„ë¦°íŒ…', 'ì••ì¶œ', 'ì‚¬ì¶œì„±í˜•']
        },
        'conducting': {
            'name': 'ì „ë„ì„± ê³ ë¶„ì',
            'description': 'ì „ê¸° ì „ë„ì„±ì„ ê°€ì§„ íŠ¹ìˆ˜ ê³ ë¶„ì',
            'subcategories': ['ë³¸ì§ˆì „ë„ì„±', 'ë³µí•©ì „ë„ì„±'],
            'examples': ['PANI', 'PPy', 'PEDOT', 'PTh', 'PAc', 'P3HT', 'Graphene composite'],
            'typical_properties': ['ì „ê¸°ì „ë„ë„', 'ë„í•‘ë ˆë²¨', 'ì•ˆì •ì„±', 'ê°€ê³µì„±', 'ê´‘í•™íŠ¹ì„±', 'ìºë¦¬ì–´ ì´ë™ë„', 'ì¼í•¨ìˆ˜'],
            'processing_methods': ['ì „ê¸°ì¤‘í•©', 'í™”í•™ì¤‘í•©', 'ìŠ¤í•€ì½”íŒ…', 'ì‰í¬ì ¯í”„ë¦°íŒ…', 'ì¦ì°©']
        },
        'composite': {
            'name': 'ë³µí•©ì¬ë£Œ',
            'description': 'ê°•í™”ì¬ì™€ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ êµ¬ì„±ëœ ì¬ë£Œ',
            'subcategories': ['ì„¬ìœ ê°•í™”', 'ì…ìê°•í™”', 'ë‚˜ë…¸ë³µí•©ì¬'],
            'examples': ['CFRP', 'GFRP', 'AFRP', 'CNT composite', 'Graphene composite', 'Clay nanocomposite'],
            'typical_properties': ['ì¸ì¥ê°•ë„', 'êµ´ê³¡ê°•ë„', 'ì¶©ê²©ê°•ë„', 'ê³„ë©´ì ‘ì°©ë ¥', 'ë¶„ì‚°ë„', 'ì„¬ìœ í•¨ëŸ‰', 'ê³µê·¹ë¥ '],
            'processing_methods': ['í”„ë¦¬í”„ë ˆê·¸', 'RTM', 'VARTM', 'í•„ë¼ë©˜íŠ¸ì™€ì¸ë”©', 'AFP', '3Dí”„ë¦°íŒ…']
        },
        'inorganic': {
            'name': 'ë¬´ê¸° ê³ ë¶„ì',
            'description': 'íƒ„ì†Œ ëŒ€ì‹  ë‹¤ë¥¸ ì›ì†Œê°€ ì£¼ì‚¬ìŠ¬ì„ ì´ë£¨ëŠ” ê³ ë¶„ì',
            'subcategories': ['ì‹¤ë¦¬ì½˜ê³„', 'ì¸ê³„', 'ë¶•ì†Œê³„'],
            'examples': ['Silicone', 'Phosphazene', 'Polysilane', 'Polysiloxane', 'Polyphosphate', 'Sol-gel'],
            'typical_properties': ['ë‚´ì—´ì„±', 'í™”í•™ì ì•ˆì •ì„±', 'ê¸°ê³„ì íŠ¹ì„±', 'ê´‘í•™íŠ¹ì„±', 'ìœ ì „íŠ¹ì„±', 'ì—´íŒ½ì°½ê³„ìˆ˜'],
            'processing_methods': ['ì¡¸ê²”', 'CVD', 'ìŠ¤í•€ì½”íŒ…', 'ë”¥ì½”íŒ…', 'ìŠ¤í”„ë ˆì´']
        }
    },
    'special_types': {
        'smart': {
            'name': 'ìŠ¤ë§ˆíŠ¸ ê³ ë¶„ì',
            'description': 'ì™¸ë¶€ ìê·¹ì— ë°˜ì‘í•˜ëŠ” ê³ ë¶„ì',
            'types': ['í˜•ìƒê¸°ì–µ', 'ìê°€ì¹˜ìœ ', 'ìê·¹ì‘ë‹µì„±'],
            'stimuli': ['ì˜¨ë„', 'pH', 'ë¹›', 'ì „ê¸°', 'ìê¸°ì¥']
        },
        'functional': {
            'name': 'ê¸°ëŠ¥ì„± ê³ ë¶„ì',
            'description': 'íŠ¹ìˆ˜ ê¸°ëŠ¥ì„ ê°€ì§„ ê³ ë¶„ì',
            'types': ['ì˜ë£Œìš©', 'ê´‘í•™ìš©', 'ì „ìì¬ë£Œìš©', 'ë¶„ë¦¬ë§‰ìš©']
        }
    }
}

# API ìƒíƒœ
class APIStatus(Enum):
    """API ì—°ê²° ìƒíƒœ"""
    ONLINE = "ğŸŸ¢ ì˜¨ë¼ì¸"
    OFFLINE = "ğŸ”´ ì˜¤í”„ë¼ì¸"
    ERROR = "âš ï¸ ì˜¤ë¥˜"
    RATE_LIMITED = "â±ï¸ ì†ë„ ì œí•œ"
    UNAUTHORIZED = "ğŸ” ì¸ì¦ í•„ìš”"
    MAINTENANCE = "ğŸ”§ ìœ ì§€ë³´ìˆ˜"

# ì‚¬ìš©ì ë ˆë²¨
class UserLevel(Enum):
    """ì‚¬ìš©ì ìˆ™ë ¨ë„"""
    BEGINNER = (1, "ğŸŒ± ì´ˆë³´ì", "ìƒì„¸í•œ ì„¤ëª…ê³¼ ê°€ì´ë“œ ì œê³µ", 0.9)
    INTERMEDIATE = (2, "ğŸŒ¿ ì¤‘ê¸‰ì", "ì„ íƒì§€ì™€ ê¶Œì¥ì‚¬í•­ ì œê³µ", 0.7)
    ADVANCED = (3, "ğŸŒ³ ê³ ê¸‰ì", "ììœ ë¡œìš´ ì„¤ì •ê³¼ ê³ ê¸‰ ê¸°ëŠ¥", 0.3)
    EXPERT = (4, "ğŸ“ ì „ë¬¸ê°€", "ì™„ì „í•œ ì œì–´ì™€ ì»¤ìŠ¤í„°ë§ˆì´ì§•", 0.1)
    
    def __init__(self, level, icon, description, help_ratio):
        self.level = level
        self.icon = icon
        self.description = description
        self.help_ratio = help_ratio  # ë„ì›€ë§ í‘œì‹œ ë¹„ìœ¨

# ì‹¤í—˜ ìƒíƒœ
class ExperimentStatus(Enum):
    """ì‹¤í—˜ ì§„í–‰ ìƒíƒœ"""
    PLANNED = "ğŸ“‹ ê³„íšë¨"
    IN_PROGRESS = "ğŸ”¬ ì§„í–‰ì¤‘"
    COMPLETED = "âœ… ì™„ë£Œ"
    FAILED = "âŒ ì‹¤íŒ¨"
    PAUSED = "â¸ï¸ ì¼ì‹œì •ì§€"
    CANCELLED = "ğŸš« ì·¨ì†Œë¨"

# ë¶„ì„ ìœ í˜•
class AnalysisType(Enum):
    """í†µê³„ ë¶„ì„ ìœ í˜•"""
    DESCRIPTIVE = "ê¸°ìˆ í†µê³„"
    ANOVA = "ë¶„ì‚°ë¶„ì„"
    REGRESSION = "íšŒê·€ë¶„ì„"
    RSM = "ë°˜ì‘í‘œë©´ë¶„ì„"
    OPTIMIZATION = "ìµœì í™”"
    PCA = "ì£¼ì„±ë¶„ë¶„ì„"
    CORRELATION = "ìƒê´€ë¶„ì„"
    TIME_SERIES = "ì‹œê³„ì—´ë¶„ì„"
    MACHINE_LEARNING = "ê¸°ê³„í•™ìŠµ"

# ==================== íƒ€ì… ì •ì˜ ====================
T = TypeVar('T')
FactorType = Union[float, int, str, bool]
ResponseType = Union[float, int]

# ==================== ë°ì´í„° í´ë˜ìŠ¤ ====================
@dataclass
class ExperimentFactor:
    """ì‹¤í—˜ ì¸ì ì •ì˜ (í™•ì¥íŒ)"""
    name: str
    unit: str = ""
    min_value: float = 0.0
    max_value: float = 100.0
    levels: List[float] = field(default_factory=list)
    categorical: bool = False
    categories: List[str] = field(default_factory=list)
    description: str = ""
    constraints: List[str] = field(default_factory=list)
    importance: float = 1.0  # ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
    cost: float = 1.0  # ë¹„ìš© ê°€ì¤‘ì¹˜
    difficulty: float = 1.0  # ì‹¤í—˜ ë‚œì´ë„
    tolerance: float = 0.01  # í—ˆìš© ì˜¤ì°¨
    controllable: bool = True  # ì œì–´ ê°€ëŠ¥ ì—¬ë¶€
    noise_factor: bool = False  # ì¡ìŒ ì¸ì ì—¬ë¶€
    transformation: Optional[str] = None  # ë³€í™˜ í•¨ìˆ˜ (log, sqrt ë“±)
    
    def validate(self) -> Tuple[bool, List[str]]:
        """ì¸ì ìœ íš¨ì„± ê²€ì¦"""
        errors = []
        
        if not self.name:
            errors.append("ì¸ì ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if not self.categorical:
            if self.min_value >= self.max_value:
                errors.append(f"{self.name}: ìµœì†Œê°’ì´ ìµœëŒ€ê°’ë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.")
            
            if self.levels:
                for level in self.levels:
                    if level < self.min_value or level > self.max_value:
                        errors.append(f"{self.name}: ìˆ˜ì¤€ {level}ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
        else:
            if not self.categories:
                errors.append(f"{self.name}: ë²”ì£¼í˜• ì¸ìëŠ” ì¹´í…Œê³ ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return len(errors) == 0, errors
    
    def get_levels(self, n_levels: int = None) -> List[FactorType]:
        """ìˆ˜ì¤€ ëª©ë¡ ë°˜í™˜"""
        if self.categorical:
            return self.categories[:n_levels] if n_levels else self.categories
        
        if self.levels:
            return self.levels[:n_levels] if n_levels else self.levels
        
        # ìˆ˜ì¤€ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ìƒì„±
        if n_levels:
            if n_levels == 2:
                return [self.min_value, self.max_value]
            else:
                return np.linspace(self.min_value, self.max_value, n_levels).tolist()
        
        return [self.min_value, self.max_value]
    
    def apply_transformation(self, value: float) -> float:
        """ë³€í™˜ ì ìš©"""
        if not self.transformation or self.categorical:
            return value
        
        if self.transformation == 'log':
            return np.log(value) if value > 0 else 0
        elif self.transformation == 'sqrt':
            return np.sqrt(value) if value >= 0 else 0
        elif self.transformation == 'inverse':
            return 1 / value if value != 0 else float('inf')
        elif self.transformation == 'square':
            return value ** 2
        
        return value

@dataclass
class ExperimentResponse:
    """ë°˜ì‘ ë³€ìˆ˜ ì •ì˜ (í™•ì¥íŒ)"""
    name: str
    unit: str = ""
    target_value: Optional[float] = None
    minimize: bool = False
    maximize: bool = False
    weight: float = 1.0
    specification_limits: Tuple[Optional[float], Optional[float]] = (None, None)
    transformation: Optional[str] = None
    measurement_error: float = 0.0  # ì¸¡ì • ì˜¤ì°¨
    cost_per_measurement: float = 0.0  # ì¸¡ì • ë¹„ìš©
    measurement_time: float = 0.0  # ì¸¡ì • ì‹œê°„ (ë¶„)
    critical: bool = False  # í•µì‹¬ ë°˜ì‘ ì—¬ë¶€
    
    def is_within_spec(self, value: float) -> bool:
        """ê·œê²© ë‚´ ì—¬ë¶€ í™•ì¸"""
        lower, upper = self.specification_limits
        
        if lower is not None and value < lower:
            return False
        if upper is not None and value > upper:
            return False
        
        return True
    
    def calculate_desirability(self, value: float) -> float:
        """ë°”ëŒì§í•¨ ì§€ìˆ˜ ê³„ì‚° (0-1)"""
        if self.target_value is not None:
            # ëª©í‘œê°’ì— ê°€ê¹Œìš¸ìˆ˜ë¡ 1
            deviation = abs(value - self.target_value)
            return np.exp(-deviation / abs(self.target_value))
        
        elif self.minimize:
            lower, upper = self.specification_limits
            if lower is None:
                return 0.5  # ì •ë³´ ë¶€ì¡±
            
            if value <= lower:
                return 1.0
            elif upper is not None and value >= upper:
                return 0.0
            else:
                return (upper - value) / (upper - lower) if upper else 0.5
        
        elif self.maximize:
            lower, upper = self.specification_limits
            if upper is None:
                return 0.5  # ì •ë³´ ë¶€ì¡±
            
            if value >= upper:
                return 1.0
            elif lower is not None and value <= lower:
                return 0.0
            else:
                return (value - lower) / (upper - lower) if lower else 0.5
        
        return 0.5  # ê¸°ë³¸ê°’

@dataclass
class ProjectInfo:
    """í”„ë¡œì íŠ¸ ì •ë³´ (í™•ì¥íŒ)"""
    id: str
    name: str
    description: str
    polymer_type: str
    polymer_system: Dict[str, Any]
    objectives: List[str]
    constraints: List[str]
    created_at: datetime
    updated_at: datetime
    owner: str
    collaborators: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    version: int = 1
    parent_project_id: Optional[str] = None
    status: str = "active"
    budget: Optional[float] = None
    deadline: Optional[datetime] = None
    notes: str = ""
    attachments: List[str] = field(default_factory=list)
    custom_fields: Dict[str, Any] = field(default_factory=dict)
    
    def add_collaborator(self, user_id: str):
        """í˜‘ì—…ì ì¶”ê°€"""
        if user_id not in self.collaborators:
            self.collaborators.append(user_id)
            self.updated_at = datetime.now()
    
    def add_tag(self, tag: str):
        """íƒœê·¸ ì¶”ê°€"""
        if tag not in self.tags:
            self.tags.append(tag)
            self.updated_at = datetime.now()
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

@dataclass
class ExperimentData:
    """ì‹¤í—˜ ë°ì´í„° (í™•ì¥íŒ)"""
    id: str
    project_id: str
    design_matrix: pd.DataFrame
    results: Optional[pd.DataFrame] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    status: ExperimentStatus = ExperimentStatus.PLANNED
    run_order: Optional[List[int]] = None
    block_structure: Optional[Dict[str, List[int]]] = None
    replicates: Dict[int, List[int]] = field(default_factory=dict)
    conditions: Dict[str, Any] = field(default_factory=dict)
    operator: Optional[str] = None
    equipment: Optional[str] = None
    notes: Dict[int, str] = field(default_factory=dict)
    
    def get_completed_runs(self) -> List[int]:
        """ì™„ë£Œëœ ì‹¤í—˜ ë²ˆí˜¸ ëª©ë¡"""
        if self.results is None:
            return []
        
        return self.results[~self.results.isnull().any(axis=1)].index.tolist()
    
    def get_progress(self) -> float:
        """ì§„í–‰ë¥  ê³„ì‚° (0-100%)"""
        total_runs = len(self.design_matrix)
        completed_runs = len(self.get_completed_runs())
        
        return (completed_runs / total_runs * 100) if total_runs > 0 else 0

@dataclass
class AIResponse:
    """AI ì‘ë‹µ ë°ì´í„°"""
    success: bool
    content: str
    model: str
    tokens_used: int = 0
    response_time: float = 0.0
    confidence: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None

@dataclass
class LearningRecord:
    """í•™ìŠµ ê¸°ë¡ ë°ì´í„°"""
    id: str
    timestamp: datetime
    user_id: str
    user_level: UserLevel
    action_type: str
    action_details: Dict[str, Any]
    outcome: Dict[str, Any]
    quality_score: float
    context: Dict[str, Any] = field(default_factory=dict)
    feedback: Optional[str] = None

# ==================== ì˜ˆì™¸ ì²˜ë¦¬ ====================
class PolymerDOEException(Exception):
    """í”Œë«í¼ ê¸°ë³¸ ì˜ˆì™¸"""
    def __init__(self, message: str, error_code: str = None, details: Dict[str, Any] = None):
        super().__init__(message)
        self.error_code = error_code
        self.details = details or {}
        self.timestamp = datetime.now()
        
        # ë¡œê¹…
        logger.error(f"Exception: {error_code} - {message}", extra=self.details)

class APIException(PolymerDOEException):
    """API ê´€ë ¨ ì˜ˆì™¸"""
    pass

class ValidationException(PolymerDOEException):
    """ê²€ì¦ ì‹¤íŒ¨ ì˜ˆì™¸"""
    pass

class DataException(PolymerDOEException):
    """ë°ì´í„° ê´€ë ¨ ì˜ˆì™¸"""
    pass

class DesignException(PolymerDOEException):
    """ì‹¤í—˜ ì„¤ê³„ ê´€ë ¨ ì˜ˆì™¸"""
    pass

class AnalysisException(PolymerDOEException):
    """ë¶„ì„ ê´€ë ¨ ì˜ˆì™¸"""
    pass

# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================
def timeit(func: Callable) -> Callable:
    """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        start = time.time()
        result = await func(*args, **kwargs)
        end = time.time()
        logger.debug(f"{func.__name__} ì‹¤í–‰ ì‹œê°„: {end - start:.3f}ì´ˆ")
        return result
    
    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        logger.debug(f"{func.__name__} ì‹¤í–‰ ì‹œê°„: {end - start:.3f}ì´ˆ")
        return result
    
    if asyncio.iscoroutinefunction(func):
        return async_wrapper
    else:
        return sync_wrapper

def retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0):
    """ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            attempt = 0
            current_delay = delay
            
            while attempt < max_attempts:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    attempt += 1
                    if attempt >= max_attempts:
                        logger.error(f"{func.__name__} ì‹¤íŒ¨ (ì‹œë„ {attempt}/{max_attempts}): {e}")
                        raise
                    
                    logger.warning(f"{func.__name__} ì¬ì‹œë„ {attempt}/{max_attempts} ({current_delay}ì´ˆ ëŒ€ê¸°)")
                    await asyncio.sleep(current_delay)
                    current_delay *= backoff
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            attempt = 0
            current_delay = delay
            
            while attempt < max_attempts:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    attempt += 1
                    if attempt >= max_attempts:
                        logger.error(f"{func.__name__} ì‹¤íŒ¨ (ì‹œë„ {attempt}/{max_attempts}): {e}")
                        raise
                    
                    logger.warning(f"{func.__name__} ì¬ì‹œë„ {attempt}/{max_attempts} ({current_delay}ì´ˆ ëŒ€ê¸°)")
                    time.sleep(current_delay)
                    current_delay *= backoff
        
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

def validate_input(value: Any, 
                  min_val: float = None, 
                  max_val: float = None,
                  allowed_values: List = None,
                  value_type: type = None) -> Tuple[bool, Optional[str]]:
    """ì…ë ¥ê°’ ê²€ì¦"""
    if value is None:
        return False, "ê°’ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    if value_type and not isinstance(value, value_type):
        return False, f"íƒ€ì…ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. {value_type.__name__}ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    
    if min_val is not None and value < min_val:
        return False, f"ìµœì†Œê°’ {min_val} ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    
    if max_val is not None and value > max_val:
        return False, f"ìµœëŒ€ê°’ {max_val} ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤."
    
    if allowed_values is not None and value not in allowed_values:
        return False, f"í—ˆìš©ëœ ê°’ì´ ì•„ë‹™ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {allowed_values}"
    
    return True, None

def generate_unique_id(prefix: str = "EXP") -> str:
    """ê³ ìœ  ID ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    random_part = hashlib.md5(f"{timestamp}{uuid.uuid4()}".encode()).hexdigest()[:6]
    return f"{prefix}_{timestamp}_{random_part}"

def safe_float_conversion(value: Any, default: float = 0.0) -> float:
    """ì•ˆì „í•œ float ë³€í™˜"""
    if value is None:
        return default
    
    try:
        # ë¬¸ìì—´ì¸ ê²½ìš° ì‰¼í‘œ ì œê±°
        if isinstance(value, str):
            value = value.replace(',', '')
        
        return float(value)
    except (ValueError, TypeError):
        logger.warning(f"Float ë³€í™˜ ì‹¤íŒ¨: {value}")
        return default

def format_number(value: float, 
                 decimals: int = 2, 
                 use_scientific: bool = True,
                 threshold: float = 1e6) -> str:
    """ìˆ«ì í¬ë§·íŒ…"""
    if pd.isna(value):
        return "N/A"
    
    if use_scientific and (abs(value) >= threshold or (abs(value) < 1e-3 and value != 0)):
        return f"{value:.{decimals}e}"
    else:
        return f"{value:,.{decimals}f}"

def sanitize_filename(filename: str) -> str:
    """íŒŒì¼ëª… ì •ë¦¬"""
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        filename = filename.replace(char, '_')
    
    # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ
    filename = filename.replace(' ', '_')
    
    # ê¸¸ì´ ì œí•œ
    max_length = 255
    if len(filename) > max_length:
        name, ext = os.path.splitext(filename)
        filename = name[:max_length - len(ext)] + ext
    
    return filename

@lru_cache(maxsize=128)
def calculate_hash(data: str) -> str:
    """ë°ì´í„° í•´ì‹œ ê³„ì‚° (ìºì‹±)"""
    return hashlib.sha256(data.encode()).hexdigest()

def create_backup(data: Any, backup_dir: str = "backups") -> str:
    """ë°ì´í„° ë°±ì—… ìƒì„±"""
    os.makedirs(backup_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = os.path.join(backup_dir, f"backup_{timestamp}.pkl")
    
    with open(backup_file, 'wb') as f:
        pickle.dump(data, f)
    
    logger.info(f"ë°±ì—… ìƒì„±: {backup_file}")
    return backup_file

def restore_backup(backup_file: str) -> Any:
    """ë°±ì—… ë³µì›"""
    if not os.path.exists(backup_file):
        raise FileNotFoundError(f"ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {backup_file}")
    
    with open(backup_file, 'rb') as f:
        data = pickle.load(f)
    
    logger.info(f"ë°±ì—… ë³µì›: {backup_file}")
    return data

class ProgressTracker:
    """ì§„í–‰ ìƒí™© ì¶”ì ê¸°"""
    def __init__(self, total: int, description: str = "Processing"):
        self.total = total
        self.current = 0
        self.description = description
        self.start_time = time.time()
        self.last_update = self.start_time
        
    def update(self, increment: int = 1):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        self.current += increment
        current_time = time.time()
        
        # 0.5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
        if current_time - self.last_update > 0.5:
            self.last_update = current_time
            self._display_progress()
    
    def _display_progress(self):
        """ì§„í–‰ë¥  í‘œì‹œ"""
        if self.total == 0:
            return
        
        progress = self.current / self.total
        elapsed = time.time() - self.start_time
        eta = elapsed / progress - elapsed if progress > 0 else 0
        
        bar_length = 30
        filled_length = int(bar_length * progress)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        st.progress(progress)
        st.text(f"{self.description}: {bar} {progress*100:.1f}% (ETA: {eta:.1f}s)")
    
    def finish(self):
        """ì™„ë£Œ"""
        self.current = self.total
        self._display_progress()
        elapsed = time.time() - self.start_time
        st.success(f"{self.description} ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")

# ==================== ì´ˆë³´ìë¥¼ ìœ„í•œ ë„ì›€ë§ ì‹œìŠ¤í…œ ====================
class HelpSystem:
    """ìƒí™©ë³„ ë„ì›€ë§ ì œê³µ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.help_database = {
            'factor_selection': {
                'title': 'ğŸ¯ ì‹¤í—˜ ì¸ìë€?',
                'basic': """
                ì‹¤í—˜ ì¸ì(Factor)ëŠ” ì‹¤í—˜ ê²°ê³¼ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ë³€ìˆ˜ì…ë‹ˆë‹¤.
                
                ì˜ˆì‹œ:
                - ğŸŒ¡ï¸ **ì˜¨ë„**: ë°˜ì‘ ì†ë„ì— ì˜í–¥
                - â±ï¸ **ì‹œê°„**: ë°˜ì‘ ì™„ì„±ë„ì— ì˜í–¥
                - ğŸ§ª **ë†ë„**: ìƒì„±ë¬¼ ì–‘ì— ì˜í–¥
                """,
                'detailed': """
                ### ì¸ì ì„ íƒ ì‹œ ê³ ë ¤ì‚¬í•­
                
                1. **ì œì–´ ê°€ëŠ¥ì„±**: ì‹¤í—˜ ì¤‘ ì •í™•íˆ ì¡°ì ˆí•  ìˆ˜ ìˆë‚˜ìš”?
                2. **ì¸¡ì • ê°€ëŠ¥ì„±**: ì •í™•íˆ ì¸¡ì •í•  ìˆ˜ ìˆë‚˜ìš”?
                3. **ì˜í–¥ë ¥**: ê²°ê³¼ì— ì‹¤ì œë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ë‚˜ìš”?
                4. **ë…ë¦½ì„±**: ë‹¤ë¥¸ ì¸ìì™€ ë…ë¦½ì ì¸ê°€ìš”?
                
                ğŸ’¡ **íŒ**: ì²˜ìŒì—ëŠ” 2-3ê°œì˜ í•µì‹¬ ì¸ìë¡œ ì‹œì‘í•˜ì„¸ìš”!
                """,
                'examples': [
                    "ê³ ë¶„ì í•©ì„±: ì˜¨ë„, ì‹œê°„, ì´‰ë§¤ ë†ë„",
                    "ë³µí•©ì¬ë£Œ: ì„¬ìœ  í•¨ëŸ‰, ê²½í™” ì˜¨ë„, ì••ë ¥",
                    "ì½”íŒ…: ë‘ê»˜, ê±´ì¡° ì‹œê°„, ìš©ë§¤ ë¹„ìœ¨"
                ]
            },
            'design_method': {
                'title': 'ğŸ“Š ì‹¤í—˜ ì„¤ê³„ ë°©ë²• ì„ íƒ',
                'basic': """
                ì‹¤í—˜ ì„¤ê³„ëŠ” ì–´ë–¤ ì¡°ê±´ì—ì„œ ì‹¤í—˜í• ì§€ ì •í•˜ëŠ” ê³„íšì…ë‹ˆë‹¤.
                
                ì£¼ìš” ë°©ë²•:
                - **ì™„ì „ ìš”ì¸**: ëª¨ë“  ì¡°í•© (ì •í™•í•˜ì§€ë§Œ ì‹¤í—˜ ë§ìŒ)
                - **ë¶€ë¶„ ìš”ì¸**: ì¼ë¶€ ì¡°í•© (íš¨ìœ¨ì )
                - **ë°˜ì‘í‘œë©´**: ìµœì ì  ì°¾ê¸° (ê³ ê¸‰)
                """,
                'detailed': """
                ### ì„¤ê³„ ë°©ë²•ë³„ íŠ¹ì§•
                
                #### ğŸ”· ì™„ì „ ìš”ì¸ ì„¤ê³„
                - **ì¥ì **: ëª¨ë“  ì •ë³´ íšë“, ì´í•´í•˜ê¸° ì‰¬ì›€
                - **ë‹¨ì **: ì‹¤í—˜ ìˆ˜ê°€ ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€
                - **ì‚¬ìš© ì‹œê¸°**: ì¸ìê°€ ì ê³ (2-4ê°œ) ì •í™•í•œ ë¶„ì„ í•„ìš”í•  ë•Œ
                
                #### ğŸ”¶ ë¶€ë¶„ ìš”ì¸ ì„¤ê³„
                - **ì¥ì **: ì‹¤í—˜ ìˆ˜ í¬ê²Œ ê°ì†Œ
                - **ë‹¨ì **: ì¼ë¶€ ìƒí˜¸ì‘ìš© ì •ë³´ ì†ì‹¤
                - **ì‚¬ìš© ì‹œê¸°**: ìŠ¤í¬ë¦¬ë‹, ë§ì€ ì¸ì(5ê°œ ì´ìƒ)
                
                #### ğŸ”´ Box-Behnken ì„¤ê³„
                - **ì¥ì **: 2ì°¨ ëª¨ë¸, ê·¹ê°’ íšŒí”¼
                - **ë‹¨ì **: 3ê°œ ì´ìƒ ì¸ì í•„ìš”
                - **ì‚¬ìš© ì‹œê¸°**: ìµœì í™”, ê³¡ì„  ê´€ê³„
                """,
                'quiz': [
                    {
                        'question': "ì¸ìê°€ 2ê°œì´ê³  ê°ê° 3ìˆ˜ì¤€ì¼ ë•Œ, ì™„ì „ ìš”ì¸ ì„¤ê³„ì˜ ì‹¤í—˜ ìˆ˜ëŠ”?",
                        'answer': "9ê°œ (3 Ã— 3)",
                        'explanation': "ê° ì¸ìì˜ ìˆ˜ì¤€ì„ ê³±í•©ë‹ˆë‹¤."
                    }
                ]
            },
            'response_variable': {
                'title': 'ğŸ“ˆ ë°˜ì‘ ë³€ìˆ˜ë€?',
                'basic': """
                ë°˜ì‘ ë³€ìˆ˜(Response)ëŠ” ì‹¤í—˜ì—ì„œ ì¸¡ì •í•˜ëŠ” ê²°ê³¼ê°’ì…ë‹ˆë‹¤.
                
                ì˜ˆì‹œ:
                - ğŸ’ª **ì¸ì¥ê°•ë„**: ì¬ë£Œì˜ ê°•ë„
                - ğŸ“ **ì‹ ìœ¨**: ëŠ˜ì–´ë‚˜ëŠ” ì •ë„
                - ğŸŒ¡ï¸ **ìœ ë¦¬ì „ì´ì˜¨ë„**: ë¬¼ì„± ë³€í™” ì˜¨ë„
                """,
                'detailed': """
                ### ì¢‹ì€ ë°˜ì‘ ë³€ìˆ˜ì˜ ì¡°ê±´
                
                1. **ì •ëŸ‰ì **: ìˆ«ìë¡œ ì¸¡ì • ê°€ëŠ¥
                2. **ì¬í˜„ì„±**: ê°™ì€ ì¡°ê±´ì—ì„œ ë¹„ìŠ·í•œ ê°’
                3. **ë¯¼ê°ì„±**: ì¸ì ë³€í™”ì— ë°˜ì‘
                4. **ê´€ë ¨ì„±**: ì—°êµ¬ ëª©ì ê³¼ ì§ê²°
                
                ### ë°˜ì‘ ë³€ìˆ˜ ìœ í˜•
                - **ëª©í‘œê°’**: íŠ¹ì • ê°’ì— ë§ì¶”ê¸° (ì˜ˆ: pH 7.0)
                - **ìµœëŒ€í™”**: í´ìˆ˜ë¡ ì¢‹ìŒ (ì˜ˆ: ê°•ë„)
                - **ìµœì†Œí™”**: ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ (ì˜ˆ: ë¶ˆëŸ‰ë¥ )
                """
            },
            'analysis': {
                'title': 'ğŸ“Š ê²°ê³¼ ë¶„ì„ ì´í•´í•˜ê¸°',
                'basic': """
                ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ìì˜ ì˜í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
                
                ì£¼ìš” ë¶„ì„:
                - **ì£¼íš¨ê³¼**: ê° ì¸ìì˜ ì˜í–¥
                - **ìƒí˜¸ì‘ìš©**: ì¸ìë“¤ì˜ ë³µí•© ì˜í–¥
                - **ìµœì  ì¡°ê±´**: ê°€ì¥ ì¢‹ì€ ì„¤ì •
                """,
                'detailed': """
                ### í†µê³„ ìš©ì–´ ì‰½ê²Œ ì´í•´í•˜ê¸°
                
                #### ğŸ“Œ p-value (ìœ ì˜í™•ë¥ )
                - **p < 0.05**: "ìš°ì—°ì´ ì•„ë‹ˆë‹¤!" âœ…
                - **p â‰¥ 0.05**: "ìš°ì—°ì¼ ìˆ˜ë„..." âŒ
                - ì‘ì„ìˆ˜ë¡ ì¸ìì˜ ì˜í–¥ì´ í™•ì‹¤í•¨
                
                #### ğŸ“Œ RÂ² (ê²°ì •ê³„ìˆ˜)
                - ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€
                - 0~1 ì‚¬ì´ ê°’ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
                - 0.8 ì´ìƒì´ë©´ ëŒ€ì²´ë¡œ ì–‘í˜¸
                
                #### ğŸ“Œ ì£¼íš¨ê³¼ ê·¸ë˜í”„
                - ê¸°ìš¸ê¸°ê°€ ê¸‰í• ìˆ˜ë¡ ì˜í–¥ì´ í¼
                - í‰í‰í•˜ë©´ ì˜í–¥ì´ ì ìŒ
                
                #### ğŸ“Œ ìƒí˜¸ì‘ìš© ê·¸ë˜í”„
                - ì„ ì´ í‰í–‰: ìƒí˜¸ì‘ìš© ì—†ìŒ
                - ì„ ì´ êµì°¨: ìƒí˜¸ì‘ìš© ìˆìŒ
                """
            }
        }
        
        self.tooltips = {
            'factor': "ê²°ê³¼ì— ì˜í–¥ì„ ì£¼ëŠ” ì‹¤í—˜ ì¡°ê±´",
            'response': "ì¸¡ì •í•˜ë ¤ëŠ” ì‹¤í—˜ ê²°ê³¼",
            'level': "ì¸ìê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’",
            'replicate': "ê°™ì€ ì¡°ê±´ì˜ ë°˜ë³µ ì‹¤í—˜",
            'block': "ì™¸ë¶€ ì˜í–¥ì„ ì¤„ì´ëŠ” ì‹¤í—˜ ê·¸ë£¹",
            'randomization': "ìˆœì„œ íš¨ê³¼ë¥¼ ì—†ì• ëŠ” ë¬´ì‘ìœ„ ë°°ì¹˜",
            'center_point': "ì¤‘ê°„ ì¡°ê±´ì—ì„œì˜ ì¶”ê°€ ì‹¤í—˜",
            'resolution': "êµ¬ë³„ ê°€ëŠ¥í•œ íš¨ê³¼ì˜ ìˆ˜ì¤€",
            'confounding': "íš¨ê³¼ë¥¼ êµ¬ë³„í•  ìˆ˜ ì—†ëŠ” ìƒíƒœ",
            'orthogonal': "ì¸ìë“¤ì´ ë…ë¦½ì ì¸ ì„¤ê³„"
        }
    
    def get_help(self, topic: str, level: str = 'basic') -> str:
        """ë„ì›€ë§ ë‚´ìš© ë°˜í™˜"""
        if topic in self.help_database:
            help_content = self.help_database[topic]
            
            if level == 'basic':
                return f"## {help_content['title']}\n{help_content['basic']}"
            elif level == 'detailed':
                return f"## {help_content['title']}\n{help_content['basic']}\n{help_content['detailed']}"
            elif level == 'examples' and 'examples' in help_content:
                examples = '\n'.join([f"- {ex}" for ex in help_content['examples']])
                return f"### ğŸ“ ì˜ˆì‹œ\n{examples}"
        
        return "ë„ì›€ë§ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def get_tooltip(self, term: str) -> str:
        """íˆ´íŒ ë°˜í™˜"""
        return self.tooltips.get(term, "")
    
    def show_help_button(self, topic: str, key: str = None):
        """ë„ì›€ë§ ë²„íŠ¼ í‘œì‹œ"""
        if st.button("â“ ë„ì›€ë§", key=key):
            st.info(self.get_help(topic, 'basic'))
            
            if st.button("ğŸ“– ë” ìì„¸íˆ", key=f"{key}_more"):
                st.info(self.get_help(topic, 'detailed'))
    
    def show_contextual_help(self, context: str, user_level: UserLevel):
        """ìƒí™©ë³„ ë„ì›€ë§ í‘œì‹œ"""
        # ì´ˆë³´ìëŠ” ìë™ìœ¼ë¡œ ë„ì›€ë§ í‘œì‹œ
        if user_level == UserLevel.BEGINNER:
            with st.expander("ğŸ’¡ ë„ì›€ë§", expanded=True):
                st.markdown(self.get_help(context, 'basic'))
        
        # ì¤‘ê¸‰ìëŠ” ë²„íŠ¼ìœ¼ë¡œ í‘œì‹œ
        elif user_level == UserLevel.INTERMEDIATE:
            self.show_help_button(context)

# ==================== ìºì‹œ ì‹œìŠ¤í…œ ====================
class CacheManager:
    """íš¨ìœ¨ì ì¸ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, cache_dir: str = ".cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        self.memory_cache = {}
        self.cache_stats = defaultdict(lambda: {'hits': 0, 'misses': 0})
    
    def _get_cache_key(self, func_name: str, *args, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{func_name}:{args}:{sorted(kwargs.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if key in self.memory_cache:
            self.cache_stats[key]['hits'] += 1
            return self.memory_cache[key]
        
        # ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    value = pickle.load(f)
                
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                self.memory_cache[key] = value
                self.cache_stats[key]['hits'] += 1
                return value
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        self.cache_stats[key]['misses'] += 1
        return None
    
    def set(self, key: str, value: Any, ttl: int = 3600):
        """ìºì‹œì— ê°’ ì €ì¥"""
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
        self.memory_cache[key] = value
        
        # ë””ìŠ¤í¬ ìºì‹œì— ì €ì¥
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(value, f)
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def invalidate(self, pattern: str = None):
        """ìºì‹œ ë¬´íš¨í™”"""
        if pattern:
            # íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ìºì‹œ ì‚­ì œ
            keys_to_remove = [k for k in self.memory_cache.keys() if pattern in k]
            for key in keys_to_remove:
                del self.memory_cache[key]
                
                cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
                if os.path.exists(cache_file):
                    os.remove(cache_file)
        else:
            # ì „ì²´ ìºì‹œ ì‚­ì œ
            self.memory_cache.clear()
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)
    
    def get_stats(self) -> Dict[str, Dict[str, int]]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        return dict(self.cache_stats)

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
cache_manager = CacheManager()

def cached(ttl: int = 3600):
    """ìºì‹± ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = cache_manager._get_cache_key(func.__name__, *args, **kwargs)
            
            # ìºì‹œ í™•ì¸
            cached_value = cache_manager.get(cache_key)
            if cached_value is not None:
                return cached_value
            
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ìºì‹œ ì €ì¥
            cache_manager.set(cache_key, result, ttl)
            
            return result
        
        return wrapper
    return decorator

# ==================== ì„¤ì • ê´€ë¦¬ ====================
class ConfigManager:
    """ì„¤ì • ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    DEFAULT_CONFIG = {
        'app': {
            'name': 'ê³ ë¶„ì ì‹¤í—˜ ì„¤ê³„ í”Œë«í¼',
            'version': VERSION,
            'theme': 'light',
            'language': 'ko',
            'timezone': 'Asia/Seoul'
        },
        'experiment': {
            'default_confidence_level': 0.95,
            'default_power': 0.8,
            'max_factors': 20,
            'max_runs': 1000,
            'auto_save': True,
            'save_interval': 300  # 5ë¶„
        },
        'analysis': {
            'significance_level': 0.05,
            'outlier_threshold': 3.0,  # í‘œì¤€í¸ì°¨
            'min_r_squared': 0.7,
            'cross_validation_folds': 5
        },
        'visualization': {
            'plot_style': 'seaborn',
            'color_palette': 'viridis',
            'figure_dpi': 150,
            'interactive_plots': True,
            'animation_speed': 1.0
        },
        'ai': {
            'default_model': 'gemini',
            'temperature': 0.7,
            'max_tokens': 2000,
            'timeout': 30,
            'retry_attempts': 3,
            'consensus_threshold': 0.7
        },
        'database': {
            'backup_enabled': True,
            'backup_interval': 86400,  # 24ì‹œê°„
            'max_backups': 7,
            'compression': True
        },
        'notifications': {
            'email_enabled': False,
            'slack_enabled': False,
            'desktop_enabled': True
        }
    }
    
    def __init__(self, config_file: str = "config.json"):
        self.config_file = config_file
        self.config = self.load_config()
    
    def load_config(self) -> Dict[str, Any]:
        """ì„¤ì • ë¡œë“œ"""
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                
                # ê¸°ë³¸ ì„¤ì •ê³¼ ë³‘í•©
                return self._merge_configs(self.DEFAULT_CONFIG, user_config)
            except Exception as e:
                logger.error(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return self.DEFAULT_CONFIG.copy()
    
    def save_config(self):
        """ì„¤ì • ì €ì¥"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            
            logger.info("ì„¤ì • ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get(self, path: str, default: Any = None) -> Any:
        """ì„¤ì • ê°’ ê°€ì ¸ì˜¤ê¸° (ì  í‘œê¸°ë²• ì§€ì›)"""
        keys = path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value
    
    def set(self, path: str, value: Any):
        """ì„¤ì • ê°’ ì„¤ì •"""
        keys = path.split('.')
        config = self.config
        
        for key in keys[:-1]:
            if key not in config:
                config[key] = {}
            config = config[key]
        
        config[keys[-1]] = value
        self.save_config()
    
    def _merge_configs(self, default: Dict, user: Dict) -> Dict:
        """ì„¤ì • ë³‘í•© (ì¬ê·€ì )"""
        result = default.copy()
        
        for key, value in user.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._merge_configs(result[key], value)
            else:
                result[key] = value
        
        return result

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
config_manager = ConfigManager()

# ==================== ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ ====================
class EventType(Enum):
    """ì´ë²¤íŠ¸ ìœ í˜•"""
    PROJECT_CREATED = "project_created"
    PROJECT_UPDATED = "project_updated"
    EXPERIMENT_STARTED = "experiment_started"
    EXPERIMENT_COMPLETED = "experiment_completed"
    ANALYSIS_COMPLETED = "analysis_completed"
    ERROR_OCCURRED = "error_occurred"
    USER_ACTION = "user_action"
    SYSTEM_EVENT = "system_event"

@dataclass
class Event:
    """ì´ë²¤íŠ¸ ë°ì´í„°"""
    type: EventType
    timestamp: datetime
    data: Dict[str, Any]
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class EventBus:
    """ì´ë²¤íŠ¸ ë²„ìŠ¤ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.subscribers: Dict[EventType, List[Callable]] = defaultdict(list)
        self.event_queue: queue.Queue = queue.Queue()
        self.event_history: deque = deque(maxlen=1000)
        self.running = False
        self.worker_thread = None
    
    def subscribe(self, event_type: EventType, callback: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        self.subscribers[event_type].append(callback)
        logger.debug(f"êµ¬ë… ì¶”ê°€: {event_type.value} -> {callback.__name__}")
    
    def unsubscribe(self, event_type: EventType, callback: Callable):
        """êµ¬ë… í•´ì œ"""
        if callback in self.subscribers[event_type]:
            self.subscribers[event_type].remove(callback)
    
    def publish(self, event: Event):
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        self.event_queue.put(event)
        self.event_history.append(event)
        
        # ì¦‰ì‹œ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°
        if event.type == EventType.ERROR_OCCURRED:
            self._process_event(event)
    
    def start(self):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œì‘"""
        if not self.running:
            self.running = True
            self.worker_thread = threading.Thread(target=self._worker, daemon=True)
            self.worker_thread.start()
            logger.info("ì´ë²¤íŠ¸ ë²„ìŠ¤ ì‹œì‘")
    
    def stop(self):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ì§€"""
        self.running = False
        if self.worker_thread:
            self.worker_thread.join()
        logger.info("ì´ë²¤íŠ¸ ë²„ìŠ¤ ì¤‘ì§€")
    
    def _worker(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤"""
        while self.running:
            try:
                event = self.event_queue.get(timeout=1)
                self._process_event(event)
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"ì´ë²¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def _process_event(self, event: Event):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        for callback in self.subscribers[event.type]:
            try:
                callback(event)
            except Exception as e:
                logger.error(f"ì´ë²¤íŠ¸ ì½œë°± ì˜¤ë¥˜: {callback.__name__} - {e}")
    
    def get_history(self, event_type: EventType = None, limit: int = 100) -> List[Event]:
        """ì´ë²¤íŠ¸ ê¸°ë¡ ì¡°íšŒ"""
        history = list(self.event_history)
        
        if event_type:
            history = [e for e in history if e.type == event_type]
        
        return history[-limit:]

# ì „ì—­ ì´ë²¤íŠ¸ ë²„ìŠ¤
event_bus = EventBus()

# ==================== ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ====================
class DatabaseManager:
    """í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 db_type: str = "sqlite",
                 db_path: str = "polymer_doe.db",
                 backup_enabled: bool = True):
        self.db_type = db_type
        self.db_path = db_path
        self.backup_enabled = backup_enabled
        self.connection_pool = []
        self.lock = threading.Lock()
        
        self._init_database()
        
        # ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬
        if backup_enabled:
            self._schedule_backups()
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        if self.db_type == "sqlite":
            self._init_sqlite()
        elif self.db_type == "mongodb" and MONGODB_AVAILABLE:
            self._init_mongodb()
        else:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DB íƒ€ì…: {self.db_type}")
            self.db_type = "sqlite"
            self._init_sqlite()
    
    def _init_sqlite(self):
        """SQLite ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # í”„ë¡œì íŠ¸ í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS projects (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    description TEXT,
                    polymer_type TEXT,
                    polymer_system TEXT,
                    objectives TEXT,
                    constraints TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    owner TEXT,
                    collaborators TEXT,
                    tags TEXT,
                    version INTEGER DEFAULT 1,
                    parent_project_id TEXT,
                    status TEXT DEFAULT 'active',
                    budget REAL,
                    deadline TIMESTAMP,
                    notes TEXT,
                    attachments TEXT,
                    custom_fields TEXT,
                    data BLOB
                )
            """)
            
            # ì‹¤í—˜ ë°ì´í„° í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS experiments (
                    id TEXT PRIMARY KEY,
                    project_id TEXT,
                    design_matrix TEXT,
                    results TEXT,
                    metadata TEXT,
                    created_at TIMESTAMP,
                    status TEXT,
                    run_order TEXT,
                    block_structure TEXT,
                    conditions TEXT,
                    operator TEXT,
                    equipment TEXT,
                    notes TEXT,
                    data BLOB,
                    FOREIGN KEY (project_id) REFERENCES projects (id)
                )
            """)
            
            # ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS analysis_results (
                    id TEXT PRIMARY KEY,
                    experiment_id TEXT,
                    analysis_type TEXT,
                    results TEXT,
                    plots TEXT,
                    statistics TEXT,
                    created_at TIMESTAMP,
                    parameters TEXT,
                    quality_metrics TEXT,
                    FOREIGN KEY (experiment_id) REFERENCES experiments (id)
                )
            """)
            
            # ì‚¬ìš©ì í™œë™ ë¡œê·¸
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS activity_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT,
                    action TEXT,
                    details TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    session_id TEXT,
                    ip_address TEXT,
                    user_agent TEXT
                )
            """)
            
            # í•™ìŠµ ë°ì´í„° í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS learning_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    user_id TEXT,
                    user_level TEXT,
                    action_type TEXT,
                    action_details TEXT,
                    outcome TEXT,
                    quality_score REAL,
                    context TEXT,
                    feedback TEXT
                )
            """)
            
            # AI ìƒí˜¸ì‘ìš© ë¡œê·¸
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS ai_interactions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    user_id TEXT,
                    prompt TEXT,
                    response TEXT,
                    model TEXT,
                    tokens_used INTEGER,
                    response_time REAL,
                    quality_rating REAL,
                    feedback TEXT
                )
            """)
            
            # í˜‘ì—… ë°ì´í„°
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS collaborations (
                    id TEXT PRIMARY KEY,
                    project_id TEXT,
                    type TEXT,  -- comment, review, suggestion
                    user_id TEXT,
                    content TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    parent_id TEXT,  -- for threaded discussions
                    status TEXT,
                    metadata TEXT,
                    FOREIGN KEY (project_id) REFERENCES projects (id)
                )
            """)
            
            # í…œí”Œë¦¿ ì €ì¥ì†Œ
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS templates (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    description TEXT,
                    type TEXT,  -- project, experiment, analysis
                    content TEXT,
                    created_by TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    usage_count INTEGER DEFAULT 0,
                    rating REAL,
                    tags TEXT,
                    public BOOLEAN DEFAULT FALSE
                )
            """)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_projects_owner ON projects(owner)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_experiments_project ON experiments(project_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_activity_user ON activity_log(user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_learning_user ON learning_data(user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_ai_user ON ai_interactions(user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_collaborations_project ON collaborations(project_id)")
            
            conn.commit()
            logger.info("SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    @contextmanager
    def get_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        conn = None
        try:
            if self.connection_pool:
                conn = self.connection_pool.pop()
            else:
                if self.db_type == "sqlite":
                    conn = sqlite3.connect(self.db_path)
                    conn.row_factory = sqlite3.Row
            
            yield conn
            
        finally:
            if conn:
                if len(self.connection_pool) < 5:
                    self.connection_pool.append(conn)
                else:
                    conn.close()
    
    def execute_query(self, query: str, params: tuple = None) -> List[Dict]:
        """ì¿¼ë¦¬ ì‹¤í–‰"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            if query.strip().upper().startswith('SELECT'):
                columns = [desc[0] for desc in cursor.description]
                return [dict(zip(columns, row)) for row in cursor.fetchall()]
            else:
                conn.commit()
                return []
    
    def save_project(self, project: ProjectInfo) -> bool:
        """í”„ë¡œì íŠ¸ ì €ì¥"""
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                # JSON ì§ë ¬í™”
                polymer_system = json.dumps(project.polymer_system)
                objectives = json.dumps(project.objectives)
                constraints = json.dumps(project.constraints)
                collaborators = json.dumps(project.collaborators)
                tags = json.dumps(project.tags)
                attachments = json.dumps(project.attachments)
                custom_fields = json.dumps(project.custom_fields)
                
                # ì „ì²´ ê°ì²´ ì§ë ¬í™” (ë°±ì—…ìš©)
                data = pickle.dumps(project)
                
                cursor.execute("""
                    INSERT OR REPLACE INTO projects 
                    (id, name, description, polymer_type, polymer_system, 
                     objectives, constraints, created_at, updated_at, 
                     owner, collaborators, tags, version, parent_project_id,
                     status, budget, deadline, notes, attachments, 
                     custom_fields, data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    project.id, project.name, project.description,
                    project.polymer_type, polymer_system, objectives,
                    constraints, project.created_at, project.updated_at,
                    project.owner, collaborators, tags, project.version,
                    project.parent_project_id, project.status, project.budget,
                    project.deadline, project.notes, attachments,
                    custom_fields, data
                ))
                
                conn.commit()
                
                # ì´ë²¤íŠ¸ ë°œí–‰
                event_bus.publish(Event(
                    type=EventType.PROJECT_CREATED,
                    timestamp=datetime.now(),
                    data={'project_id': project.id, 'name': project.name},
                    user_id=project.owner
                ))
                
                return True
                
        except Exception as e:
            logger.error(f"í”„ë¡œì íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_project(self, project_id: str) -> Optional[ProjectInfo]:
        """í”„ë¡œì íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            results = self.execute_query(
                "SELECT data FROM projects WHERE id = ?",
                (project_id,)
            )
            
            if results:
                return pickle.loads(results[0]['data'])
            
            return None
            
        except Exception as e:
            logger.error(f"í”„ë¡œì íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def search_projects(self, 
                       owner: str = None,
                       tags: List[str] = None,
                       polymer_type: str = None,
                       status: str = None,
                       limit: int = 100) -> List[ProjectInfo]:
        """í”„ë¡œì íŠ¸ ê²€ìƒ‰"""
        query = "SELECT data FROM projects WHERE 1=1"
        params = []
        
        if owner:
            query += " AND owner = ?"
            params.append(owner)
        
        if polymer_type:
            query += " AND polymer_type = ?"
            params.append(polymer_type)
        
        if status:
            query += " AND status = ?"
            params.append(status)
        
        if tags:
            # íƒœê·¸ëŠ” JSON ë°°ì—´ë¡œ ì €ì¥ë˜ì–´ ìˆìŒ
            for tag in tags:
                query += " AND tags LIKE ?"
                params.append(f'%"{tag}"%')
        
        query += " ORDER BY updated_at DESC LIMIT ?"
        params.append(limit)
        
        results = self.execute_query(query, tuple(params))
        
        projects = []
        for row in results:
            try:
                project = pickle.loads(row['data'])
                projects.append(project)
            except:
                continue
        
        return projects
    
    def save_experiment(self, experiment: ExperimentData) -> bool:
        """ì‹¤í—˜ ë°ì´í„° ì €ì¥"""
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                # ì§ë ¬í™”
                design_matrix = experiment.design_matrix.to_json()
                results = experiment.results.to_json() if experiment.results is not None else None
                metadata = json.dumps(experiment.metadata)
                run_order = json.dumps(experiment.run_order)
                block_structure = json.dumps(experiment.block_structure)
                conditions = json.dumps(experiment.conditions)
                notes = json.dumps(experiment.notes)
                data = pickle.dumps(experiment)
                
                cursor.execute("""
                    INSERT OR REPLACE INTO experiments 
                    (id, project_id, design_matrix, results, metadata,
                     created_at, status, run_order, block_structure,
                     conditions, operator, equipment, notes, data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    experiment.id, experiment.project_id, design_matrix,
                    results, metadata, experiment.created_at, 
                    experiment.status.value, run_order, block_structure,
                    conditions, experiment.operator, experiment.equipment,
                    notes, data
                ))
                
                conn.commit()
                
                # ì´ë²¤íŠ¸ ë°œí–‰
                event_bus.publish(Event(
                    type=EventType.EXPERIMENT_STARTED,
                    timestamp=datetime.now(),
                    data={'experiment_id': experiment.id, 'project_id': experiment.project_id}
                ))
                
                return True
                
        except Exception as e:
            logger.error(f"ì‹¤í—˜ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def save_analysis_result(self, 
                           experiment_id: str,
                           analysis_type: str,
                           results: Dict[str, Any],
                           plots: List[str] = None,
                           parameters: Dict[str, Any] = None) -> bool:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            analysis_id = generate_unique_id("ANALYSIS")
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            quality_metrics = self._calculate_quality_metrics(results)
            
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute("""
                    INSERT INTO analysis_results
                    (id, experiment_id, analysis_type, results, plots,
                     statistics, created_at, parameters, quality_metrics)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    analysis_id, experiment_id, analysis_type,
                    json.dumps(results), json.dumps(plots or []),
                    json.dumps(results.get('statistics', {})),
                    datetime.now(), json.dumps(parameters or {}),
                    json.dumps(quality_metrics)
                ))
                
                conn.commit()
                
                # ì´ë²¤íŠ¸ ë°œí–‰
                event_bus.publish(Event(
                    type=EventType.ANALYSIS_COMPLETED,
                    timestamp=datetime.now(),
                    data={
                        'analysis_id': analysis_id,
                        'experiment_id': experiment_id,
                        'type': analysis_type
                    }
                ))
                
                return True
                
        except Exception as e:
            logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def log_activity(self, 
                    user_id: str,
                    action: str,
                    details: Dict[str, Any],
                    session_id: str = None):
        """í™œë™ ë¡œê·¸ ê¸°ë¡"""
        try:
            self.execute_query("""
                INSERT INTO activity_log 
                (user_id, action, details, session_id)
                VALUES (?, ?, ?, ?)
            """, (user_id, action, json.dumps(details), session_id))
            
        except Exception as e:
            logger.error(f"í™œë™ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    def save_learning_record(self, record: LearningRecord):
        """í•™ìŠµ ê¸°ë¡ ì €ì¥"""
        try:
            self.execute_query("""
                INSERT INTO learning_data
                (timestamp, user_id, user_level, action_type,
                 action_details, outcome, quality_score, context, feedback)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                record.timestamp, record.user_id, record.user_level.name,
                record.action_type, json.dumps(record.action_details),
                json.dumps(record.outcome), record.quality_score,
                json.dumps(record.context), record.feedback
            ))
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_learning_recommendations(self, 
                                   user_id: str,
                                   context: str,
                                   limit: int = 5) -> List[Dict[str, Any]]:
        """í•™ìŠµ ê¸°ë°˜ ì¶”ì²œ"""
        # ìœ ì‚¬í•œ ì»¨í…ìŠ¤íŠ¸ì˜ ì„±ê³µ ì‚¬ë¡€ ê²€ìƒ‰
        query = """
            SELECT action_details, outcome, quality_score
            FROM learning_data
            WHERE user_id = ? AND action_type = ? AND quality_score > 0.7
            ORDER BY quality_score DESC, timestamp DESC
            LIMIT ?
        """
        
        results = self.execute_query(query, (user_id, context, limit * 2))
        
        recommendations = []
        for row in results[:limit]:
            try:
                recommendations.append({
                    'action': json.loads(row['action_details']),
                    'outcome': json.loads(row['outcome']),
                    'score': row['quality_score']
                })
            except:
                continue
        
        return recommendations
    
    def save_template(self, 
                     name: str,
                     template_type: str,
                     content: Dict[str, Any],
                     user_id: str,
                     description: str = "",
                     tags: List[str] = None,
                     public: bool = False) -> str:
        """í…œí”Œë¦¿ ì €ì¥"""
        template_id = generate_unique_id("TEMPLATE")
        
        try:
            self.execute_query("""
                INSERT INTO templates
                (id, name, description, type, content, created_by,
                 created_at, updated_at, tags, public)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                template_id, name, description, template_type,
                json.dumps(content), user_id, datetime.now(),
                datetime.now(), json.dumps(tags or []), public
            ))
            
            return template_id
            
        except Exception as e:
            logger.error(f"í…œí”Œë¦¿ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def load_template(self, template_id: str) -> Optional[Dict[str, Any]]:
        """í…œí”Œë¦¿ ë¶ˆëŸ¬ì˜¤ê¸°"""
        results = self.execute_query(
            "SELECT * FROM templates WHERE id = ?",
            (template_id,)
        )
        
        if results:
            template = results[0]
            template['content'] = json.loads(template['content'])
            template['tags'] = json.loads(template['tags'])
            
            # ì‚¬ìš© íšŸìˆ˜ ì¦ê°€
            self.execute_query(
                "UPDATE templates SET usage_count = usage_count + 1 WHERE id = ?",
                (template_id,)
            )
            
            return template
        
        return None
    
    def search_templates(self,
                        template_type: str = None,
                        tags: List[str] = None,
                        public_only: bool = True,
                        limit: int = 50) -> List[Dict[str, Any]]:
        """í…œí”Œë¦¿ ê²€ìƒ‰"""
        query = "SELECT * FROM templates WHERE 1=1"
        params = []
        
        if template_type:
            query += " AND type = ?"
            params.append(template_type)
        
        if public_only:
            query += " AND public = 1"
        
        if tags:
            for tag in tags:
                query += " AND tags LIKE ?"
                params.append(f'%"{tag}"%')
        
        query += " ORDER BY usage_count DESC, rating DESC LIMIT ?"
        params.append(limit)
        
        results = self.execute_query(query, tuple(params))
        
        templates = []
        for row in results:
            template = dict(row)
            template['content'] = json.loads(template['content'])
            template['tags'] = json.loads(template['tags'])
            templates.append(template)
        
        return templates
    
    def _calculate_quality_metrics(self, results: Dict[str, Any]) -> Dict[str, float]:
        """ë¶„ì„ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        
        # RÂ² ê°’
        if 'r_squared' in results:
            metrics['r_squared'] = results['r_squared']
        
        # p-value ê¸°ë°˜ ì‹ ë¢°ë„
        if 'p_values' in results:
            p_values = results['p_values']
            if isinstance(p_values, dict):
                significant_count = sum(1 for p in p_values.values() if p < 0.05)
                metrics['significance_ratio'] = significant_count / len(p_values) if p_values else 0
        
        # ì”ì°¨ ë¶„ì„
        if 'residuals' in results:
            residuals = results['residuals']
            if isinstance(residuals, dict):
                metrics['residual_normality'] = residuals.get('normality_p_value', 0)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        quality_score = 0
        weights = {'r_squared': 0.4, 'significance_ratio': 0.3, 'residual_normality': 0.3}
        
        for metric, weight in weights.items():
            if metric in metrics:
                quality_score += metrics[metric] * weight
        
        metrics['overall_quality'] = quality_score
        
        return metrics
    
    def backup_database(self) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = "backups"
        os.makedirs(backup_dir, exist_ok=True)
        
        if self.db_type == "sqlite":
            backup_file = os.path.join(backup_dir, f"polymer_doe_{timestamp}.db")
            
            try:
                import shutil
                shutil.copy2(self.db_path, backup_file)
                
                # ì••ì¶•
                if config_manager.get('database.compression'):
                    import gzip
                    with open(backup_file, 'rb') as f_in:
                        with gzip.open(f"{backup_file}.gz", 'wb') as f_out:
                            f_out.writelines(f_in)
                    
                    os.remove(backup_file)
                    backup_file = f"{backup_file}.gz"
                
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ: {backup_file}")
                
                # ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ
                self._cleanup_old_backups(backup_dir)
                
                return backup_file
                
            except Exception as e:
                logger.error(f"ë°±ì—… ì‹¤íŒ¨: {e}")
                return None
    
    def _cleanup_old_backups(self, backup_dir: str):
        """ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ"""
        max_backups = config_manager.get('database.max_backups', 7)
        
        backups = sorted([
            f for f in os.listdir(backup_dir) 
            if f.startswith('polymer_doe_') and (f.endswith('.db') or f.endswith('.db.gz'))
        ])
        
        if len(backups) > max_backups:
            for old_backup in backups[:-max_backups]:
                try:
                    os.remove(os.path.join(backup_dir, old_backup))
                    logger.info(f"ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {old_backup}")
                except:
                    pass
    
    def _schedule_backups(self):
        """ë°±ì—… ìŠ¤ì¼€ì¤„ë§"""
        def backup_task():
            while self.backup_enabled:
                interval = config_manager.get('database.backup_interval', 86400)
                time.sleep(interval)
                self.backup_database()
        
        backup_thread = threading.Thread(target=backup_task, daemon=True)
        backup_thread.start()

# ì „ì—­ ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
db_manager = DatabaseManager()

# ==================== í˜‘ì—… ì‹œìŠ¤í…œ ====================
class CollaborationType(Enum):
    """í˜‘ì—… ìœ í˜•"""
    COMMENT = "comment"
    REVIEW = "review"
    SUGGESTION = "suggestion"
    APPROVAL = "approval"
    QUESTION = "question"

@dataclass
class Collaboration:
    """í˜‘ì—… ë°ì´í„°"""
    id: str
    project_id: str
    type: CollaborationType
    user_id: str
    content: str
    created_at: datetime
    updated_at: datetime
    parent_id: Optional[str] = None
    status: str = "active"
    metadata: Dict[str, Any] = field(default_factory=dict)
    reactions: Dict[str, List[str]] = field(default_factory=dict)  # emoji -> user_ids

class CollaborationManager:
    """í˜‘ì—… ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db = db_manager
        self.active_sessions: Dict[str, List[str]] = defaultdict(list)  # project_id -> user_ids
    
    def add_collaboration(self, 
                         project_id: str,
                         collab_type: CollaborationType,
                         user_id: str,
                         content: str,
                         parent_id: str = None) -> str:
        """í˜‘ì—… í•­ëª© ì¶”ê°€"""
        collab_id = generate_unique_id("COLLAB")
        
        collaboration = Collaboration(
            id=collab_id,
            project_id=project_id,
            type=collab_type,
            user_id=user_id,
            content=content,
            created_at=datetime.now(),
            updated_at=datetime.now(),
            parent_id=parent_id
        )
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self.db.execute_query("""
            INSERT INTO collaborations
            (id, project_id, type, user_id, content, created_at,
             updated_at, parent_id, status, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            collab_id, project_id, collab_type.value, user_id,
            content, collaboration.created_at, collaboration.updated_at,
            parent_id, collaboration.status, json.dumps(collaboration.metadata)
        ))
        
        # ì‹¤ì‹œê°„ ì•Œë¦¼ (í™œì„± ì‚¬ìš©ìì—ê²Œ)
        self._notify_collaborators(project_id, collaboration)
        
        return collab_id
    
    def get_collaborations(self, 
                          project_id: str,
                          collab_type: CollaborationType = None,
                          parent_id: str = None) -> List[Collaboration]:
        """í˜‘ì—… í•­ëª© ì¡°íšŒ"""
        query = "SELECT * FROM collaborations WHERE project_id = ?"
        params = [project_id]
        
        if collab_type:
            query += " AND type = ?"
            params.append(collab_type.value)
        
        if parent_id is not None:
            query += " AND parent_id = ?"
            params.append(parent_id)
        elif parent_id is None:
            query += " AND parent_id IS NULL"
        
        query += " ORDER BY created_at DESC"
        
        results = self.db.execute_query(query, tuple(params))
        
        collaborations = []
        for row in results:
            collab = Collaboration(
                id=row['id'],
                project_id=row['project_id'],
                type=CollaborationType(row['type']),
                user_id=row['user_id'],
                content=row['content'],
                created_at=row['created_at'],
                updated_at=row['updated_at'],
                parent_id=row['parent_id'],
                status=row['status'],
                metadata=json.loads(row['metadata']) if row['metadata'] else {}
            )
            collaborations.append(collab)
        
        return collaborations
    
    def update_collaboration(self, 
                           collab_id: str,
                           content: str = None,
                           status: str = None) -> bool:
        """í˜‘ì—… í•­ëª© ìˆ˜ì •"""
        updates = []
        params = []
        
        if content is not None:
            updates.append("content = ?")
            params.append(content)
        
        if status is not None:
            updates.append("status = ?")
            params.append(status)
        
        if not updates:
            return False
        
        updates.append("updated_at = ?")
        params.append(datetime.now())
        
        params.append(collab_id)
        
        query = f"UPDATE collaborations SET {', '.join(updates)} WHERE id = ?"
        
        try:
            self.db.execute_query(query, tuple(params))
            return True
        except:
            return False
    
    def add_reaction(self, collab_id: str, user_id: str, emoji: str):
        """ë°˜ì‘ ì¶”ê°€"""
        # í˜„ì¬ ë°˜ì‘ ê°€ì ¸ì˜¤ê¸°
        results = self.db.execute_query(
            "SELECT metadata FROM collaborations WHERE id = ?",
            (collab_id,)
        )
        
        if results:
            metadata = json.loads(results[0]['metadata']) if results[0]['metadata'] else {}
            reactions = metadata.get('reactions', {})
            
            if emoji not in reactions:
                reactions[emoji] = []
            
            if user_id not in reactions[emoji]:
                reactions[emoji].append(user_id)
            
            metadata['reactions'] = reactions
            
            # ì—…ë°ì´íŠ¸
            self.db.execute_query(
                "UPDATE collaborations SET metadata = ? WHERE id = ?",
                (json.dumps(metadata), collab_id)
            )
    
    def join_session(self, project_id: str, user_id: str):
        """í˜‘ì—… ì„¸ì…˜ ì°¸ê°€"""
        if user_id not in self.active_sessions[project_id]:
            self.active_sessions[project_id].append(user_id)
            logger.info(f"ì‚¬ìš©ì {user_id}ê°€ í”„ë¡œì íŠ¸ {project_id} ì„¸ì…˜ì— ì°¸ê°€")
    
    def leave_session(self, project_id: str, user_id: str):
        """í˜‘ì—… ì„¸ì…˜ ë‚˜ê°€ê¸°"""
        if user_id in self.active_sessions[project_id]:
            self.active_sessions[project_id].remove(user_id)
            logger.info(f"ì‚¬ìš©ì {user_id}ê°€ í”„ë¡œì íŠ¸ {project_id} ì„¸ì…˜ì—ì„œ ë‚˜ê°")
    
    def get_active_users(self, project_id: str) -> List[str]:
        """í™œì„± ì‚¬ìš©ì ëª©ë¡"""
        return self.active_sessions.get(project_id, [])
    
    def _notify_collaborators(self, project_id: str, collaboration: Collaboration):
        """í˜‘ì—…ìì—ê²Œ ì•Œë¦¼"""
        active_users = self.get_active_users(project_id)
        
        for user_id in active_users:
            if user_id != collaboration.user_id:
                # ì‹¤ì‹œê°„ ì•Œë¦¼ ì „ì†¡ (WebSocket ë“± ì‚¬ìš© ì‹œ)
                logger.info(f"ì•Œë¦¼: {user_id}ì—ê²Œ ìƒˆ {collaboration.type.value} ì•Œë¦¼")

# ==================== API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ (í™•ì¥) ====================
class APIKeyManager:
    """API í‚¤ë¥¼ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if 'api_keys' not in st.session_state:
            st.session_state.api_keys = {}
        if 'api_keys_initialized' not in st.session_state:
            st.session_state.api_keys_initialized = False
        
        # API ì„¤ì • ì •ì˜ (í™•ì¥)
        self.api_configs = {
            # AI APIs
            'openai': {
                'name': 'OpenAI',
                'env_key': 'OPENAI_API_KEY',
                'required': False,
                'test_endpoint': 'https://api.openai.com/v1/models',
                'category': 'ai',
                'description': 'GPT ëª¨ë¸ì„ ì‚¬ìš©í•œ ê³ ê¸‰ ì–¸ì–´ ì²˜ë¦¬',
                'features': ['í…ìŠ¤íŠ¸ ìƒì„±', 'ì½”ë“œ ìƒì„±', 'ë¶„ì„', 'ë²ˆì—­'],
                'rate_limit': {'rpm': 3500, 'tpm': 90000},
                'models': ['gpt-4', 'gpt-3.5-turbo', 'text-embedding-ada-002']
            },
            'gemini': {
                'name': 'Google Gemini',
                'env_key': 'GOOGLE_API_KEY',
                'required': False,
                'test_endpoint': 'https://generativelanguage.googleapis.com/v1beta/models',
                'category': 'ai',
                'description': 'Googleì˜ ìµœì‹  AI ëª¨ë¸',
                'features': ['ë‹¤ì¤‘ ëª¨ë‹¬', 'ê¸´ ì»¨í…ìŠ¤íŠ¸', 'ì¶”ë¡ ', 'ì°½ì˜ì„±'],
                'rate_limit': {'rpm': 60, 'rpd': 1500},
                'models': ['gemini-pro', 'gemini-pro-vision']
            },
            'anthropic': {
                'name': 'Anthropic Claude',
                'env_key': 'ANTHROPIC_API_KEY',
                'required': False,
                'test_endpoint': 'https://api.anthropic.com/v1/messages',
                'category': 'ai',
                'description': 'Claude AI ëª¨ë¸',
                'features': ['ê¸´ ì»¨í…ìŠ¤íŠ¸', 'ì•ˆì „ì„±', 'ì¶”ë¡ ', 'ì½”ë”©'],
                'rate_limit': {'rpm': 50, 'tpm': 100000},
                'models': ['claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku']
            },
            # ... ê¸°ì¡´ APIë“¤ ...
            
            # Database APIs (í™•ì¥)
            'materials_project': {
                'name': 'Materials Project',
                'env_key': 'MP_API_KEY',
                'required': False,
                'test_endpoint': 'https://api.materialsproject.org',
                'category': 'database',
                'description': 'ì¬ë£Œ ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤',
                'features': ['ì¬ë£Œ íŠ¹ì„±', 'ê³„ì‚° ë°ì´í„°', 'êµ¬ì¡° ì •ë³´'],
                'rate_limit': {'rpd': 1000}
            },
            'polymer_database': {
                'name': 'PoLyInfo',
                'env_key': 'POLYINFO_API_KEY',
                'required': False,
                'test_endpoint': 'https://polymer.nims.go.jp/api',
                'category': 'database',
                'description': 'ê³ ë¶„ì ë¬¼ì„± ë°ì´í„°ë² ì´ìŠ¤',
                'features': ['ê³ ë¶„ì ë¬¼ì„±', 'í™”í•™ êµ¬ì¡°', 'ê°€ê³µ ì¡°ê±´'],
                'rate_limit': {'rpd': 500}
            },
            'chemspider': {
                'name': 'ChemSpider',
                'env_key': 'CHEMSPIDER_API_KEY',
                'required': False,
                'test_endpoint': 'https://api.rsc.org/compounds/v1',
                'category': 'database',
                'description': 'í™”í•™ êµ¬ì¡° ë°ì´í„°ë² ì´ìŠ¤',
                'features': ['í™”í•™ êµ¬ì¡°', 'ë¬¼ì„± ì˜ˆì¸¡', 'InChI/SMILES'],
                'rate_limit': {'rpm': 15}
            }
        }
        
        self.rate_limiters = {}
        self.initialize_keys()
    
    def initialize_keys(self):
        """API í‚¤ ì´ˆê¸°í™”"""
        if not st.session_state.api_keys_initialized:
            # 1. Streamlit secretsì—ì„œ ë¡œë“œ
            self._load_from_secrets()
            
            # 2. í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ
            self._load_from_env()
            
            # 3. ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ (ê°œë°œìš©)
            self._load_from_file()
            
            # 4. ì‚¬ìš©ì ì…ë ¥ í‚¤ ë¡œë“œ
            self._load_user_keys()
            
            # 5. Rate limiter ì´ˆê¸°í™”
            self._init_rate_limiters()
            
            st.session_state.api_keys_initialized = True
            logger.info("API í‚¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_rate_limiters(self):
        """Rate limiter ì´ˆê¸°í™”"""
        for api_id, config in self.api_configs.items():
            if 'rate_limit' in config:
                self.rate_limiters[api_id] = RateLimiter(
                    api_id,
                    config['rate_limit']
                )
    
    def _load_user_keys(self):
        """ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ ë¡œë“œ"""
        if 'user_api_keys' in st.session_state:
            for key_id, value in st.session_state.user_api_keys.items():
                if value and key_id not in st.session_state.api_keys:
                    st.session_state.api_keys[key_id] = value
    
    def validate_and_set_key(self, key_id: str, key: str) -> Tuple[bool, str]:
        """API í‚¤ ê²€ì¦ ë° ì„¤ì •"""
        # í˜•ì‹ ê²€ì¦
        if not self.validate_key_format(key_id, key):
            return False, "API í‚¤ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
        
        # ì‹¤ì œ ì—°ê²° í…ŒìŠ¤íŠ¸
        test_result = self.test_api_connection(key_id, key)
        
        if test_result['status'] == 'success':
            self.set_key(key_id, key)
            return True, test_result['message']
        else:
            return False, test_result['message']
    
    @retry(max_attempts=3, delay=1.0)
    async def call_api_with_limit(self, api_id: str, api_call: Callable, *args, **kwargs):
        """Rate limitingì´ ì ìš©ëœ API í˜¸ì¶œ"""
        if api_id in self.rate_limiters:
            await self.rate_limiters[api_id].acquire()
        
        try:
            return await api_call(*args, **kwargs)
        except Exception as e:
            logger.error(f"API í˜¸ì¶œ ì‹¤íŒ¨ ({api_id}): {e}")
            raise

# ==================== Rate Limiter ====================
class RateLimiter:
    """API í˜¸ì¶œ ì†ë„ ì œí•œê¸°"""
    
    def __init__(self, api_id: str, limits: Dict[str, int]):
        self.api_id = api_id
        self.limits = limits  # {'rpm': 60, 'rpd': 1500, 'tpm': 10000}
        self.calls = defaultdict(lambda: deque(maxlen=10000))
        self.lock = threading.Lock()
    
    async def acquire(self):
        """í˜¸ì¶œ ê¶Œí•œ íšë“"""
        while not self._can_make_request():
            await asyncio.sleep(0.1)
        
        self._record_request()
    
    def _can_make_request(self) -> bool:
        """ìš”ì²­ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        now = datetime.now()
        
        with self.lock:
            # ë¶„ë‹¹ ì œí•œ (rpm)
            if 'rpm' in self.limits:
                minute_ago = now - timedelta(minutes=1)
                recent_calls = [t for t in self.calls['minute'] if t > minute_ago]
                if len(recent_calls) >= self.limits['rpm']:
                    return False
            
            # ì¼ì¼ ì œí•œ (rpd)
            if 'rpd' in self.limits:
                day_ago = now - timedelta(days=1)
                recent_calls = [t for t in self.calls['day'] if t > day_ago]
                if len(recent_calls) >= self.limits['rpd']:
                    return False
            
            # í† í° ì œí•œ (tpm)
            if 'tpm' in self.limits:
                # í† í° ìˆ˜ëŠ” ë³„ë„ë¡œ ì¶”ì  í•„ìš”
                pass
        
        return True
    
    def _record_request(self):
        """ìš”ì²­ ê¸°ë¡"""
        now = datetime.now()
        
        with self.lock:
            self.calls['minute'].append(now)
            self.calls['day'].append(now)

# ==================== API ëª¨ë‹ˆí„° (í™•ì¥) ====================
class APIMonitor:
    """API ìƒíƒœ ëª¨ë‹ˆí„°ë§ (í™•ì¥íŒ)"""
    
    def __init__(self):
        if 'api_status' not in st.session_state:
            st.session_state.api_status = {}
        if 'api_metrics' not in st.session_state:
            st.session_state.api_metrics = defaultdict(lambda: {
                'total_calls': 0,
                'successful_calls': 0,
                'failed_calls': 0,
                'total_response_time': 0,
                'total_tokens': 0,
                'total_cost': 0.0,
                'last_call': None,
                'errors': [],
                'hourly_calls': defaultdict(int),
                'daily_success_rate': [],
                'response_times': deque(maxlen=100),
                'token_usage': deque(maxlen=100)
            })
        
        self.cost_per_token = {
            'openai': {'input': 0.00003, 'output': 0.00006},  # GPT-4 ê¸°ì¤€
            'gemini': {'input': 0.00001, 'output': 0.00002},
            'anthropic': {'input': 0.00003, 'output': 0.00015}
        }
    
    def update_status(self, api_name: str, status: APIStatus, 
                     response_time: float = 0, error_msg: str = None,
                     tokens: Dict[str, int] = None):
        """API ìƒíƒœ ì—…ë°ì´íŠ¸ (í™•ì¥)"""
        st.session_state.api_status[api_name] = {
            'status': status,
            'last_update': datetime.now(),
            'response_time': response_time,
            'error': error_msg
        }
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        metrics = st.session_state.api_metrics[api_name]
        metrics['total_calls'] += 1
        metrics['last_call'] = datetime.now()
        
        # ì‹œê°„ë³„ í˜¸ì¶œ ê¸°ë¡
        current_hour = datetime.now().strftime("%Y-%m-%d %H:00")
        metrics['hourly_calls'][current_hour] += 1
        
        if status == APIStatus.ONLINE:
            metrics['successful_calls'] += 1
            metrics['total_response_time'] += response_time
            metrics['response_times'].append(response_time)
            
            # í† í° ë° ë¹„ìš© ê³„ì‚°
            if tokens:
                total_tokens = tokens.get('input', 0) + tokens.get('output', 0)
                metrics['total_tokens'] += total_tokens
                metrics['token_usage'].append(total_tokens)
                
                # ë¹„ìš© ê³„ì‚°
                if api_name in self.cost_per_token:
                    cost = (tokens.get('input', 0) * self.cost_per_token[api_name]['input'] +
                           tokens.get('output', 0) * self.cost_per_token[api_name]['output'])
                    metrics['total_cost'] += cost
        else:
            metrics['failed_calls'] += 1
            if error_msg:
                metrics['errors'].append({
                    'time': datetime.now(),
                    'error': error_msg
                })
                metrics['errors'] = metrics['errors'][-10:]
        
        # ì¼ë³„ ì„±ê³µë¥  ì—…ë°ì´íŠ¸
        self._update_daily_success_rate(api_name)
    
    def _update_daily_success_rate(self, api_name: str):
        """ì¼ë³„ ì„±ê³µë¥  ì—…ë°ì´íŠ¸"""
        metrics = st.session_state.api_metrics[api_name]
        today = datetime.now().date()
        success_rate = self.get_success_rate(api_name)
        
        if metrics['daily_success_rate']:
            last_entry = metrics['daily_success_rate'][-1]
            if last_entry['date'] == today:
                last_entry['rate'] = success_rate
            else:
                metrics['daily_success_rate'].append({
                    'date': today,
                    'rate': success_rate
                })
        else:
            metrics['daily_success_rate'].append({
                'date': today,
                'rate': success_rate
            })
        
        metrics['daily_success_rate'] = metrics['daily_success_rate'][-30:]
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° ì¤€ë¹„"""
        dashboard_data = {
            'summary': {
                'total_apis': len(api_key_manager.api_configs),
                'online_apis': 0,
                'total_calls': 0,
                'total_cost': 0.0,
                'avg_response_time': 0.0
            },
            'apis': {},
            'trends': {
                'hourly_calls': defaultdict(int),
                'daily_costs': defaultdict(float)
            }
        }
        
        # APIë³„ ë°ì´í„° ìˆ˜ì§‘
        for api_name in api_key_manager.api_configs:
            status = self.get_status(api_name)
            metrics = self.get_metrics(api_name)
            
            if status == APIStatus.ONLINE:
                dashboard_data['summary']['online_apis'] += 1
            
            dashboard_data['summary']['total_calls'] += metrics['total_calls']
            dashboard_data['summary']['total_cost'] += metrics['total_cost']
            
            dashboard_data['apis'][api_name] = {
                'status': status,
                'metrics': metrics,
                'success_rate': self.get_success_rate(api_name),
                'avg_response_time': self.get_average_response_time(api_name)
            }
            
            # íŠ¸ë Œë“œ ë°ì´í„°
            for hour, count in metrics['hourly_calls'].items():
                dashboard_data['trends']['hourly_calls'][hour] += count
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„
        total_time = sum(d['metrics']['total_response_time'] for d in dashboard_data['apis'].values())
        total_success = sum(d['metrics']['successful_calls'] for d in dashboard_data['apis'].values())
        
        if total_success > 0:
            dashboard_data['summary']['avg_response_time'] = total_time / total_success
        
        return dashboard_data
    
    def display_enhanced_dashboard(self):
        """í–¥ìƒëœ ìƒíƒœ ëŒ€ì‹œë³´ë“œ"""
        data = self.get_dashboard_data()
        
        # ìš”ì•½ ë©”íŠ¸ë¦­
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "í™œì„± API",
                f"{data['summary']['online_apis']}/{data['summary']['total_apis']}",
                delta=f"{data['summary']['online_apis']/data['summary']['total_apis']*100:.0f}%"
            )
        
        with col2:
            st.metric(
                "ì´ í˜¸ì¶œ ìˆ˜",
                f"{data['summary']['total_calls']:,}",
                delta=f"+{data['summary']['total_calls']}"
            )
        
        with col3:
            st.metric(
                "í‰ê·  ì‘ë‹µì‹œê°„",
                f"{data['summary']['avg_response_time']:.2f}s"
            )
        
        with col4:
            st.metric(
                "ì´ ë¹„ìš©",
                f"${data['summary']['total_cost']:.4f}"
            )
        
        # ì‹œê°„ë³„ íŠ¸ë Œë“œ ì°¨íŠ¸
        if data['trends']['hourly_calls']:
            fig = self._create_trend_chart(data['trends']['hourly_calls'])
            st.plotly_chart(fig, use_container_width=True)
        
        # APIë³„ ìƒì„¸ ì •ë³´
        st.markdown("### APIë³„ ìƒì„¸ ì •ë³´")
        
        for api_name, api_data in data['apis'].items():
            if api_data['metrics']['total_calls'] > 0:
                with st.expander(f"{api_key_manager.api_configs[api_name]['name']} ({api_name})"):
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("ìƒíƒœ", api_data['status'].value)
                        st.metric("ì„±ê³µë¥ ", f"{api_data['success_rate']:.1f}%")
                    
                    with col2:
                        st.metric("í‰ê·  ì‘ë‹µì‹œê°„", f"{api_data['avg_response_time']:.2f}s")
                        st.metric("ì´ í† í°", f"{api_data['metrics']['total_tokens']:,}")
                    
                    with col3:
                        st.metric("ì´ í˜¸ì¶œ", api_data['metrics']['total_calls'])
                        st.metric("ë¹„ìš©", f"${api_data['metrics']['total_cost']:.4f}")
                    
                    # ìµœê·¼ ì—ëŸ¬
                    if api_data['metrics']['errors']:
                        st.markdown("#### ìµœê·¼ ì—ëŸ¬")
                        for error in api_data['metrics']['errors'][-3:]:
                            st.error(f"{error['time'].strftime('%H:%M:%S')} - {error['error']}")
    
    def _create_trend_chart(self, hourly_data: Dict[str, int]) -> go.Figure:
        """íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„±"""
        # ë°ì´í„° ì •ë ¬
        sorted_hours = sorted(hourly_data.keys())
        hours = [h.split()[-1] for h in sorted_hours]
        counts = [hourly_data[h] for h in sorted_hours]
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=hours,
            y=counts,
            mode='lines+markers',
            name='API í˜¸ì¶œ ìˆ˜',
            line=dict(color='#667eea', width=2),
            marker=dict(size=8)
        ))
        
        fig.update_layout(
            title="ì‹œê°„ë³„ API í˜¸ì¶œ íŠ¸ë Œë“œ",
            xaxis_title="ì‹œê°„",
            yaxis_title="í˜¸ì¶œ ìˆ˜",
            hovermode='x unified',
            template='plotly_white'
        )
        
        return fig

# ì „ì—­ API ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
api_monitor = APIMonitor()

# ==================== ë²ˆì—­ ì„œë¹„ìŠ¤ (í™•ì¥) ====================
class TranslationService:
    """ë‹¤êµ­ì–´ ë²ˆì—­ ì„œë¹„ìŠ¤ (í™•ì¥íŒ)"""
    
    def __init__(self):
        self.translator = None
        self.available = False
        self.cache = {}
        self.supported_languages = SUPPORTED_LANGUAGES
        self.language_detector = None
        
        # ê¸°ìˆ  ìš©ì–´ ì‚¬ì „
        self.technical_terms = {
            'ko': {
                'polymer': 'ê³ ë¶„ì',
                'experiment': 'ì‹¤í—˜',
                'design': 'ì„¤ê³„',
                'factor': 'ì¸ì',
                'response': 'ë°˜ì‘',
                'optimization': 'ìµœì í™”',
                'analysis': 'ë¶„ì„',
                'thermoplastic': 'ì—´ê°€ì†Œì„±',
                'thermosetting': 'ì—´ê²½í™”ì„±',
                'elastomer': 'íƒ„ì„±ì²´',
                'composite': 'ë³µí•©ì¬ë£Œ'
            },
            'en': {
                'ê³ ë¶„ì': 'polymer',
                'ì‹¤í—˜': 'experiment',
                'ì„¤ê³„': 'design',
                'ì¸ì': 'factor',
                'ë°˜ì‘': 'response',
                'ìµœì í™”': 'optimization',
                'ë¶„ì„': 'analysis',
                'ì—´ê°€ì†Œì„±': 'thermoplastic',
                'ì—´ê²½í™”ì„±': 'thermosetting',
                'íƒ„ì„±ì²´': 'elastomer',
                'ë³µí•©ì¬ë£Œ': 'composite'
            }
        }
        
        self._initialize()
    
    def _initialize(self):
        """ë²ˆì—­ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        if TRANSLATION_AVAILABLE:
            try:
                self.translator = Translator()
                self.available = True
                logger.info("ë²ˆì—­ ì„œë¹„ìŠ¤ í™œì„±í™”")
                
                # ì–¸ì–´ ê°ì§€ê¸° ì´ˆê¸°í™”
                if NLP_AVAILABLE:
                    import spacy
                    try:
                        self.language_detector = spacy.load("xx_ent_wiki_sm")
                    except:
                        pass
                        
            except Exception as e:
                logger.error(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€ (ê°œì„ )"""
        if not self.available or not text:
            return 'en'
        
        try:
            # langdetect ì‚¬ìš©
            detected = langdetect.detect(text)
            
            # ì‹ ë¢°ë„ í™•ì¸
            probs = langdetect.detect_langs(text)
            if probs and probs[0].prob > 0.9:
                return detected
            
            # ë¶ˆí™•ì‹¤í•œ ê²½ìš° ì¶”ê°€ ê²€ì¦
            if self.language_detector:
                # spaCyë¥¼ ì‚¬ìš©í•œ ì¶”ê°€ ê²€ì¦
                doc = self.language_detector(text)
                if doc.lang_:
                    return doc.lang_
            
            return detected
            
        except Exception as e:
            logger.warning(f"ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            return 'en'
    
    def translate(self, text: str, target_lang: str = 'ko', 
                 source_lang: str = None, preserve_terms: bool = True) -> str:
        """í…ìŠ¤íŠ¸ ë²ˆì—­ (ê°œì„ )"""
        if not self.available or not text:
            return text
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{text}_{source_lang}_{target_lang}_{preserve_terms}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            if source_lang is None:
                source_lang = self.detect_language(text)
            
            if source_lang == target_lang:
                return text
            
            # ê¸°ìˆ  ìš©ì–´ ë³´í˜¸
            protected_text = text
            replacements = {}
            
            if preserve_terms and source_lang in self.technical_terms:
                terms = self.technical_terms[source_lang]
                for term, translation in terms.items():
                    if term in protected_text:
                        placeholder = f"__TERM_{len(replacements)}__"
                        protected_text = protected_text.replace(term, placeholder)
                        replacements[placeholder] = self.technical_terms.get(
                            target_lang, {}
                        ).get(term, translation)
            
            # ë²ˆì—­
            result = self.translator.translate(
                protected_text,
                src=source_lang,
                dest=target_lang
            )
            
            translated_text = result.text
            
            # ë³´í˜¸ëœ ìš©ì–´ ë³µì›
            for placeholder, term in replacements.items():
                translated_text = translated_text.replace(placeholder, term)
            
            self.cache[cache_key] = translated_text
            return translated_text
            
        except Exception as e:
            logger.error(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text
    
    def translate_dataframe(self, df: pd.DataFrame, columns: List[str], 
                          target_lang: str = 'ko', 
                          preserve_terms: bool = True) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ ë²ˆì—­ (ê°œì„ )"""
        if not self.available:
            return df
        
        df_translated = df.copy()
        
        # ì§„í–‰ë¥  í‘œì‹œ
        progress_bar = st.progress(0)
        total_cells = len(columns) * len(df)
        current = 0
        
        for col in columns:
            if col in df.columns:
                translated_col = f"{col}_{target_lang}"
                df_translated[translated_col] = ""
                
                for idx, value in df[col].items():
                    if pd.notna(value):
                        df_translated.at[idx, translated_col] = self.translate(
                            str(value), target_lang, preserve_terms=preserve_terms
                        )
                    
                    current += 1
                    progress_bar.progress(current / total_cells)
        
        progress_bar.empty()
        return df_translated
    
    def create_multilingual_report(self, report_content: str, 
                                 languages: List[str] = ['en', 'ko', 'ja']) -> Dict[str, str]:
        """ë‹¤êµ­ì–´ ë³´ê³ ì„œ ìƒì„±"""
        reports = {}
        
        for lang in languages:
            if lang in self.supported_languages:
                reports[lang] = self.translate(
                    report_content, 
                    target_lang=lang,
                    preserve_terms=True
                )
        
        return reports

# ì „ì—­ ë²ˆì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
translation_service = TranslationService()

# ==================== ê³ ê¸‰ ì‹¤í—˜ ì„¤ê³„ ì—”ì§„ ====================
class AdvancedDesignEngine:
    """ê³ ê¸‰ ì‹¤í—˜ ì„¤ê³„ ì—”ì§„"""
    
    def __init__(self):
        self.base_engine = ExperimentDesignEngine()
        self.ml_models = {}
        self.design_history = []
        
    def generate_adaptive_design(self, 
                               factors: List[ExperimentFactor],
                               responses: List[ExperimentResponse],
                               initial_data: pd.DataFrame = None,
                               budget: int = 50,
                               strategy: str = 'expected_improvement') -> pd.DataFrame:
        """ì ì‘í˜• ì‹¤í—˜ ì„¤ê³„ ìƒì„±"""
        
        # ì´ˆê¸° ì„¤ê³„
        if initial_data is None:
            # ì´ˆê¸° ì‹¤í—˜ì  ìƒì„± (LHS ë˜ëŠ” ì‘ì€ factorial)
            n_initial = min(len(factors) * 4, budget // 3)
            initial_design = self.base_engine.generate_design(
                factors, 
                method='latin_hypercube',
                n_samples=n_initial
            )
        else:
            initial_design = initial_data
            n_initial = len(initial_data)
        
        # ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
        bounds = []
        for factor in factors:
            if not factor.categorical:
                bounds.append((factor.min_value, factor.max_value))
        
        # ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ëª¨ë¸ ìƒì„±
        kernel = Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)
        gp_model = GaussianProcessRegressor(
            kernel=kernel,
            alpha=1e-6,
            normalize_y=True,
            n_restarts_optimizer=10
        )
        
        # ìˆœì°¨ì  ì„¤ê³„
        current_design = initial_design.copy()
        remaining_budget = budget - n_initial
        
        for i in range(remaining_budget):
            # ë‹¤ìŒ ì‹¤í—˜ì  ì„ íƒ
            next_point = self._select_next_point(
                current_design,
                factors,
                gp_model,
                bounds,
                strategy
            )
            
            # ì„¤ê³„ì— ì¶”ê°€
            new_row = pd.DataFrame([next_point])
            new_row['Run'] = len(current_design) + 1
            new_row['Adaptive'] = True
            
            current_design = pd.concat([current_design, new_row], ignore_index=True)
        
        return current_design
    
    def _select_next_point(self, 
                          current_design: pd.DataFrame,
                          factors: List[ExperimentFactor],
                          gp_model: GaussianProcessRegressor,
                          bounds: List[Tuple[float, float]],
                          strategy: str) -> Dict[str, Any]:
        """ë‹¤ìŒ ì‹¤í—˜ì  ì„ íƒ"""
        
        # í˜„ì¬ ë°ì´í„°ë¡œ GP ëª¨ë¸ í•™ìŠµ (ì‹œë®¬ë ˆì´ì…˜)
        X = current_design[[f.name for f in factors if not f.categorical]].values
        
        # ê°€ìƒì˜ ë°˜ì‘ê°’ ìƒì„± (ì‹¤ì œë¡œëŠ” ì‹¤í—˜ ê²°ê³¼ ì‚¬ìš©)
        y = np.random.randn(len(X))  # ì‹¤ì œ êµ¬í˜„ì‹œ ì‹¤í—˜ ê²°ê³¼ ì‚¬ìš©
        
        if len(X) > 0:
            gp_model.fit(X, y)
        
        # íšë“ í•¨ìˆ˜ ìµœì í™”
        if strategy == 'expected_improvement':
            acquisition_func = self._expected_improvement
        elif strategy == 'upper_confidence_bound':
            acquisition_func = self._upper_confidence_bound
        else:
            acquisition_func = self._probability_of_improvement
        
        # ìµœì í™”
        result = differential_evolution(
            lambda x: -acquisition_func(x.reshape(1, -1), gp_model, y.max() if len(y) > 0 else 0),
            bounds,
            seed=42,
            maxiter=100
        )
        
        # ë‹¤ìŒ í¬ì¸íŠ¸ ìƒì„±
        next_point = {}
        continuous_idx = 0
        
        for factor in factors:
            if factor.categorical:
                # ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ëœë¤ ì„ íƒ
                next_point[factor.name] = np.random.choice(factor.categories)
            else:
                next_point[factor.name] = result.x[continuous_idx]
                continuous_idx += 1
        
        return next_point
    
    def _expected_improvement(self, X: np.ndarray, gp_model: GaussianProcessRegressor, 
                            y_best: float, xi: float = 0.01) -> np.ndarray:
        """Expected Improvement íšë“ í•¨ìˆ˜"""
        mu, sigma = gp_model.predict(X, return_std=True)
        
        with np.errstate(divide='warn'):
            Z = (mu - y_best - xi) / sigma
            ei = (mu - y_best - xi) * stats.norm.cdf(Z) + sigma * stats.norm.pdf(Z)
            ei[sigma == 0.0] = 0.0
        
        return ei
    
    def _upper_confidence_bound(self, X: np.ndarray, gp_model: GaussianProcessRegressor,
                               beta: float = 2.0) -> np.ndarray:
        """Upper Confidence Bound íšë“ í•¨ìˆ˜"""
        mu, sigma = gp_model.predict(X, return_std=True)
        return mu + beta * sigma
    
    def _probability_of_improvement(self, X: np.ndarray, gp_model: GaussianProcessRegressor,
                                  y_best: float, xi: float = 0.01) -> np.ndarray:
        """Probability of Improvement íšë“ í•¨ìˆ˜"""
        mu, sigma = gp_model.predict(X, return_std=True)
        
        with np.errstate(divide='warn'):
            Z = (mu - y_best - xi) / sigma
            pi = stats.norm.cdf(Z)
            pi[sigma == 0.0] = 0.0
        
        return pi
    
    def generate_mixture_design(self, 
                              components: List[str],
                              constraints: Dict[str, Tuple[float, float]] = None,
                              include_process_vars: List[ExperimentFactor] = None,
                              design_type: str = 'simplex_lattice',
                              degree: int = 3) -> pd.DataFrame:
        """í˜¼í•©ë¬¼ ì‹¤í—˜ ì„¤ê³„ ìƒì„±"""
        
        n_components = len(components)
        
        if design_type == 'simplex_lattice':
            # Simplex-Lattice ì„¤ê³„
            points = self._generate_simplex_lattice(n_components, degree)
        elif design_type == 'simplex_centroid':
            # Simplex-Centroid ì„¤ê³„
            points = self._generate_simplex_centroid(n_components)
        elif design_type == 'extreme_vertices':
            # Extreme Vertices ì„¤ê³„
            points = self._generate_extreme_vertices(n_components, constraints)
        else:
            # ê¸°ë³¸: ê· ë“± ë¶„í¬
            points = self._generate_uniform_mixture(n_components, 20)
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        design = pd.DataFrame(points, columns=components)
        
        # ì œì•½ ì¡°ê±´ í™•ì¸ ë° í•„í„°ë§
        if constraints:
            valid_rows = []
            for idx, row in design.iterrows():
                valid = True
                for comp, (min_val, max_val) in constraints.items():
                    if comp in row:
                        if row[comp] < min_val or row[comp] > max_val:
                            valid = False
                            break
                if valid:
                    valid_rows.append(idx)
            
            design = design.loc[valid_rows].reset_index(drop=True)
        
        # ê³µì • ë³€ìˆ˜ ì¶”ê°€
        if include_process_vars:
            process_design = self.base_engine.generate_design(
                include_process_vars,
                method='full_factorial'
            )
            
            # í˜¼í•©ë¬¼ x ê³µì • ë³€ìˆ˜ ì¡°í•©
            n_mixture = len(design)
            n_process = len(process_design)
            
            expanded_design = pd.DataFrame()
            
            for i in range(n_mixture):
                for j in range(n_process):
                    row = pd.concat([
                        design.iloc[i],
                        process_design.iloc[j].drop('Run')
                    ])
                    expanded_design = expanded_design.append(row, ignore_index=True)
            
            design = expanded_design
        
        # Run ë²ˆí˜¸ ì¶”ê°€
        design.insert(0, 'Run', range(1, len(design) + 1))
        
        return design
    
    def _generate_simplex_lattice(self, n_components: int, degree: int) -> np.ndarray:
        """Simplex-Lattice í¬ì¸íŠ¸ ìƒì„±"""
        points = []
        
        # ê° ë ˆë²¨ì—ì„œì˜ ê°€ëŠ¥í•œ ì¡°í•© ìƒì„±
        def generate_combinations(n, q, current=[]):
            if len(current) == n:
                if sum(current) == q:
                    points.append([x/q for x in current])
                return
            
            for i in range(q - sum(current) + 1):
                generate_combinations(n, q, current + [i])
        
        generate_combinations(n_components, degree, [])
        
        return np.array(points)
    
    def _generate_simplex_centroid(self, n_components: int) -> np.ndarray:
        """Simplex-Centroid í¬ì¸íŠ¸ ìƒì„±"""
        points = []
        
        # ì •ì 
        for i in range(n_components):
            point = [0] * n_components
            point[i] = 1
            points.append(point)
        
        # ëª¨ë“  ë¶€ë¶„ì§‘í•©ì˜ ì¤‘ì‹¬ì 
        from itertools import combinations
        
        for r in range(2, n_components + 1):
            for combo in combinations(range(n_components), r):
                point = [0] * n_components
                for idx in combo:
                    point[idx] = 1 / len(combo)
                points.append(point)
        
        return np.array(points)
    
    def _generate_extreme_vertices(self, n_components: int, 
                                 constraints: Dict[str, Tuple[float, float]]) -> np.ndarray:
        """Extreme Vertices ì„¤ê³„"""
        # ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê·¹ì  ì°¾ê¸°
        # ê°„ë‹¨í•œ êµ¬í˜„: ê·¸ë¦¬ë“œ ì„œì¹˜
        points = []
        
        # ê° ì„±ë¶„ì˜ ë²”ìœ„
        ranges = []
        for i in range(n_components):
            comp_name = f"Component_{i+1}"
            if comp_name in constraints:
                ranges.append(constraints[comp_name])
            else:
                ranges.append((0, 1))
        
        # ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ ìƒì„±
        n_points_per_dim = 5
        grid_points = []
        
        for min_val, max_val in ranges:
            grid_points.append(np.linspace(min_val, max_val, n_points_per_dim))
        
        # ê°€ëŠ¥í•œ ì¡°í•© ì¤‘ í•©ì´ 1ì¸ ê²ƒë§Œ ì„ íƒ
        from itertools import product
        
        for combo in product(*grid_points):
            if abs(sum(combo) - 1.0) < 0.01:  # í—ˆìš© ì˜¤ì°¨
                points.append(list(combo))
        
        return np.array(points) if points else np.array([[1/n_components] * n_components])
    
    def _generate_uniform_mixture(self, n_components: int, n_points: int) -> np.ndarray:
        """ê· ë“± í˜¼í•©ë¬¼ í¬ì¸íŠ¸ ìƒì„±"""
        points = []
        
        for _ in range(n_points):
            # Dirichlet ë¶„í¬ ì‚¬ìš©
            point = np.random.dirichlet(np.ones(n_components))
            points.append(point)
        
        return np.array(points)

# ==================== ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ====================
class MLPredictionSystem:
    """ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.model_performance = {}
        
    def train_models(self, 
                    X: pd.DataFrame, 
                    y: pd.Series,
                    model_types: List[str] = ['rf', 'gb', 'xgb', 'nn'],
                    cv_folds: int = 5) -> Dict[str, Dict[str, float]]:
        """ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        X_scaled, scaler = self._preprocess_data(X)
        self.scalers['main'] = scaler
        
        results = {}
        
        for model_type in model_types:
            logger.info(f"í•™ìŠµ ì¤‘: {model_type}")
            
            # ëª¨ë¸ ìƒì„±
            model = self._create_model(model_type, X.shape[1])
            
            # êµì°¨ ê²€ì¦
            cv_scores = cross_val_score(
                model, X_scaled, y,
                cv=cv_folds,
                scoring='r2',
                n_jobs=-1
            )
            
            # ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ
            model.fit(X_scaled, y)
            
            # ì˜ˆì¸¡ ë° í‰ê°€
            y_pred = model.predict(X_scaled)
            
            metrics = {
                'r2': r2_score(y, y_pred),
                'mse': mean_squared_error(y, y_pred),
                'mae': mean_absolute_error(y, y_pred),
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            # ëª¨ë¸ ì €ì¥
            self.models[model_type] = model
            self.model_performance[model_type] = metrics
            results[model_type] = metrics
            
            # íŠ¹ì„± ì¤‘ìš”ë„
            if hasattr(model, 'feature_importances_'):
                self.feature_importance[model_type] = dict(
                    zip(X.columns, model.feature_importances_)
                )
        
        return results
    
    def _preprocess_data(self, X: pd.DataFrame) -> Tuple[np.ndarray, StandardScaler]:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        X_encoded = pd.get_dummies(X, drop_first=True)
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_encoded)
        
        return X_scaled, scaler
    
    def _create_model(self, model_type: str, n_features: int):
        """ëª¨ë¸ ìƒì„±"""
        if model_type == 'rf':
            return RandomForestRegressor(
                n_estimators=100,
                max_depth=None,
                min_samples_split=2,
                min_samples_leaf=1,
                random_state=42,
                n_jobs=-1
            )
        
        elif model_type == 'gb':
            return GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=3,
                random_state=42
            )
        
        elif model_type == 'xgb':
            return xgb.XGBRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=3,
                random_state=42,
                n_jobs=-1
            )
        
        elif model_type == 'nn':
            return MLPRegressor(
                hidden_layer_sizes=(n_features * 2, n_features),
                activation='relu',
                solver='adam',
                alpha=0.0001,
                max_iter=1000,
                random_state=42
            )
        
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    def predict(self, X: pd.DataFrame, model_type: str = None, 
               return_uncertainty: bool = False) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        
        if model_type is None:
            # ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ ì„ íƒ
            model_type = max(self.model_performance.items(), 
                           key=lambda x: x[1]['cv_mean'])[0]
        
        if model_type not in self.models:
            raise ValueError(f"Model {model_type} not trained")
        
        # ì „ì²˜ë¦¬
        X_encoded = pd.get_dummies(X, drop_first=True)
        X_scaled = self.scalers['main'].transform(X_encoded)
        
        model = self.models[model_type]
        
        if return_uncertainty:
            if model_type == 'rf':
                # Random Forestì˜ ê²½ìš° ê°œë³„ íŠ¸ë¦¬ ì˜ˆì¸¡ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
                predictions = []
                for tree in model.estimators_:
                    predictions.append(tree.predict(X_scaled))
                
                predictions = np.array(predictions)
                mean_pred = predictions.mean(axis=0)
                std_pred = predictions.std(axis=0)
                
                return mean_pred, std_pred
            else:
                # ë‹¤ë¥¸ ëª¨ë¸ì˜ ê²½ìš° ê¸°ë³¸ ì˜ˆì¸¡ë§Œ
                pred = model.predict(X_scaled)
                return pred, np.zeros_like(pred)
        else:
            return model.predict(X_scaled)
    
    def explain_predictions(self, X: pd.DataFrame, model_type: str = None) -> pd.DataFrame:
        """ì˜ˆì¸¡ ì„¤ëª… (SHAP values ê³„ì‚°)"""
        try:
            import shap
            
            if model_type is None:
                model_type = max(self.model_performance.items(), 
                               key=lambda x: x[1]['cv_mean'])[0]
            
            model = self.models[model_type]
            X_encoded = pd.get_dummies(X, drop_first=True)
            X_scaled = self.scalers['main'].transform(X_encoded)
            
            # SHAP ì„¤ëª…ì ìƒì„±
            if model_type in ['rf', 'gb', 'xgb']:
                explainer = shap.TreeExplainer(model)
            else:
                explainer = shap.KernelExplainer(model.predict, X_scaled[:100])
            
            # SHAP values ê³„ì‚°
            shap_values = explainer.shap_values(X_scaled)
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            shap_df = pd.DataFrame(
                shap_values,
                columns=X_encoded.columns,
                index=X.index
            )
            
            return shap_df
            
        except ImportError:
            logger.warning("SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
    
    def optimize_hyperparameters(self, X: pd.DataFrame, y: pd.Series, 
                               model_type: str, n_trials: int = 50) -> Dict[str, Any]:
        """ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        try:
            import optuna
            
            X_scaled, _ = self._preprocess_data(X)
            
            def objective(trial):
                # ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜
                if model_type == 'rf':
                    params = {
                        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                        'max_depth': trial.suggest_int('max_depth', 3, 20),
                        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
                    }
                    model = RandomForestRegressor(**params, random_state=42)
                
                elif model_type == 'xgb':
                    params = {
                        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                        'max_depth': trial.suggest_int('max_depth', 3, 10),
                        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)
                    }
                    model = xgb.XGBRegressor(**params, random_state=42)
                
                else:
                    return 0
                
                # êµì°¨ ê²€ì¦
                scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
                return scores.mean()
            
            # ìµœì í™” ì‹¤í–‰
            study = optuna.create_study(direction='maximize')
            study.optimize(objective, n_trials=n_trials)
            
            return {
                'best_params': study.best_params,
                'best_score': study.best_value,
                'optimization_history': study.trials_dataframe()
            }
            
        except ImportError:
            logger.warning("Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}

# ==================== í†µê³„ ë¶„ì„ ì—”ì§„ (í™•ì¥) ====================
class AdvancedStatisticalAnalyzer:
    """ê³ ê¸‰ í†µê³„ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self):
        self.basic_analyzer = StatisticalAnalyzer()
        self.results_cache = {}
        
    def comprehensive_analysis(self, 
                             design: pd.DataFrame, 
                             results: pd.DataFrame,
                             responses: List[ExperimentResponse],
                             alpha: float = 0.05) -> Dict[str, Any]:
        """ì¢…í•© í†µê³„ ë¶„ì„"""
        
        analysis_results = {
            'timestamp': datetime.now(),
            'design_info': self._analyze_design_properties(design),
            'descriptive': {},
            'inferential': {},
            'regression': {},
            'diagnostics': {},
            'recommendations': []
        }
        
        # ê° ë°˜ì‘ ë³€ìˆ˜ë³„ ë¶„ì„
        for response in responses:
            if response.name not in results.columns:
                continue
            
            response_data = results[response.name].dropna()
            
            # 1. ê¸°ìˆ  í†µê³„
            analysis_results['descriptive'][response.name] = self._enhanced_descriptive_stats(
                response_data, response
            )
            
            # 2. ì •ê·œì„± ë° ë¶„í¬ ê²€ì •
            analysis_results['diagnostics'][response.name] = self._distribution_tests(
                response_data
            )
            
            # 3. ANOVA ë° íš¨ê³¼ ë¶„ì„
            analysis_results['inferential'][response.name] = self._comprehensive_anova(
                design, response_data, alpha
            )
            
            # 4. íšŒê·€ ë¶„ì„
            analysis_results['regression'][response.name] = self._advanced_regression(
                design, response_data, response
            )
            
            # 5. ìµœì í™” ê¶Œì¥ì‚¬í•­
            recommendations = self._generate_recommendations(
                analysis_results, response
            )
            analysis_results['recommendations'].extend(recommendations)
        
        # ë‹¤ì¤‘ ë°˜ì‘ ë¶„ì„
        if len(responses) > 1:
            analysis_results['multi_response'] = self._multi_response_analysis(
                design, results, responses
            )
        
        return analysis_results
    
    def _analyze_design_properties(self, design: pd.DataFrame) -> Dict[str, Any]:
        """ì„¤ê³„ íŠ¹ì„± ë¶„ì„"""
        factor_cols = [col for col in design.columns if col not in ['Run', 'Block', 'Adaptive']]
        
        properties = {
            'n_runs': len(design),
            'n_factors': len(factor_cols),
            'factors': factor_cols,
            'design_type': self._identify_design_type(design),
            'balance': self._check_balance(design, factor_cols),
            'orthogonality': self._check_orthogonality(design, factor_cols),
            'power': self._calculate_statistical_power(design)
        }
        
        return properties
    
    def _identify_design_type(self, design: pd.DataFrame) -> str:
        """ì„¤ê³„ ìœ í˜• ì‹ë³„"""
        n_runs = len(design)
        factor_cols = [col for col in design.columns if col not in ['Run', 'Block', 'Adaptive']]
        n_factors = len(factor_cols)
        
        # ê° ì¸ìì˜ ìˆ˜ì¤€ ìˆ˜ í™•ì¸
        levels = []
        for col in factor_cols:
            levels.append(len(design[col].unique()))
        
        # ì™„ì „ ìš”ì¸ ì„¤ê³„ í™•ì¸
        expected_full = np.prod(levels)
        if n_runs == expected_full:
            return "Full Factorial"
        
        # ë¶€ë¶„ ìš”ì¸ ì„¤ê³„ í™•ì¸
        if all(l == 2 for l in levels) and n_runs < expected_full:
            resolution = self._estimate_resolution(design, factor_cols)
            return f"Fractional Factorial (Resolution {resolution})"
        
        # ì¤‘ì‹¬ì  í¬í•¨ í™•ì¸
        center_points = []
        for col in factor_cols:
            if design[col].dtype in ['float64', 'int64']:
                mid_value = (design[col].min() + design[col].max()) / 2
                center_points.append(len(design[design[col] == mid_value]))
        
        if min(center_points) >= 3:
            if n_runs == 2**n_factors + 2*n_factors + min(center_points):
                return "Central Composite Design"
            elif self._is_box_behnken(design, factor_cols):
                return "Box-Behnken Design"
        
        # Plackett-Burman í™•ì¸
        pb_runs = [4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48]
        if n_runs in pb_runs and all(l == 2 for l in levels):
            return "Plackett-Burman Design"
        
        # ê¸°íƒ€
        if 'Adaptive' in design.columns and design['Adaptive'].any():
            return "Adaptive/Sequential Design"
        
        return "Custom Design"
    
    def _check_balance(self, design: pd.DataFrame, factor_cols: List[str]) -> Dict[str, bool]:
        """ê· í˜•ì„± í™•ì¸"""
        balance_info = {}
        
        for col in factor_cols:
            value_counts = design[col].value_counts()
            is_balanced = value_counts.std() / value_counts.mean() < 0.1 if len(value_counts) > 1 else True
            balance_info[col] = is_balanced
        
        balance_info['overall'] = all(balance_info.values())
        return balance_info
    
    def _check_orthogonality(self, design: pd.DataFrame, factor_cols: List[str]) -> float:
        """ì§êµì„± í™•ì¸"""
        # ìˆ˜ì¹˜í˜• ì¸ìë§Œ ì„ íƒ
        numeric_cols = [col for col in factor_cols if design[col].dtype in ['float64', 'int64']]
        
        if len(numeric_cols) < 2:
            return 1.0
        
        # ì½”ë“œí™” (-1, 1)
        coded_design = design[numeric_cols].copy()
        for col in numeric_cols:
            min_val = coded_design[col].min()
            max_val = coded_design[col].max()
            if max_val > min_val:
                coded_design[col] = 2 * (coded_design[col] - min_val) / (max_val - min_val) - 1
        
        # ìƒê´€ í–‰ë ¬
        corr_matrix = coded_design.corr()
        
        # ë¹„ëŒ€ê° ì›ì†Œì˜ í‰ê·  ì ˆëŒ€ê°’
        n = len(corr_matrix)
        off_diagonal_sum = 0
        count = 0
        
        for i in range(n):
            for j in range(i + 1, n):
                off_diagonal_sum += abs(corr_matrix.iloc[i, j])
                count += 1
        
        orthogonality = 1 - (off_diagonal_sum / count) if count > 0 else 1.0
        return orthogonality
    
    def _calculate_statistical_power(self, design: pd.DataFrame) -> Dict[str, float]:
        """í†µê³„ì  ê²€ì •ë ¥ ê³„ì‚°"""
        n = len(design)
        factor_cols = [col for col in design.columns if col not in ['Run', 'Block', 'Adaptive']]
        k = len(factor_cols)
        
        # ì£¼íš¨ê³¼ ê²€ì •ë ¥
        effect_size = 0.25  # Cohen's f
        alpha = 0.05
        
        # ë¹„ì¤‘ì‹¬ ëª¨ìˆ˜
        lambda_main = n * effect_size**2
        
        # F ë¶„í¬ ì„ê³„ê°’
        df1_main = k
        df2_main = n - k - 1
        
        power_results = {}
        
        if df2_main > 0:
            f_crit_main = stats.f.ppf(1 - alpha, df1_main, df2_main)
            power_main = 1 - stats.ncf.cdf(f_crit_main, df1_main, df2_main, lambda_main)
            power_results['main_effects'] = power_main
        
        # 2ì°¨ ìƒí˜¸ì‘ìš© ê²€ì •ë ¥
        if k >= 2:
            df1_int = k * (k - 1) // 2
            df2_int = n - df1_int - k - 1
            
            if df2_int > 0:
                lambda_int = n * (effect_size/2)**2  # ìƒí˜¸ì‘ìš©ì€ ì£¼íš¨ê³¼ì˜ ì ˆë°˜ìœ¼ë¡œ ê°€ì •
                f_crit_int = stats.f.ppf(1 - alpha, df1_int, df2_int)
                power_int = 1 - stats.ncf.cdf(f_crit_int, df1_int, df2_int, lambda_int)
                power_results['interactions'] = power_int
        
        return power_results
    
    def _enhanced_descriptive_stats(self, data: pd.Series, 
                                  response: ExperimentResponse) -> Dict[str, Any]:
        """í–¥ìƒëœ ê¸°ìˆ  í†µê³„"""
        stats_dict = {
            'count': len(data),
            'mean': data.mean(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max(),
            'range': data.max() - data.min(),
            'cv': (data.std() / data.mean() * 100) if data.mean() != 0 else np.inf,
            'q1': data.quantile(0.25),
            'median': data.median(),
            'q3': data.quantile(0.75),
            'iqr': data.quantile(0.75) - data.quantile(0.25),
            'skewness': stats.skew(data),
            'kurtosis': stats.kurtosis(data),
            'outliers': self._detect_outliers(data)
        }
        
        # ëª©í‘œê°’ê³¼ì˜ ë¹„êµ
        if response.target_value is not None:
            stats_dict['target_deviation'] = {
                'mean_deviation': abs(data.mean() - response.target_value),
                'percent_in_spec': self._calculate_in_spec_percentage(data, response),
                'process_capability': self._calculate_capability_indices(data, response)
            }
        
        # ì‹ ë¢°êµ¬ê°„
        confidence_level = 0.95
        stats_dict['confidence_interval'] = stats.t.interval(
            confidence_level,
            len(data) - 1,
            loc=data.mean(),
            scale=stats.sem(data)
        )
        
        return stats_dict
    
    def _detect_outliers(self, data: pd.Series) -> Dict[str, List[float]]:
        """ì´ìƒì¹˜ ê²€ì¶œ"""
        outliers = {}
        
        # IQR ë°©ë²•
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        iqr_outliers = data[(data < lower_bound) | (data > upper_bound)].tolist()
        outliers['iqr_method'] = iqr_outliers
        
        # Z-score ë°©ë²•
        z_scores = np.abs(stats.zscore(data))
        z_outliers = data[z_scores > 3].tolist()
        outliers['z_score_method'] = z_outliers
        
        # Modified Z-score (MAD ê¸°ë°˜)
        median = data.median()
        mad = np.median(np.abs(data - median))
        modified_z_scores = 0.6745 * (data - median) / mad if mad > 0 else np.zeros_like(data)
        mad_outliers = data[np.abs(modified_z_scores) > 3.5].tolist()
        outliers['mad_method'] = mad_outliers
        
        # Isolation Forest
        try:
            from sklearn.ensemble import IsolationForest
            
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            outlier_labels = iso_forest.fit_predict(data.values.reshape(-1, 1))
            iso_outliers = data[outlier_labels == -1].tolist()
            outliers['isolation_forest'] = iso_outliers
        except:
            pass
        
        return outliers
    
    def _calculate_in_spec_percentage(self, data: pd.Series, 
                                    response: ExperimentResponse) -> float:
        """ê·œê²© ë‚´ ë¹„ìœ¨ ê³„ì‚°"""
        lower, upper = response.specification_limits
        
        if lower is None and upper is None:
            return 100.0
        
        in_spec = data.copy()
        
        if lower is not None:
            in_spec = in_spec[in_spec >= lower]
        
        if upper is not None:
            in_spec = in_spec[in_spec <= upper]
        
        return len(in_spec) / len(data) * 100
    
    def _calculate_capability_indices(self, data: pd.Series, 
                                    response: ExperimentResponse) -> Dict[str, float]:
        """ê³µì •ëŠ¥ë ¥ì§€ìˆ˜ ê³„ì‚°"""
        lower, upper = response.specification_limits
        
        if lower is None and upper is None:
            return {}
        
        indices = {}
        
        # ê¸°ë³¸ í†µê³„
        mean = data.mean()
        std = data.std()
        
        if std == 0:
            return {'error': 'Standard deviation is zero'}
        
        # Cp (ì ì¬ ëŠ¥ë ¥)
        if lower is not None and upper is not None:
            cp = (upper - lower) / (6 * std)
            indices['Cp'] = cp
        
        # Cpk (ì‹¤ì œ ëŠ¥ë ¥)
        if lower is not None and upper is not None:
            cpu = (upper - mean) / (3 * std)
            cpl = (mean - lower) / (3 * std)
            cpk = min(cpu, cpl)
            indices['Cpk'] = cpk
            indices['Cpu'] = cpu
            indices['Cpl'] = cpl
        elif lower is not None:
            cpl = (mean - lower) / (3 * std)
            indices['Cpl'] = cpl
        elif upper is not None:
            cpu = (upper - mean) / (3 * std)
            indices['Cpu'] = cpu
        
        # Cpm (ëª©í‘œê°’ ê³ ë ¤)
        if response.target_value is not None:
            target = response.target_value
            
            if lower is not None and upper is not None:
                tau = np.sqrt(std**2 + (mean - target)**2)
                cpm = (upper - lower) / (6 * tau)
                indices['Cpm'] = cpm
        
        # ì˜ˆìƒ ë¶ˆëŸ‰ë¥  (ppm)
        if lower is not None and upper is not None:
            z_lower = (lower - mean) / std
            z_upper = (upper - mean) / std
            
            p_lower = stats.norm.cdf(z_lower)
            p_upper = 1 - stats.norm.cdf(z_upper)
            
            ppm = (p_lower + p_upper) * 1e6
            indices['expected_ppm'] = ppm
        
        return indices
    
    def _distribution_tests(self, data: pd.Series) -> Dict[str, Any]:
        """ë¶„í¬ ê²€ì •"""
        tests = {}
        
        # ì •ê·œì„± ê²€ì •
        # Shapiro-Wilk
        if len(data) >= 3:
            shapiro_stat, shapiro_p = stats.shapiro(data)
            tests['shapiro_wilk'] = {
                'statistic': shapiro_stat,
                'p_value': shapiro_p,
                'normal': shapiro_p > 0.05
            }
        
        # Anderson-Darling
        if len(data) >= 7:
            anderson_result = stats.anderson(data)
            tests['anderson_darling'] = {
                'statistic': anderson_result.statistic,
                'critical_values': dict(zip(
                    anderson_result.significance_level,
                    anderson_result.critical_values
                )),
                'normal': anderson_result.statistic < anderson_result.critical_values[2]  # 5% level
            }
        
        # Kolmogorov-Smirnov
        ks_stat, ks_p = stats.kstest(data, 'norm', args=(data.mean(), data.std()))
        tests['kolmogorov_smirnov'] = {
            'statistic': ks_stat,
            'p_value': ks_p,
            'normal': ks_p > 0.05
        }
        
        # ë‹¤ë¥¸ ë¶„í¬ ì í•©ë„ ê²€ì •
        distributions = {
            'exponential': stats.expon,
            'lognormal': stats.lognorm,
            'weibull': stats.weibull_min,
            'gamma': stats.gamma
        }
        
        best_fit = {'distribution': 'normal', 'aic': float('inf')}
        
        for dist_name, dist_func in distributions.items():
            try:
                # íŒŒë¼ë¯¸í„° ì¶”ì •
                params = dist_func.fit(data)
                
                # Log-likelihood
                log_likelihood = np.sum(dist_func.logpdf(data, *params))
                
                # AIC
                k = len(params)
                aic = 2 * k - 2 * log_likelihood
                
                if aic < best_fit['aic']:
                    best_fit = {
                        'distribution': dist_name,
                        'aic': aic,
                        'parameters': params
                    }
            except:
                continue
        
        tests['best_fit_distribution'] = best_fit
        
        return tests
    
    def _comprehensive_anova(self, design: pd.DataFrame, response_data: pd.Series, 
                           alpha: float = 0.05) -> Dict[str, Any]:
        """ì¢…í•© ANOVA ë¶„ì„"""
        results = {
            'main_effects': {},
            'interactions': {},
            'model_adequacy': {},
            'post_hoc': {}
        }
        
        # ì¸ì ì‹ë³„
        factor_cols = [col for col in design.columns 
                      if col not in ['Run', 'Block', 'Adaptive'] and col in design.columns]
        
        # ê° ì¸ìë³„ ì£¼íš¨ê³¼
        for factor in factor_cols:
            groups = []
            levels = design[factor].unique()
            
            for level in levels:
                mask = design[factor] == level
                if mask.sum() > 0:
                    groups.append(response_data[mask].values)
            
            if len(groups) >= 2:
                # One-way ANOVA
                f_stat, p_value = stats.f_oneway(*groups)
                
                # íš¨ê³¼ í¬ê¸°
                ss_between = sum(len(g) * (np.mean(g) - response_data.mean())**2 for g in groups)
                ss_total = sum((response_data - response_data.mean())**2)
                eta_squared = ss_between / ss_total if ss_total > 0 else 0
                
                # ê²€ì •ë ¥
                effect_size = np.sqrt(eta_squared / (1 - eta_squared)) if eta_squared < 1 else np.inf
                df1 = len(groups) - 1
                df2 = len(response_data) - len(groups)
                
                if df2 > 0:
                    lambda_nc = len(response_data) * effect_size**2
                    f_crit = stats.f.ppf(1 - alpha, df1, df2)
                    power = 1 - stats.ncf.cdf(f_crit, df1, df2, lambda_nc)
                else:
                    power = 0
                
                results['main_effects'][factor] = {
                    'f_statistic': f_stat,
                    'p_value': p_value,
                    'significant': p_value < alpha,
                    'eta_squared': eta_squared,
                    'power': power,
                    'levels': list(levels),
                    'means': {str(level): np.mean(groups[i]) for i, level in enumerate(levels)}
                }
                
                # ì‚¬í›„ ê²€ì • (Tukey HSD)
                if p_value < alpha and len(groups) > 2:
                    results['post_hoc'][factor] = self._tukey_hsd(groups, levels, alpha)
        
        # 2ì°¨ ìƒí˜¸ì‘ìš©
        if len(factor_cols) >= 2:
            from itertools import combinations
            
            for f1, f2 in combinations(factor_cols, 2):
                interaction_key = f"{f1}*{f2}"
                
                # 2-way ANOVAë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
                try:
                    interaction_results = self._two_way_anova(
                        design, response_data, f1, f2, alpha
                    )
                    
                    if interaction_results:
                        results['interactions'][interaction_key] = interaction_results
                except:
                    continue
        
        # ëª¨ë¸ ì í•©ë„
        results['model_adequacy'] = self._assess_model_adequacy(
            design, response_data, factor_cols
        )
        
        return results
    
    def _tukey_hsd(self, groups: List[np.ndarray], levels: List[Any], 
                  alpha: float = 0.05) -> List[Dict[str, Any]]:
        """Tukey HSD ì‚¬í›„ ê²€ì •"""
        from statsmodels.stats.multicomp import pairwise_tukeyhsd
        
        # ë°ì´í„° ì¤€ë¹„
        data_list = []
        group_list = []
        
        for i, (group, level) in enumerate(zip(groups, levels)):
            data_list.extend(group)
            group_list.extend([str(level)] * len(group))
        
        # Tukey HSD
        tukey_result = pairwise_tukeyhsd(data_list, group_list, alpha=alpha)
        
        # ê²°ê³¼ ì •ë¦¬
        comparisons = []
        for i in range(len(tukey_result.reject)):
            comparisons.append({
                'group1': tukey_result.groupsunique[tukey_result._results_table[i+1][0]],
                'group2': tukey_result.groupsunique[tukey_result._results_table[i+1][1]],
                'mean_diff': tukey_result._results_table[i+1][2],
                'p_adj': tukey_result._results_table[i+1][5],
                'reject': tukey_result.reject[i]
            })
        
        return comparisons
    
    def _two_way_anova(self, design: pd.DataFrame, response_data: pd.Series,
                      factor1: str, factor2: str, alpha: float = 0.05) -> Dict[str, Any]:
        """ì´ì› ë¶„ì‚°ë¶„ì„"""
        import statsmodels.api as sm
        from statsmodels.formula.api import ols
        
        # ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„
        anova_df = design[[factor1, factor2]].copy()
        anova_df['response'] = response_data
        
        # ëª¨ë¸ ì í•©
        formula = f'response ~ C({factor1}) + C({factor2}) + C({factor1}):C({factor2})'
        model = ols(formula, data=anova_df).fit()
        
        # ANOVA í…Œì´ë¸”
        anova_table = sm.stats.anova_lm(model, typ=2)
        
        # ê²°ê³¼ ì •ë¦¬
        interaction_row = f'C({factor1}):C({factor2})'
        
        if interaction_row in anova_table.index:
            return {
                'f_statistic': anova_table.loc[interaction_row, 'F'],
                'p_value': anova_table.loc[interaction_row, 'PR(>F)'],
                'significant': anova_table.loc[interaction_row, 'PR(>F)'] < alpha,
                'sum_sq': anova_table.loc[interaction_row, 'sum_sq'],
                'mean_sq': anova_table.loc[interaction_row, 'mean_sq']
            }
        
        return None
    
    def _assess_model_adequacy(self, design: pd.DataFrame, response_data: pd.Series,
                              factor_cols: List[str]) -> Dict[str, Any]:
        """ëª¨ë¸ ì í•©ë„ í‰ê°€"""
        from sklearn.linear_model import LinearRegression
        
        # ì„¤ê³„ í–‰ë ¬ ì¤€ë¹„
        X = pd.get_dummies(design[factor_cols], drop_first=True)
        y = response_data.values
        
        # ì„ í˜• ëª¨ë¸ ì í•©
        model = LinearRegression()
        model.fit(X, y)
        
        # ì˜ˆì¸¡ê°’ê³¼ ì”ì°¨
        y_pred = model.predict(X)
        residuals = y - y_pred
        
        # ì í•©ë„ ì§€í‘œ
        r_squared = r2_score(y, y_pred)
        adj_r_squared = 1 - (1 - r_squared) * (len(y) - 1) / (len(y) - X.shape[1] - 1)
        
        # ì”ì°¨ ë¶„ì„
        residual_analysis = {
            'mean': residuals.mean(),
            'std': residuals.std(),
            'normality': stats.shapiro(residuals)[1] if len(residuals) >= 3 else None,
            'homoscedasticity': self._breusch_pagan_test(X, residuals),
            'autocorrelation': self._durbin_watson(residuals)
        }
        
        # Lack of Fit ê²€ì •
        lack_of_fit = self._lack_of_fit_test(design, response_data, factor_cols)
        
        return {
            'r_squared': r_squared,
            'adjusted_r_squared': adj_r_squared,
            'residual_analysis': residual_analysis,
            'lack_of_fit': lack_of_fit
        }
    
    def _breusch_pagan_test(self, X: pd.DataFrame, residuals: np.ndarray) -> Dict[str, float]:
        """Breusch-Pagan ì´ë¶„ì‚°ì„± ê²€ì •"""
        # ì”ì°¨ ì œê³±
        residuals_squared = residuals ** 2
        
        # ë³´ì¡° íšŒê·€
        aux_model = LinearRegression()
        aux_model.fit(X, residuals_squared)
        aux_pred = aux_model.predict(X)
        
        # LM í†µê³„ëŸ‰
        n = len(residuals)
        rss = np.sum((residuals_squared - aux_pred) ** 2)
        tss = np.sum((residuals_squared - residuals_squared.mean()) ** 2)
        r_squared_aux = 1 - rss / tss if tss > 0 else 0
        
        lm_statistic = n * r_squared_aux
        p_value = 1 - stats.chi2.cdf(lm_statistic, X.shape[1])
        
        return {
            'statistic': lm_statistic,
            'p_value': p_value,
            'homoscedastic': p_value > 0.05
        }
    
    def _durbin_watson(self, residuals: np.ndarray) -> float:
        """Durbin-Watson ìê¸°ìƒê´€ ê²€ì •"""
        diff = np.diff(residuals)
        dw = np.sum(diff ** 2) / np.sum(residuals ** 2)
        return dw
    
    def _lack_of_fit_test(self, design: pd.DataFrame, response_data: pd.Series,
                         factor_cols: List[str]) -> Dict[str, Any]:
        """ì í•©ê²°ì—¬ ê²€ì •"""
        # ë°˜ë³µì‹¤í—˜ ì°¾ê¸°
        design_with_response = design[factor_cols].copy()
        design_with_response['response'] = response_data
        
        # ê·¸ë£¹í™”
        grouped = design_with_response.groupby(factor_cols)
        
        pure_error_ss = 0
        pure_error_df = 0
        
        for name, group in grouped:
            if len(group) > 1:
                group_mean = group['response'].mean()
                pure_error_ss += np.sum((group['response'] - group_mean) ** 2)
                pure_error_df += len(group) - 1
        
        if pure_error_df == 0:
            return {'test_possible': False, 'reason': 'No replicates found'}
        
        # ì „ì²´ ì˜¤ì°¨
        total_mean = response_data.mean()
        total_ss = np.sum((response_data - total_mean) ** 2)
        
        # ëª¨ë¸ ì í•©
        X = pd.get_dummies(design[factor_cols], drop_first=True)
        model = LinearRegression()
        model.fit(X, response_data)
        y_pred = model.predict(X)
        
        residual_ss = np.sum((response_data - y_pred) ** 2)
        
        # Lack of fit
        lack_of_fit_ss = residual_ss - pure_error_ss
        lack_of_fit_df = len(response_data) - X.shape[1] - 1 - pure_error_df
        
        if lack_of_fit_df <= 0:
            return {'test_possible': False, 'reason': 'Insufficient degrees of freedom'}
        
        # F ê²€ì •
        f_statistic = (lack_of_fit_ss / lack_of_fit_df) / (pure_error_ss / pure_error_df)
        p_value = 1 - stats.f.cdf(f_statistic, lack_of_fit_df, pure_error_df)
        
        return {
            'test_possible': True,
            'f_statistic': f_statistic,
            'p_value': p_value,
            'adequate_fit': p_value > 0.05,
            'pure_error_df': pure_error_df,
            'lack_of_fit_df': lack_of_fit_df
        }
    
    def _advanced_regression(self, design: pd.DataFrame, response_data: pd.Series,
                           response: ExperimentResponse) -> Dict[str, Any]:
        """ê³ ê¸‰ íšŒê·€ ë¶„ì„"""
        factor_cols = [col for col in design.columns 
                      if col not in ['Run', 'Block', 'Adaptive']]
        
        # ë‹¤í•­ì‹ ì°¨ìˆ˜ ê²°ì •
        poly_degree = self._determine_polynomial_degree(design, response_data, factor_cols)
        
        # ëª¨ë¸ êµ¬ì¶•
        from sklearn.preprocessing import PolynomialFeatures
        
        X = design[factor_cols]
        X_encoded = pd.get_dummies(X, drop_first=True)
        
        # ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„±
        if poly_degree > 1:
            poly = PolynomialFeatures(degree=poly_degree, include_bias=False)
            X_poly = poly.fit_transform(X_encoded)
            feature_names = poly.get_feature_names_out(X_encoded.columns)
        else:
            X_poly = X_encoded.values
            feature_names = X_encoded.columns.tolist()
        
        # ì—¬ëŸ¬ íšŒê·€ ë°©ë²• ë¹„êµ
        models = {
            'linear': LinearRegression(),
            'ridge': Ridge(alpha=1.0),
            'lasso': Lasso(alpha=0.1),
            'elastic_net': ElasticNet(alpha=0.1, l1_ratio=0.5)
        }
        
        best_model = None
        best_score = -np.inf
        model_results = {}
        
        for name, model in models.items():
            try:
                # êµì°¨ ê²€ì¦
                cv_scores = cross_val_score(model, X_poly, response_data, cv=5, scoring='r2')
                
                # ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ
                model.fit(X_poly, response_data)
                y_pred = model.predict(X_poly)
                
                # í‰ê°€
                r2 = r2_score(response_data, y_pred)
                
                model_results[name] = {
                    'r2': r2,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'coefficients': dict(zip(feature_names, model.coef_)) if hasattr(model, 'coef_') else {}
                }
                
                if cv_scores.mean() > best_score:
                    best_score = cv_scores.mean()
                    best_model = name
                    
            except:
                continue
        
        # ìµœì¢… ëª¨ë¸ë¡œ ìƒì„¸ ë¶„ì„
        if best_model:
            final_model = models[best_model]
            final_model.fit(X_poly, response_data)
            
            # ë°˜ì‘í‘œë©´ ë°©ì •ì‹
            equation = self._generate_regression_equation(
                final_model, feature_names, poly_degree
            )
            
            # ìµœì  ì¡°ê±´ ì˜ˆì¸¡
            optimal_conditions = self._find_optimal_conditions(
                final_model, X_encoded, response, poly
            )
            
            model_results['best_model'] = best_model
            model_results['equation'] = equation
            model_results['optimal_conditions'] = optimal_conditions
        
        return model_results
    
    def _determine_polynomial_degree(self, design: pd.DataFrame, response_data: pd.Series,
                                   factor_cols: List[str]) -> int:
        """ì ì ˆí•œ ë‹¤í•­ì‹ ì°¨ìˆ˜ ê²°ì •"""
        n_samples = len(design)
        n_factors = len(factor_cols)
        
        # íœ´ë¦¬ìŠ¤í‹± ê·œì¹™
        if n_samples < 10:
            return 1
        elif n_samples < 20:
            return min(2, n_factors)
        else:
            # AIC ê¸°ë°˜ ì„ íƒ
            best_aic = np.inf
            best_degree = 1
            
            for degree in range(1, min(4, n_factors + 1)):
                try:
                    from sklearn.preprocessing import PolynomialFeatures
                    
                    X = pd.get_dummies(design[factor_cols], drop_first=True)
                    poly = PolynomialFeatures(degree=degree, include_bias=False)
                    X_poly = poly.fit_transform(X)
                    
                    if X_poly.shape[1] >= n_samples:
                        break
                    
                    model = LinearRegression()
                    model.fit(X_poly, response_data)
                    y_pred = model.predict(X_poly)
                    
                    # AIC ê³„ì‚°
                    rss = np.sum((response_data - y_pred) ** 2)
                    k = X_poly.shape[1] + 1  # íŒŒë¼ë¯¸í„° ìˆ˜
                    
                    if n_samples > k + 1:
                        aic = n_samples * np.log(rss / n_samples) + 2 * k
                        
                        if aic < best_aic:
                            best_aic = aic
                            best_degree = degree
                except:
                    break
            
            return best_degree
    
    def _generate_regression_equation(self, model, feature_names: List[str], 
                                    degree: int) -> str:
        """íšŒê·€ ë°©ì •ì‹ ìƒì„±"""
        equation_parts = []
        
        # ì ˆí¸
        if hasattr(model, 'intercept_'):
            equation_parts.append(f"{model.intercept_:.4f}")
        
        # ê³„ìˆ˜
        if hasattr(model, 'coef_'):
            for feature, coef in zip(feature_names, model.coef_):
                if abs(coef) > 1e-6:
                    sign = "+" if coef > 0 else "-"
                    
                    if equation_parts or sign == "-":
                        equation_parts.append(f" {sign} {abs(coef):.4f}*{feature}")
                    else:
                        equation_parts.append(f"{coef:.4f}*{feature}")
        
        return "Y = " + "".join(equation_parts)
    
    def _find_optimal_conditions(self, model, X_encoded: pd.DataFrame, 
                               response: ExperimentResponse,
                               poly_transformer=None) -> Dict[str, Any]:
        """ìµœì  ì¡°ê±´ ì°¾ê¸°"""
        bounds = []
        for col in X_encoded.columns:
            if X_encoded[col].dtype in ['float64', 'int64']:
                bounds.append((X_encoded[col].min(), X_encoded[col].max()))
            else:
                bounds.append((0, 1))  # ë”ë¯¸ ë³€ìˆ˜
        
        # ëª©ì  í•¨ìˆ˜
        def objective(x):
            if poly_transformer:
                x_poly = poly_transformer.transform(x.reshape(1, -1))
            else:
                x_poly = x.reshape(1, -1)
            
            pred = model.predict(x_poly)[0]
            
            # ìµœì í™” ë°©í–¥
            if response.maximize:
                return -pred
            elif response.minimize:
                return pred
            elif response.target_value is not None:
                return abs(pred - response.target_value)
            else:
                return pred
        
        # ìµœì í™”
        result = differential_evolution(objective, bounds, seed=42, maxiter=1000)
        
        # ìµœì  ì¡°ê±´
        optimal_x = result.x
        if poly_transformer:
            optimal_pred = model.predict(poly_transformer.transform(optimal_x.reshape(1, -1)))[0]
        else:
            optimal_pred = model.predict(optimal_x.reshape(1, -1))[0]
        
        optimal_conditions = dict(zip(X_encoded.columns, optimal_x))
        
        return {
            'conditions': optimal_conditions,
            'predicted_value': optimal_pred,
            'optimization_success': result.success,
            'iterations': result.nit
        }
    
    def _multi_response_analysis(self, design: pd.DataFrame, results: pd.DataFrame,
                               responses: List[ExperimentResponse]) -> Dict[str, Any]:
        """ë‹¤ì¤‘ ë°˜ì‘ ë¶„ì„"""
        multi_results = {
            'correlation_matrix': {},
            'desirability': {},
            'pareto_optimal': [],
            'compromise_solution': {}
        }
        
        # ìƒê´€ í–‰ë ¬
        response_names = [r.name for r in responses if r.name in results.columns]
        if len(response_names) >= 2:
            corr_matrix = results[response_names].corr()
            multi_results['correlation_matrix'] = corr_matrix.to_dict()
        
        # ì¢…í•© ë°”ëŒì§í•¨ ì§€ìˆ˜
        desirability_scores = []
        
        for idx, row in results.iterrows():
            individual_desirabilities = []
            
            for response in responses:
                if response.name in row:
                    d = response.calculate_desirability(row[response.name])
                    individual_desirabilities.append(d * response.weight)
            
            if individual_desirabilities:
                # ê¸°í•˜í‰ê· 
                overall_desirability = np.prod(individual_desirabilities) ** (1/len(individual_desirabilities))
                desirability_scores.append(overall_desirability)
            else:
                desirability_scores.append(0)
        
        multi_results['desirability']['scores'] = desirability_scores
        multi_results['desirability']['best_run'] = int(np.argmax(desirability_scores))
        multi_results['desirability']['best_score'] = max(desirability_scores)
        
        # Pareto ìµœì í•´
        pareto_front = self._find_pareto_front(results, responses)
        multi_results['pareto_optimal'] = pareto_front
        
        # íƒ€í˜‘í•´ (TOPSIS)
        compromise = self._topsis_analysis(results, responses)
        multi_results['compromise_solution'] = compromise
        
        return multi_results
    
    def _find_pareto_front(self, results: pd.DataFrame, 
                          responses: List[ExperimentResponse]) -> List[int]:
        """Pareto ìµœì í•´ ì°¾ê¸°"""
        # ëª©ì  í•¨ìˆ˜ ê°’ ì¶”ì¶œ
        objectives = []
        
        for response in responses:
            if response.name in results.columns:
                values = results[response.name].values
                
                # ìµœëŒ€í™”ëŠ” ìŒìˆ˜ë¡œ ë³€í™˜ (ìµœì†Œí™”ë¡œ í†µì¼)
                if response.maximize:
                    objectives.append(-values)
                else:
                    objectives.append(values)
        
        if not objectives:
            return []
        
        objectives = np.array(objectives).T
        n_points = len(objectives)
        
        # Pareto ì§€ë°° í™•ì¸
        pareto_front = []
        
        for i in range(n_points):
            dominated = False
            
            for j in range(n_points):
                if i != j:
                    # jê°€ ië¥¼ ì§€ë°°í•˜ëŠ”ì§€ í™•ì¸
                    if all(objectives[j] <= objectives[i]) and any(objectives[j] < objectives[i]):
                        dominated = True
                        break
            
            if not dominated:
                pareto_front.append(i)
        
        return pareto_front
    
    def _topsis_analysis(self, results: pd.DataFrame, 
                        responses: List[ExperimentResponse]) -> Dict[str, Any]:
        """TOPSIS ë‹¤ê¸°ì¤€ ì˜ì‚¬ê²°ì •"""
        # ê²°ì • í–‰ë ¬ êµ¬ì„±
        decision_matrix = []
        weights = []
        directions = []  # True: maximize, False: minimize
        
        for response in responses:
            if response.name in results.columns:
                decision_matrix.append(results[response.name].values)
                weights.append(response.weight)
                directions.append(response.maximize)
        
        if not decision_matrix:
            return {}
        
        decision_matrix = np.array(decision_matrix).T
        weights = np.array(weights)
        weights = weights / weights.sum()  # ì •ê·œí™”
        
        # ì •ê·œí™”
        norm_matrix = decision_matrix / np.sqrt((decision_matrix ** 2).sum(axis=0))
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        weighted_matrix = norm_matrix * weights
        
        # ì´ìƒì ì¸ í•´ì™€ ë°˜ì´ìƒì ì¸ í•´
        ideal_solution = []
        anti_ideal_solution = []
        
        for j, maximize in enumerate(directions):
            if maximize:
                ideal_solution.append(weighted_matrix[:, j].max())
                anti_ideal_solution.append(weighted_matrix[:, j].min())
            else:
                ideal_solution.append(weighted_matrix[:, j].min())
                anti_ideal_solution.append(weighted_matrix[:, j].max())
        
        ideal_solution = np.array(ideal_solution)
        anti_ideal_solution = np.array(anti_ideal_solution)
        
        # ê±°ë¦¬ ê³„ì‚°
        dist_to_ideal = np.sqrt(((weighted_matrix - ideal_solution) ** 2).sum(axis=1))
        dist_to_anti_ideal = np.sqrt(((weighted_matrix - anti_ideal_solution) ** 2).sum(axis=1))
        
        # ìƒëŒ€ì  ê·¼ì ‘ë„
        relative_closeness = dist_to_anti_ideal / (dist_to_ideal + dist_to_anti_ideal + 1e-10)
        
        # ìµœì í•´
        best_idx = np.argmax(relative_closeness)
        
        return {
            'best_run': int(best_idx),
            'closeness_scores': relative_closeness.tolist(),
            'ideal_solution': ideal_solution.tolist(),
            'anti_ideal_solution': anti_ideal_solution.tolist()
        }
    
    def _generate_recommendations(self, analysis_results: Dict[str, Any],
                                response: ExperimentResponse) -> List[str]:
        """ë¶„ì„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # í†µê³„ì  ìœ ì˜ì„± ê¸°ë°˜
        if response.name in analysis_results['inferential']:
            anova_results = analysis_results['inferential'][response.name]
            
            significant_factors = [
                factor for factor, result in anova_results['main_effects'].items()
                if result['significant']
            ]
            
            if significant_factors:
                recommendations.append(
                    f"âœ… {response.name}ì— ìœ ì˜í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¸ì: {', '.join(significant_factors)}"
                )
                
                # ìµœì  ìˆ˜ì¤€ ì¶”ì²œ
                for factor in significant_factors:
                    means = anova_results['main_effects'][factor]['means']
                    
                    if response.maximize:
                        best_level = max(means.items(), key=lambda x: x[1])
                        recommendations.append(
                            f"   â†’ {factor}ëŠ” {best_level[0]} ìˆ˜ì¤€ì—ì„œ ìµœëŒ€ê°’ ë‹¬ì„±"
                        )
                    elif response.minimize:
                        best_level = min(means.items(), key=lambda x: x[1])
                        recommendations.append(
                            f"   â†’ {factor}ëŠ” {best_level[0]} ìˆ˜ì¤€ì—ì„œ ìµœì†Œê°’ ë‹¬ì„±"
                        )
        
        # ëª¨ë¸ ì í•©ë„ ê¸°ë°˜
        if response.name in analysis_results['regression']:
            reg_results = analysis_results['regression'][response.name]
            
            if 'best_model' in reg_results:
                r2 = reg_results[reg_results['best_model']]['r2']
                
                if r2 < 0.7:
                    recommendations.append(
                        f"âš ï¸ ëª¨ë¸ ì„¤ëª…ë ¥ì´ ë‚®ìŒ (RÂ² = {r2:.3f}). ì¶”ê°€ ì¸ìë‚˜ ë¹„ì„ í˜• í•­ ê³ ë ¤ í•„ìš”"
                    )
                
                if 'optimal_conditions' in reg_results:
                    opt_cond = reg_results['optimal_conditions']
                    recommendations.append(
                        f"ğŸ¯ ì˜ˆì¸¡ ìµœì  ì¡°ê±´: {opt_cond['predicted_value']:.3f} {response.unit}"
                    )
        
        # ê³µì •ëŠ¥ë ¥ ê¸°ë°˜
        if response.name in analysis_results['descriptive']:
            desc_stats = analysis_results['descriptive'][response.name]
            
            if 'target_deviation' in desc_stats:
                capability = desc_stats['target_deviation'].get('process_capability', {})
                
                if 'Cpk' in capability:
                    cpk = capability['Cpk']
                    
                    if cpk < 1.0:
                        recommendations.append(
                            f"âŒ ê³µì •ëŠ¥ë ¥ ë¶€ì¡± (Cpk = {cpk:.2f}). ë³€ë™ ê°ì†Œ í•„ìš”"
                        )
                    elif cpk < 1.33:
                        recommendations.append(
                            f"âš ï¸ ê³µì •ëŠ¥ë ¥ ê°œì„  í•„ìš” (Cpk = {cpk:.2f})"
                        )
                    else:
                        recommendations.append(
                            f"âœ… ìš°ìˆ˜í•œ ê³µì •ëŠ¥ë ¥ (Cpk = {cpk:.2f})"
                        )
        
        return recommendations

# ==================== ì‹œê°í™” ì—”ì§„ (í™•ì¥) ====================
class EnhancedVisualizationEngine:
    """í–¥ìƒëœ ì‹œê°í™” ì—”ì§„"""
    
    def __init__(self):
        self.color_palettes = {
            'default': px.colors.qualitative.Plotly,
            'sequential': px.colors.sequential.Viridis,
            'diverging': px.colors.diverging.RdBu,
            'polymer': ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'],
            'professional': ['#003f5c', '#58508d', '#bc5090', '#ff6361', '#ffa600']
        }
        
        self.plot_templates = {
            'clean': dict(
                layout=go.Layout(
                    font=dict(family="Arial", size=12),
                    plot_bgcolor='white',
                    paper_bgcolor='white',
                    margin=dict(l=60, r=60, t=80, b=60)
                )
            ),
            'dark': dict(
                layout=go.Layout(
                    font=dict(family="Arial", size=12, color='white'),
                    plot_bgcolor='#1e1e1e',
                    paper_bgcolor='#1e1e1e',
                    margin=dict(l=60, r=60, t=80, b=60)
                )
            )
        }
    
    def create_main_effects_plot(self, analysis_results: Dict[str, Any],
                                response_name: str) -> go.Figure:
        """ì£¼íš¨ê³¼ í”Œë¡¯ ìƒì„±"""
        if response_name not in analysis_results['inferential']:
            return go.Figure()
        
        main_effects = analysis_results['inferential'][response_name]['main_effects']
        
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        n_factors = len(main_effects)
        cols = min(3, n_factors)
        rows = (n_factors + cols - 1) // cols
        
        fig = make_subplots(
            rows=rows, cols=cols,
            subplot_titles=list(main_effects.keys()),
            vertical_spacing=0.15,
            horizontal_spacing=0.1
        )
        
        # ê° ì¸ìë³„ í”Œë¡¯
        for idx, (factor, data) in enumerate(main_effects.items()):
            row = idx // cols + 1
            col = idx % cols + 1
            
            levels = list(data['means'].keys())
            means = list(data['means'].values())
            
            # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
            std_err = np.std(means) / np.sqrt(len(means))
            ci = 1.96 * std_err
            
            fig.add_trace(
                go.Scatter(
                    x=levels,
                    y=means,
                    mode='lines+markers',
                    marker=dict(size=10, color='#1f77b4'),
                    line=dict(width=2),
                    error_y=dict(
                        type='constant',
                        value=ci,
                        visible=True
                    ),
                    name=factor,
                    showlegend=False
                ),
                row=row, col=col
            )
            
            # ìœ ì˜ì„± í‘œì‹œ
            if data['significant']:
                fig.add_annotation(
                    x=levels[len(levels)//2],
                    y=max(means) + ci,
                    text="*",
                    showarrow=False,
                    font=dict(size=20, color='red'),
                    row=row, col=col
                )
        
        fig.update_layout(
            title=f"ì£¼íš¨ê³¼ í”Œë¡¯: {response_name}",
            height=300 * rows,
            showlegend=False,
            template='plotly_white'
        )
        
        # Yì¶• ë ˆì´ë¸”
        fig.update_yaxes(title_text=response_name)
        
        return fig
    
    def create_interaction_plot(self, design: pd.DataFrame, results: pd.DataFrame,
                              factor1: str, factor2: str, response: str) -> go.Figure:
        """ìƒí˜¸ì‘ìš© í”Œë¡¯ ìƒì„±"""
        # ë°ì´í„° ì¤€ë¹„
        plot_data = design[[factor1, factor2]].copy()
        plot_data[response] = results[response]
        
        # í‰ê·  ê³„ì‚°
        interaction_means = plot_data.groupby([factor1, factor2])[response].agg(['mean', 'std', 'count'])
        
        fig = go.Figure()
        
        # factor2ì˜ ê° ìˆ˜ì¤€ë³„ë¡œ ì„  ê·¸ë¦¬ê¸°
        for level2 in plot_data[factor2].unique():
            data_subset = interaction_means.xs(level2, level=1)
            
            # ì‹ ë¢°êµ¬ê°„
            ci = 1.96 * data_subset['std'] / np.sqrt(data_subset['count'])
            
            fig.add_trace(go.Scatter(
                x=data_subset.index,
                y=data_subset['mean'],
                mode='lines+markers',
                name=f"{factor2}={level2}",
                error_y=dict(
                    type='data',
                    array=ci,
                    visible=True
                )
            ))
        
        fig.update_layout(
            title=f"ìƒí˜¸ì‘ìš© í”Œë¡¯: {factor1} Ã— {factor2}",
            xaxis_title=factor1,
            yaxis_title=response,
            hovermode='x unified',
            template='plotly_white'
        )
        
        return fig
    
    def create_response_surface_3d(self, model, factor1_range: Tuple[float, float],
                                 factor2_range: Tuple[float, float],
                                 factor_names: List[str],
                                 response_name: str,
                                 other_factors: Dict[str, float] = None) -> go.Figure:
        """3D ë°˜ì‘í‘œë©´ í”Œë¡¯"""
        # ê·¸ë¦¬ë“œ ìƒì„±
        n_points = 50
        x = np.linspace(factor1_range[0], factor1_range[1], n_points)
        y = np.linspace(factor2_range[0], factor2_range[1], n_points)
        X, Y = np.meshgrid(x, y)
        
        # ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        prediction_points = []
        
        for i in range(n_points):
            for j in range(n_points):
                point = other_factors.copy() if other_factors else {}
                point[factor_names[0]] = X[i, j]
                point[factor_names[1]] = Y[i, j]
                prediction_points.append(point)
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        pred_df = pd.DataFrame(prediction_points)
        
        # ì˜ˆì¸¡
        Z = model.predict(pred_df).reshape(n_points, n_points)
        
        # 3D Surface plot
        fig = go.Figure(data=[
            go.Surface(
                x=x,
                y=y,
                z=Z,
                colorscale='Viridis',
                contours=dict(
                    z=dict(show=True, usecolormap=True, highlightcolor="limegreen", project_z=True)
                )
            )
        ])
        
        # ë ˆì´ì•„ì›ƒ
        fig.update_layout(
            title=f"ë°˜ì‘í‘œë©´: {response_name}",
            scene=dict(
                xaxis_title=factor_names[0],
                yaxis_title=factor_names[1],
                zaxis_title=response_name,
                camera=dict(
                    eye=dict(x=1.5, y=1.5, z=1.2)
                )
            ),
            width=800,
            height=600
        )
        
        return fig
    
    def create_contour_plot(self, model, factor1_range: Tuple[float, float],
                          factor2_range: Tuple[float, float],
                          factor_names: List[str],
                          response_name: str,
                          other_factors: Dict[str, float] = None,
                          show_optimum: bool = True) -> go.Figure:
        """ë“±ê³ ì„  í”Œë¡¯"""
        # ê·¸ë¦¬ë“œ ìƒì„±
        n_points = 100
        x = np.linspace(factor1_range[0], factor1_range[1], n_points)
        y = np.linspace(factor2_range[0], factor2_range[1], n_points)
        X, Y = np.meshgrid(x, y)
        
        # ì˜ˆì¸¡
        prediction_points = []
        for i in range(n_points):
            for j in range(n_points):
                point = other_factors.copy() if other_factors else {}
                point[factor_names[0]] = X[i, j]
                point[factor_names[1]] = Y[i, j]
                prediction_points.append(point)
        
        pred_df = pd.DataFrame(prediction_points)
        Z = model.predict(pred_df).reshape(n_points, n_points)
        
        # Contour plot
        fig = go.Figure()
        
        # Filled contour
        fig.add_trace(go.Contour(
            x=x,
            y=y,
            z=Z,
            colorscale='Viridis',
            contours=dict(
                coloring='heatmap',
                showlabels=True,
                labelfont=dict(size=12, color='white')
            ),
            colorbar=dict(title=response_name)
        ))
        
        # ìµœì ì  í‘œì‹œ
        if show_optimum:
            # ê°„ë‹¨í•œ ìµœì ì  ì°¾ê¸° (ê·¸ë¦¬ë“œ ì„œì¹˜)
            opt_idx = np.unravel_index(Z.argmax(), Z.shape)
            opt_x = X[opt_idx]
            opt_y = Y[opt_idx]
            opt_z = Z[opt_idx]
            
            fig.add_trace(go.Scatter(
                x=[opt_x],
                y=[opt_y],
                mode='markers',
                marker=dict(
                    size=15,
                    color='red',
                    symbol='star',
                    line=dict(color='white', width=2)
                ),
                name=f'ìµœì ì  ({opt_z:.2f})',
                showlegend=True
            ))
        
        fig.update_layout(
            title=f"ë“±ê³ ì„  í”Œë¡¯: {response_name}",
            xaxis_title=factor_names[0],
            yaxis_title=factor_names[1],
            template='plotly_white',
            width=700,
            height=600
        )
        
        return fig
    
    def create_pareto_chart(self, data: Dict[str, float], title: str = "Pareto Chart") -> go.Figure:
        """íŒŒë ˆí†  ì°¨íŠ¸"""
        # ë°ì´í„° ì •ë ¬
        sorted_items = sorted(data.items(), key=lambda x: abs(x[1]), reverse=True)
        
        categories = [item[0] for item in sorted_items]
        values = [abs(item[1]) for item in sorted_items]
        
        # ëˆ„ì  ë¹„ìœ¨ ê³„ì‚°
        total = sum(values)
        cumulative = []
        cum_sum = 0
        
        for val in values:
            cum_sum += val
            cumulative.append(cum_sum / total * 100)
        
        # ê·¸ë˜í”„ ìƒì„±
        fig = go.Figure()
        
        # ë§‰ëŒ€ ê·¸ë˜í”„
        fig.add_trace(go.Bar(
            x=categories,
            y=values,
            name='íš¨ê³¼',
            marker_color='lightblue',
            yaxis='y'
        ))
        
        # ëˆ„ì  ì„  ê·¸ë˜í”„
        fig.add_trace(go.Scatter(
            x=categories,
            y=cumulative,
            name='ëˆ„ì  %',
            mode='lines+markers',
            line=dict(color='red', width=2),
            marker=dict(size=8),
            yaxis='y2'
        ))
        
        # 80% ì„ 
        fig.add_hline(y=80, line_dash="dash", line_color="gray", 
                     annotation_text="80%", yref='y2')
        
        # ë ˆì´ì•„ì›ƒ
        fig.update_layout(
            title=title,
            xaxis=dict(title='ìš”ì¸'),
            yaxis=dict(title='íš¨ê³¼ í¬ê¸°', side='left'),
            yaxis2=dict(title='ëˆ„ì  ë¹„ìœ¨ (%)', side='right', overlaying='y', range=[0, 100]),
            hovermode='x unified',
            template='plotly_white'
        )
        
        return fig
    
    def create_residual_plots(self, actual: np.ndarray, predicted: np.ndarray,
                            feature_names: List[str] = None) -> go.Figure:
        """ì”ì°¨ ì§„ë‹¨ í”Œë¡¯"""
        residuals = actual - predicted
        
        # 4ê°œ ì„œë¸Œí”Œë¡¯
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('ì”ì°¨ vs ì í•©ê°’', 'ì •ê·œ Q-Q í”Œë¡¯', 
                          'ì²™ë„-ìœ„ì¹˜ í”Œë¡¯', 'ì”ì°¨ vs ë ˆë²„ë¦¬ì§€'),
            vertical_spacing=0.15,
            horizontal_spacing=0.1
        )
        
        # 1. ì”ì°¨ vs ì í•©ê°’
        fig.add_trace(
            go.Scatter(
                x=predicted,
                y=residuals,
                mode='markers',
                marker=dict(color='blue', size=6),
                showlegend=False
            ),
            row=1, col=1
        )
        fig.add_hline(y=0, line_dash="dash", line_color="red", row=1, col=1)
        
        # 2. Q-Q í”Œë¡¯
        qq_data = stats.probplot(residuals, dist="norm")
        fig.add_trace(
            go.Scatter(
                x=qq_data[0][0],
                y=qq_data[0][1],
                mode='markers',
                marker=dict(color='blue', size=6),
                showlegend=False
            ),
            row=1, col=2
        )
        
        # Q-Q ë¼ì¸
        fig.add_trace(
            go.Scatter(
                x=qq_data[0][0],
                y=qq_data[1][1] + qq_data[1][0] * qq_data[0][0],
                mode='lines',
                line=dict(color='red', dash='dash'),
                showlegend=False
            ),
            row=1, col=2
        )
        
        # 3. ì²™ë„-ìœ„ì¹˜ í”Œë¡¯
        standardized_residuals = residuals / np.std(residuals)
        sqrt_abs_residuals = np.sqrt(np.abs(standardized_residuals))
        
        fig.add_trace(
            go.Scatter(
                x=predicted,
                y=sqrt_abs_residuals,
                mode='markers',
                marker=dict(color='blue', size=6),
                showlegend=False
            ),
            row=2, col=1
        )
        
        # í‰í™œì„  ì¶”ê°€ (LOWESS)
        from statsmodels.nonparametric.smoothers_lowess import lowess
        smoothed = lowess(sqrt_abs_residuals, predicted, frac=0.6)
        
        fig.add_trace(
            go.Scatter(
                x=smoothed[:, 0],
                y=smoothed[:, 1],
                mode='lines',
                line=dict(color='red', width=2),
                showlegend=False
            ),
            row=2, col=1
        )
        
        # 4. ì”ì°¨ vs ë ˆë²„ë¦¬ì§€
        # ê°„ë‹¨í•œ ë ˆë²„ë¦¬ì§€ ê³„ì‚° (ì‹¤ì œë¡œëŠ” hat matrix í•„ìš”)
        n = len(residuals)
        leverage = np.ones(n) / n  # ë‹¨ìˆœí™”
        
        fig.add_trace(
            go.Scatter(
                x=leverage,
                y=standardized_residuals,
                mode='markers',
                marker=dict(color='blue', size=6),
                showlegend=False
            ),
            row=2, col=2
        )
        
        # Cook's distance ì„ê³„ì„ 
        fig.add_hline(y=2, line_dash="dash", line_color="red", row=2, col=2)
        fig.add_hline(y=-2, line_dash="dash", line_color="red", row=2, col=2)
        
        # ì¶• ë ˆì´ë¸”
        fig.update_xaxes(title_text="ì í•©ê°’", row=1, col=1)
        fig.update_yaxes(title_text="ì”ì°¨", row=1, col=1)
        
        fig.update_xaxes(title_text="ì´ë¡ ì  ë¶„ìœ„ìˆ˜", row=1, col=2)
        fig.update_yaxes(title_text="í‘œë³¸ ë¶„ìœ„ìˆ˜", row=1, col=2)
        
        fig.update_xaxes(title_text="ì í•©ê°’", row=2, col=1)
        fig.update_yaxes(title_text="âˆš|í‘œì¤€í™” ì”ì°¨|", row=2, col=1)
        
        fig.update_xaxes(title_text="ë ˆë²„ë¦¬ì§€", row=2, col=2)
        fig.update_yaxes(title_text="í‘œì¤€í™” ì”ì°¨", row=2, col=2)
        
        fig.update_layout(
            title="ì”ì°¨ ì§„ë‹¨ í”Œë¡¯",
            height=800,
            showlegend=False,
            template='plotly_white'
        )
        
        return fig
    
    def create_optimization_history_plot(self, optimization_results: Dict[str, Any]) -> go.Figure:
        """ìµœì í™” ì´ë ¥ í”Œë¡¯"""
        if 'optimization_history' not in optimization_results:
            return go.Figure()
        
        history = optimization_results['optimization_history']
        
        fig = go.Figure()
        
        # ëª©ì  í•¨ìˆ˜ ê°’ ì¶”ì´
        fig.add_trace(go.Scatter(
            x=list(range(len(history))),
            y=[h['value'] for h in history],
            mode='lines+markers',
            name='ëª©ì  í•¨ìˆ˜ ê°’',
            line=dict(color='blue', width=2)
        ))
        
        # ìµœì ê°’ ì¶”ì´
        best_values = []
        current_best = float('inf')
        
        for h in history:
            current_best = min(current_best, h['value'])
            best_values.append(current_best)
        
        fig.add_trace(go.Scatter(
            x=list(range(len(history))),
            y=best_values,
            mode='lines',
            name='ìµœì ê°’',
            line=dict(color='red', width=2, dash='dash')
        ))
        
        fig.update_layout(
            title="ìµœì í™” ìˆ˜ë ´ ì´ë ¥",
            xaxis_title="ë°˜ë³µ íšŸìˆ˜",
            yaxis_title="ëª©ì  í•¨ìˆ˜ ê°’",
            hovermode='x unified',
            template='plotly_white'
        )
        
        return fig
    
    def create_3d_molecule_view(self, smiles: str = None, sdf_file: str = None) -> str:
        """3D ë¶„ì ì‹œê°í™”"""
        if not PY3DMOL_AVAILABLE:
            return "<p>3D ë¶„ì ì‹œê°í™”ë¥¼ ìœ„í•´ py3Dmol ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.</p>"
        
        if not RDKIT_AVAILABLE:
            return "<p>ë¶„ì ì²˜ë¦¬ë¥¼ ìœ„í•´ RDKit ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.</p>"
        
        # ë¶„ì ìƒì„±
        if smiles:
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                return "<p>ìœ íš¨í•˜ì§€ ì•Šì€ SMILES ë¬¸ìì—´ì…ë‹ˆë‹¤.</p>"
            
            # 3D ì¢Œí‘œ ìƒì„±
            mol = Chem.AddHs(mol)
            AllChem.EmbedMolecule(mol, randomSeed=42)
            AllChem.MMFFOptimizeMolecule(mol)
            
            # SDF í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            mol_block = Chem.MolToMolBlock(mol)
        
        elif sdf_file:
            with open(sdf_file, 'r') as f:
                mol_block = f.read()
        
        else:
            return "<p>SMILES ë˜ëŠ” SDF íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.</p>"
        
        # 3D ë·°ì–´ ìƒì„±
        viewer = py3Dmol.view(width=800, height=600)
        viewer.addModel(mol_block, 'sdf')
        viewer.setStyle({'stick': {}})
        viewer.setBackgroundColor('white')
        viewer.zoomTo()
        
        return viewer.render()

