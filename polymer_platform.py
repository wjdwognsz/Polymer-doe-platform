# Polymer-doe-platform - Part 1: ê¸°ë³¸ êµ¬ì¡°, ë°ì´í„° í´ë˜ìŠ¤, ìƒìˆ˜ ì •ì˜
# Polymer-doe-platform - Part 2: ê¸°ë³¸ êµ¬ì¡°, ë°ì´í„° í´ë˜ìŠ¤, ìƒìˆ˜ ì •ì˜
# Polymer-doe-platform - Part 3: í†µê³„ ë¶„ì„, ì‹¤í—˜ ì„¤ê³„ ìƒì„±ê¸°
#    - ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ (ì´ë²¤íŠ¸ ë²„ìŠ¤, êµ¬ë…/ë°œí–‰ íŒ¨í„´)
#    - ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € (í™•ì¥ëœ í…Œì´ë¸” êµ¬ì¡°, ë°±ì—… ì‹œìŠ¤í…œ)
#    - í˜‘ì—… ì‹œìŠ¤í…œ (ì‹¤ì‹œê°„ í˜‘ì—…, ëŒ“ê¸€, ë¦¬ë·° ë“±)
#    - API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ í™•ì¥ (Rate limiting í¬í•¨)
# Polymer-doe-platform - Part 4: í†µê³„ ë¶„ì„, ì‹¤í—˜ ì„¤ê³„ ìƒì„±ê¸°
#    - Rate Limiter: API í˜¸ì¶œ ì†ë„ ì œí•œ ê´€ë¦¬
#    - í–¥ìƒëœ API Monitor: ë¹„ìš© ì¶”ì , í† í° ì‚¬ìš©ëŸ‰, ìƒì„¸ ëŒ€ì‹œë³´ë“œ
#    - í™•ì¥ëœ ë²ˆì—­ ì„œë¹„ìŠ¤: ê¸°ìˆ  ìš©ì–´ ë³´í˜¸, ë‹¤êµ­ì–´ ë³´ê³ ì„œ ìƒì„±
#    - ê³ ê¸‰ ì‹¤í—˜ ì„¤ê³„ ì—”ì§„:
#    	â€¢	ì ì‘í˜• ì„¤ê³„ (ë² ì´ì§€ì•ˆ ìµœì í™”)
#	    â€¢	í˜¼í•©ë¬¼ ì„¤ê³„ (Simplex-Lattice, Centroid ë“±)
#    - ê¸°ê³„í•™ìŠµ ì˜ˆì¸¡ ì‹œìŠ¤í…œ:
#    	â€¢	ì—¬ëŸ¬ ëª¨ë¸ ì•™ìƒë¸” (RF, GB, XGBoost, Neural Network)
#    	â€¢	SHAPì„ í†µí•œ ì˜ˆì¸¡ ì„¤ëª…
#    	â€¢	Optunaë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
# Polymer-doe-platform - Part 5: ì‹œê°í™” ì‹œìŠ¤í…œ
#    - ê³ ê¸‰ í†µê³„ ë¶„ì„ ì—”ì§„:
#	    â€¢	ì¢…í•© í†µê³„ ë¶„ì„ (ê¸°ìˆ í†µê³„, ANOVA, íšŒê·€, ì§„ë‹¨)
#    	â€¢	ì„¤ê³„ íŠ¹ì„± ë¶„ì„ (ê· í˜•ì„±, ì§êµì„±, ê²€ì •ë ¥)
#	    â€¢	ë¶„í¬ ê²€ì • ë° ìµœì  ë¶„í¬ ì°¾ê¸°
#    	â€¢	ê³µì •ëŠ¥ë ¥ì§€ìˆ˜ ê³„ì‚° (Cp, Cpk, Cpm)
#    	â€¢	ë‹¤ì¤‘ ë°˜ì‘ ë¶„ì„ (Pareto ìµœì í•´, TOPSIS)
#    - í–¥ìƒëœ ì‹œê°í™” ì—”ì§„:
#    	â€¢	ì£¼íš¨ê³¼ ë° ìƒí˜¸ì‘ìš© í”Œë¡¯
#    	â€¢	3D ë°˜ì‘í‘œë©´ ë° ë“±ê³ ì„  í”Œë¡¯
#    	â€¢	íŒŒë ˆí†  ì°¨íŠ¸
#    	â€¢	ì”ì°¨ ì§„ë‹¨ í”Œë¡¯ (4ê°œ ì„œë¸Œí”Œë¡¯)
#    	â€¢	ìµœì í™” ìˆ˜ë ´ ì´ë ¥
#    	â€¢	3D ë¶„ì ì‹œê°í™” (RDKit/py3Dmol ì‚¬ìš©)
# Polymer-doe-platform - Part 6 AI ì—”ì§„ í†µí•©, í•©ì˜ ì‹œìŠ¤í…œ
#    - BaseAIEngine í´ë˜ìŠ¤: ëª¨ë“  AI ì—”ì§„ì˜ ê¸°ë³¸ êµ¬ì¡°
#    - ê°œë³„ AI ì—”ì§„ êµ¬í˜„: Gemini, Grok, SambaNova, DeepSeek, Groq, HuggingFace
#    - AIOrchestrator: ë‹¤ì¤‘ AI ì¡°ì • ì‹œìŠ¤í…œ
#    - ì‚¬ìš©ì ë ˆë²¨ë³„ í”„ë¡¬í”„íŠ¸ ì¡°ì •: ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€ ë§ì¶¤í˜• ì‘ë‹µ
#    - ìºì‹± ë° ì‚¬ìš©ëŸ‰ ì¶”ì : íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
# Polymer-doe-platform - Part 7 AI ì—”ì§„ í†µí•©, í•©ì˜ ì‹œìŠ¤í…œ
#    - AI í•©ì˜ ì‹œìŠ¤í…œ:
#    	â€¢	ë‹¤ì¤‘ AI ì‘ë‹µì˜ ìœ ì‚¬ë„ ë¶„ì„
#    	â€¢	í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ì˜ê²¬ ê·¸ë£¹í™”
#    	â€¢	ìµœì  ì‘ë‹µ ì„ íƒ ì „ëµ
#    - AI í•™ìŠµ ì‹œìŠ¤í…œ:
#    	â€¢	ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ë¶„ì„
#    	â€¢	ì„±ê³µ/ì‹¤íŒ¨ íŒ¨í„´ í•™ìŠµ
#    	â€¢	ì§€ì†ì ì¸ ì„±ëŠ¥ ê°œì„ 
#    - ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ë§¤ë‹ˆì €:
#    	â€¢	9ê°œ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
#    	â€¢	ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
#    	â€¢	ê²°ê³¼ í†µí•© ë° ìºì‹±
#    - ê²€ìƒ‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°:
#    	â€¢	ê²€ìƒ‰ ìœ í˜•ë³„ ìµœì í™”
#    	â€¢	í†µí•© ê²€ìƒ‰ ê²°ê³¼ ì œê³µ
# Polymer-doe-platform - Part 8: ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸
#    - ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„:
#    	â€¢	Materials Project, PubChem, PolyInfo
#    	â€¢	Protocols.io, GitHub, OpenAlex
#    	â€¢	ê°ê°ì˜ API íŠ¹ì„±ì— ë§ì¶˜ êµ¬í˜„
#    - ê³ ê¸‰ ì‹¤í—˜ ì„¤ê³„ ì—”ì§„:
#    	â€¢	AI ê¸°ë°˜ ì„¤ê³„ ì „ëµ ì„ íƒ
#    	â€¢	ë¬¸í—Œ ë° í”„ë¡œí† ì½œ ìë™ ê²€ìƒ‰
#    	â€¢	ë‹¤ì¤‘ AI í˜‘ì—…ì„ í†µí•œ ì„¤ê³„ ìƒì„±
#    - ì°¸ì¡° ë°ì´í„° í†µí•©:
#	    â€¢	ê´€ë ¨ ë…¼ë¬¸, í”„ë¡œí† ì½œ, ì¬ë£Œ ë°ì´í„° ìˆ˜ì§‘
#    	â€¢	AIê°€ ì°¸ê³ ìë£Œë¥¼ í™œìš©í•œ ì„¤ê³„ ìƒì„±
# Polymer-doe-platform - Part 9: ê²€ì¦ ë° ìµœì í™” ì‹œìŠ¤í…œ
#    - ì‹¤í—˜ ì„¤ê³„ ê²€ì¦ ë° ìµœì í™”:
#    	â€¢	í†µê³„ì , ì‹¤ìš©ì , ì•ˆì „ì„± ê²€ì¦
#    	â€¢	AI ê¸°ë°˜ ì„¤ê³„ ìµœì í™”
#    	â€¢	ì„¤ê³„ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
#    - ë¹„ìš© ë° ì‹œê°„ ì¶”ì •:
#    	â€¢	ê³„ì‚° ê¸°ë°˜ ì¶”ì •ê³¼ AI ì¶”ì • í†µí•©
#    	â€¢	ë³‘ë ¬/ìˆœì°¨ ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡
#    - ì‚¬ìš©ì ë ˆë²¨ë³„ ì ì‘:
#    	â€¢	ì´ˆë³´ìë¥¼ ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œ
#    	â€¢	ë ˆë²¨ë³„ ë§ì¶¤í˜• ì„¤ëª… ìƒì„±
#    - ì‹¤í—˜ ì„¤ê³„ ì „ëµ êµ¬í˜„:
#    	â€¢	Screening, Optimization, Mixture, Robust, Adaptive ì „ëµ
#    	â€¢	ê° ì „ëµë³„ íŠ¹í™”ëœ ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
#    - ì„¤ê³„ ê²€ì¦ ì‹œìŠ¤í…œ:
#    	â€¢	í†µê³„ì  ì†ì„± ê²€ì¦
#    	â€¢	ê· í˜•ì„±, ì§êµì„±, ê²€ì •ë ¥ ë¶„ì„
#    	â€¢	ì‹¤ìš©ì„± ì œì•½ ê²€ì¦ (ì‹œê°„, ì¥ë¹„, ì¬ë£Œ)
#    	â€¢	ì•ˆì „ì„± ê²€ì¦ ë° í”„ë¡œí† ì½œ ìƒì„±
#    	â€¢	ì‹¤í—˜ ìˆœì„œ ìµœì í™”
# Polymer-doe-platform - Part 10: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤, í˜‘ì—… ì‹œìŠ¤í…œ
#    - ë¹„ìš© ì¶”ì •ê¸°:
#    	â€¢	ì¬ë£Œë¹„, ë¶„ì„ë¹„, ì¸ê±´ë¹„ ê³„ì‚°
#    	â€¢	ê³ ë¶„ìë³„ ê°€ê²© ë°ì´í„°ë² ì´ìŠ¤
#    	â€¢	ìƒì„¸ ë¹„ìš© ë¶„ì„
#    - ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ì‹œìŠ¤í…œ:
#    	â€¢	Streamlit ê¸°ë°˜ UI êµ¬ì¡°
#    	â€¢	ì‚¬ìš©ì ë ˆë²¨ë³„ ì ì‘í˜• ì¸í„°í˜ì´ìŠ¤
#    	â€¢	í˜ì´ì§€ë³„ ë Œë”ë§ ì‹œìŠ¤í…œ
#    - ì‹¤ì‹œê°„ í˜‘ì—… ì‹œìŠ¤í…œ:
#    	â€¢	í˜‘ì—… ì„¸ì…˜ ê´€ë¦¬
#    	â€¢	ì„¤ê³„ ê³µìœ  ë° ëŒ“ê¸€ ê¸°ëŠ¥
#    	â€¢	ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ
#    	â€¢	ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸
#    	â€¢	ì±„íŒ… ê¸°ëŠ¥
#    	â€¢	ì„¤ê³„ íˆ¬í‘œ ì‹œìŠ¤í…œ
# Polymer-doe-platform - Part 11: í˜ì´ì§€ êµ¬í˜„, ë°ì´í„° ë¶„ì„
#    - í”„ë¡œì íŠ¸ ì„¤ì • í˜ì´ì§€:
#    	â€¢	ê³ ë¶„ì ì„ íƒ ì‹œìŠ¤í…œ
#    	â€¢	AI ì¶”ì²œ ê¸°ëŠ¥
#    	â€¢	ì¥ë¹„ ë° ì œì•½ì¡°ê±´ ì„¤ì •
#    - ì‹¤í—˜ ì„¤ê³„ í˜ì´ì§€:
#    	â€¢	ìš”ì¸ ì„ íƒ (ì¶”ì²œ ë° ì‚¬ìš©ì ì •ì˜)
#    	â€¢	ë°˜ì‘ë³€ìˆ˜ ì •ì˜
#    	â€¢	ì„¤ê³„ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
#    - ì‚¬ìš©ì ë ˆë²¨ë³„ ì ì‘:
#    	â€¢	ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª… ì¶”ê°€
#    	â€¢	ë ˆë²¨ë³„ UI ì¡°ì •
#    	â€¢	ë„ì›€ë§ ì‹œìŠ¤í…œ
#    	â€¢	ê²€ì¦ ë° ìµœì í™” íƒ­ (ê²Œì´ì§€ ì°¨íŠ¸, ìƒì„¸ ê²€ì¦)
#    	â€¢	ìµœì¢… í™•ì¸ íƒ­ (ì²´í¬ë¦¬ìŠ¤íŠ¸, QR ì½”ë“œ ìƒì„±)
# Polymer-doe-platform - Part 12: í˜ì´ì§€ êµ¬í˜„, ë°ì´í„° ë¶„ì„
#    - ë°ì´í„° ë¶„ì„ í˜ì´ì§€:
#    	â€¢	ë‹¤ì–‘í•œ ë°ì´í„° ì…ë ¥ ë°©ë²•
#    	â€¢	ê¸°ë³¸ í†µê³„ ë¶„ì„ (ê¸°ìˆ í†µê³„, ì •ê·œì„± ê²€ì •, ì£¼íš¨ê³¼)
#	    â€¢	ê³ ê¸‰ ë¶„ì„ ì¤€ë¹„
#    - ì‹œê°í™” ë° ë³´ê³ :
#    	â€¢	ëŒ€í™”í˜• ê·¸ë˜í”„
#    	â€¢	ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ
#    	â€¢	í†µê³„ ê²€ì • ê²°ê³¼
#    - ì‚¬ìš©ì ê²½í—˜ ê°œì„ :
#    	â€¢	ë ˆë²¨ë³„ ê°€ì´ë“œ ì œê³µ
#    	â€¢	ì§ê´€ì ì¸ UI êµ¬ì„±
#	    â€¢	ë°ì´í„° ê²€ì¦ ê¸°ëŠ¥
# Polymer-doe-platform - Part 13: ê³ ê¸‰ ë¶„ì„, AI ì¸ì‚¬ì´íŠ¸, í•™ìŠµ ì‹œìŠ¤í…œ
#    - ê³ ê¸‰ ë¶„ì„ ë©”ì„œë“œ:
#    	â€¢	ANOVA ë¶„ì„ (ì”ì°¨ ë¶„ì„ í¬í•¨)
#    	â€¢	íšŒê·€ë¶„ì„ (ë‹¤ì–‘í•œ ëª¨ë¸)
#    	â€¢	ë°˜ì‘í‘œë©´ë¶„ì„ (3D ì‹œê°í™”)
#    	â€¢	ìµœì í™” (Desirability, Pareto, GA)
#    	â€¢	ê¸°ê³„í•™ìŠµ ë¶„ì„
#    - AI ì¸ì‚¬ì´íŠ¸ ì‹œìŠ¤í…œ:
#    	â€¢	5ê°€ì§€ ì¸ì‚¬ì´íŠ¸ ìœ í˜•
#    	â€¢	ë‹¤ì¤‘ AI í˜‘ì—… ë¶„ì„
#    	â€¢	ìë™ ì‹œê°í™” ìƒì„±
#    - í•™ìŠµ ì„¼í„°:
#    	â€¢	êµ¬ì¡°í™”ëœ í•™ìŠµ ëª¨ë“ˆ
#    	â€¢	ì‚¬ìš©ì ë ˆë²¨ë³„ ì»¨í…ì¸ 
#    	â€¢	ì¸í„°ë™í‹°ë¸Œ í€´ì¦ˆì™€ ì‹¤ìŠµ
#    	â€¢	í•™ìŠµ ì»¨í…ì¸  ë¡œë“œ
#    - ê³ ê¸‰ ì‹œê°í™”:
#    	â€¢	3D ë°˜ì‘í‘œë©´
#    	â€¢	ë“±ê³ ì„  í”Œë¡¯
#    	â€¢	ì”ì°¨ ë¶„ì„ ì°¨íŠ¸
# Polymer-doe-platform - Part 14: ë³´ê³ ì„œ ìƒì„±, ë©”ì¸ ì•±
#    - ë³´ê³ ì„œ ìƒì„±
#    - ë©”ì¸ ì•± êµ¬ì¡°

# Polymer-doe-platform - Part 1
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§¬ ë²”ìš© ê³ ë¶„ì ì‹¤í—˜ ì„¤ê³„ í”Œë«í¼ (Universal Polymer Design of Experiments Platform)
================================================================================

Enhanced Version 4.0.0
- ì™„ì „í•œ AI-ë°ì´í„°ë² ì´ìŠ¤ í†µí•©
- ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ
- ì´ˆë³´ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤
- 3D ë¶„ì ì‹œê°í™”
- ì‹¤ì‹œê°„ í˜‘ì—… ê¸°ëŠ¥

ê°œë°œ: Polymer DOE Research Team
ë¼ì´ì„ ìŠ¤: MIT
"""

# ==================== í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ====================
import os
import sys
import json
import time
import hashlib
import base64
import io
import re
import logging
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union, Callable, TypeVar, Generic
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from collections import defaultdict, OrderedDict, deque
from functools import lru_cache, wraps, partial
from pathlib import Path
import tempfile
import shutil
import traceback
import pickle
import sqlite3
import threading
import queue
import asyncio
import concurrent.futures
import uuid
import mimetypes
import zipfile
import tarfile
from contextlib import contextmanager
import subprocess
import platform

# ==================== ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ====================
import numpy as np
import pandas as pd
from scipy import stats, optimize, interpolate, signal
from scipy.stats import (
    f_oneway, ttest_ind, shapiro, levene, anderson,
    kruskal, mannwhitneyu, wilcoxon, friedmanchisquare
)
from scipy.optimize import minimize, differential_evolution, dual_annealing
from scipy.spatial.distance import cdist
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
#from sklearn.decomposition import PCA, ICA, NMF
from sklearn.decomposition import PCA, FastICA, NMF
try:
    from sklearn.decomposition import FastICA
    # ICAë¥¼ FastICAì˜ ë³„ì¹­ìœ¼ë¡œ ì„¤ì •
    ICA = FastICA
except ImportError:
    pass
from sklearn.manifold import TSNE, MDS
from sklearn.model_selection import (
    train_test_split, cross_val_score, KFold, 
    GridSearchCV, RandomizedSearchCV
)
try:
    from skopt import BayesSearchCV
    SKOPT_AVAILABLE = True
except ImportError:
    SKOPT_AVAILABLE = False

from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor,
    ExtraTreesRegressor, AdaBoostRegressor
)
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    explained_variance_score, max_error
)
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import (
    RBF, Matern, RationalQuadratic, ExpSineSquared,
    DotProduct, WhiteKernel, ConstantKernel
)
from sklearn.neural_network import MLPRegressor
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
import xgboost as xgb
import lightgbm as lgb

# ==================== ì‹œê°í™” ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.animation import FuncAnimation
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.io as pio
import altair as alt
import holoviews as hv
import bokeh.plotting as bk
from PIL import Image, ImageDraw, ImageFont
import cv2

# ==================== ì›¹ í”„ë ˆì„ì›Œí¬ ====================
import streamlit as st
from streamlit_option_menu import option_menu
from streamlit_drawable_canvas import st_canvas
from streamlit_ace import st_ace
try:
    from streamlit_aggrid import AgGrid, GridOptionsBuilder
    AGGRID_AVAILABLE = True
except ImportError:
    try:
        # ìƒˆë¡œìš´ ë²„ì „ì˜ import ë°©ì‹ ì‹œë„
        from st_aggrid import AgGrid, GridOptionsBuilder
        AGGRID_AVAILABLE = True
    except ImportError:
        AGGRID_AVAILABLE = False
        # AgGrid ëŒ€ì²´ êµ¬í˜„
        class AgGrid:
            def __init__(self, *args, **kwargs):
                st.warning("streamlit-aggridë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.")
                if args and isinstance(args[0], pd.DataFrame):
                    st.dataframe(args[0], use_container_width=True)
        
        class GridOptionsBuilder:
            @staticmethod
            def from_dataframe(df):
                return GridOptionsBuilder()
            
            def configure_pagination(self, *args, **kwargs):
                return self
            
            def configure_selection(self, *args, **kwargs):
                return self
            
            def build(self):
                return {}
from streamlit_elements import elements, mui, html
from streamlit_timeline import timeline
from streamlit_folium import folium_static
import streamlit.components.v1 as components

# ==================== 3D ì‹œê°í™” ====================
try:
    import py3Dmol
    from stmol import showmol
    import nglview
    PY3DMOL_AVAILABLE = True
except ImportError:
    PY3DMOL_AVAILABLE = False

# ==================== í™”í•™ ì •ë³´í•™ ====================
try:
    from rdkit import Chem
    from rdkit.Chem import Draw, Descriptors, AllChem
    from rdkit.Chem.Draw import IPythonConsole
    import pubchempy as pcp
    RDKIT_AVAILABLE = True
except ImportError:
    RDKIT_AVAILABLE = False

# ==================== API ë° ì™¸ë¶€ ì„œë¹„ìŠ¤ ====================
import requests
import aiohttp
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import httpx
import websocket
import socketio

# ==================== AI ì„œë¹„ìŠ¤ ====================
# OpenAI
try:
    import openai
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Google AI
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Anthropic Claude
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# ì¶”ê°€ AI ì„œë¹„ìŠ¤ë“¤
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

try:
    from huggingface_hub import InferenceClient, HfApi
    from transformers import pipeline
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False

# ==================== ë°ì´í„°ë² ì´ìŠ¤ ë° ì €ì¥ì†Œ ====================
try:
    import gspread
    from google.oauth2.service_account import Credentials
    from google.auth.transport.requests import Request
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaFileUpload
    GSPREAD_AVAILABLE = True
except ImportError:
    GSPREAD_AVAILABLE = False

try:
    from github import Github
    GITHUB_AVAILABLE = True
except ImportError:
    GITHUB_AVAILABLE = False

try:
    import pymongo
    from motor.motor_asyncio import AsyncIOMotorClient
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

# ==================== ë²ˆì—­ ë° ìì—°ì–´ ì²˜ë¦¬ ====================
#try:
#    from googletrans import Translator
#    import langdetect
#    TRANSLATION_AVAILABLE = True
#except ImportError:
#    TRANSLATION_AVAILABLE = False

try:
    from deep_translator import GoogleTranslator
    import langdetect
    TRANSLATION_AVAILABLE = True
    
    # Translator í´ë˜ìŠ¤ë¥¼ GoogleTranslatorë¡œ ë§¤í•‘
    class Translator:
        def __init__(self):
            pass
        
        def translate(self, text, dest='en', src='auto'):
            translator = GoogleTranslator(source=src, target=dest)
            result = translator.translate(text)
            # googletransì™€ ë¹„ìŠ·í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            class TranslationResult:
                def __init__(self, text):
                    self.text = text
            return TranslationResult(result)
            
except ImportError:
    TRANSLATION_AVAILABLE = False

try:
    import spacy
    import nltk
    NLP_AVAILABLE = True
except ImportError:
    NLP_AVAILABLE = False

# ==================== ì‹¤í—˜ ì„¤ê³„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ====================
try:
    import pyDOE2
    PYDOE_AVAILABLE = True
except ImportError:
    PYDOE_AVAILABLE = False

try:
    from smt.sampling_methods import LHS
    from smt.surrogate_models import KRG
    SMT_AVAILABLE = True
except ImportError:
    SMT_AVAILABLE = False

# ==================== ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ====================
try:
    import pdfkit
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph
    from reportlab.lib.styles import getSampleStyleSheet
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    import qrcode
    QRCODE_AVAILABLE = True
except ImportError:
    QRCODE_AVAILABLE = False

# ==================== ì„¤ì • ë° ìƒìˆ˜ ====================
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
logging.basicConfig(level=logging.INFO, format=log_format)

# íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
file_handler = logging.FileHandler('polymer_doe.log')
file_handler.setFormatter(logging.Formatter(log_format))
logger = logging.getLogger(__name__)
logger.addHandler(file_handler)

# ë²„ì „ ì •ë³´
VERSION = "4.0.0"
BUILD_DATE = "2024-01-20"
API_VERSION = "v1"

# ì§€ì› ì–¸ì–´ (í™•ì¥)
SUPPORTED_LANGUAGES = {
    'ko': 'í•œêµ­ì–´',
    'en': 'English',
    'ja': 'æ—¥æœ¬èª',
    'zh-cn': 'ç®€ä½“ä¸­æ–‡',
    'zh-tw': 'ç¹é«”ä¸­æ–‡',
    'de': 'Deutsch',
    'fr': 'FranÃ§ais',
    'es': 'EspaÃ±ol',
    'it': 'Italiano',
    'pt': 'PortuguÃªs',
    'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹',
    'ar': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©',
    'hi': 'à¤¹à¤¿à¤¨à¥à¤¦à¥€',
    'th': 'à¹„à¸—à¸¢',
    'vi': 'Tiáº¿ng Viá»‡t'
}

# ì‹¤í—˜ ì„¤ê³„ ë°©ë²• (í™•ì¥)
DESIGN_METHODS = {
    'full_factorial': {
        'name': 'ì™„ì „ ìš”ì¸ ì„¤ê³„ (Full Factorial)',
        'description': 'ëª¨ë“  ì¸ì ì¡°í•©ì„ ì‹œí—˜í•˜ëŠ” ê°€ì¥ ì™„ì „í•œ ì„¤ê³„',
        'pros': ['ì™„ì „í•œ ì •ë³´', 'ëª¨ë“  ìƒí˜¸ì‘ìš© íŒŒì•… ê°€ëŠ¥'],
        'cons': ['ì‹¤í—˜ ìˆ˜ê°€ ë§ìŒ', 'ë¹„ìš©ì´ ë†’ìŒ'],
        'suitable_for': 'ì¸ì ìˆ˜ê°€ ì ì€ ê²½ìš° (2-4ê°œ)',
        'min_factors': 2,
        'max_factors': 5
    },
    'fractional_factorial': {
        'name': 'ë¶€ë¶„ ìš”ì¸ ì„¤ê³„ (Fractional Factorial)',
        'description': 'ì£¼ìš” íš¨ê³¼ì™€ ì¼ë¶€ ìƒí˜¸ì‘ìš©ë§Œ ì¶”ì •í•˜ëŠ” íš¨ìœ¨ì  ì„¤ê³„',
        'pros': ['ì‹¤í—˜ ìˆ˜ ì ˆê°', 'íš¨ìœ¨ì '],
        'cons': ['ì¼ë¶€ ìƒí˜¸ì‘ìš© í˜¼ë™', 'í•´ìƒë„ ì œí•œ'],
        'suitable_for': 'ìŠ¤í¬ë¦¬ë‹ ì‹¤í—˜, ë§ì€ ì¸ì',
        'min_factors': 3,
        'max_factors': 15
    },
    'plackett_burman': {
        'name': 'Plackett-Burman ì„¤ê³„',
        'description': 'ì£¼íš¨ê³¼ë§Œ ì¶”ì •í•˜ëŠ” ìŠ¤í¬ë¦¬ë‹ ì„¤ê³„',
        'pros': ['ë§¤ìš° íš¨ìœ¨ì ', 'ë§ì€ ì¸ì ì²˜ë¦¬ ê°€ëŠ¥'],
        'cons': ['ìƒí˜¸ì‘ìš© ì¶”ì • ë¶ˆê°€', '2ìˆ˜ì¤€ë§Œ ê°€ëŠ¥'],
        'suitable_for': 'ì´ˆê¸° ìŠ¤í¬ë¦¬ë‹',
        'min_factors': 3,
        'max_factors': 47
    },
    'box_behnken': {
        'name': 'Box-Behnken ì„¤ê³„',
        'description': '2ì°¨ ëª¨ë¸ì„ ìœ„í•œ 3ìˆ˜ì¤€ ì„¤ê³„',
        'pros': ['ê·¹ê°’ ì¡°ê±´ íšŒí”¼', 'íš¨ìœ¨ì ì¸ 2ì°¨ ëª¨ë¸'],
        'cons': ['3ê°œ ì´ìƒ ì¸ì í•„ìš”', 'ì •ìœ¡ë©´ì²´ ì˜ì—­ë§Œ'],
        'suitable_for': 'ë°˜ì‘í‘œë©´ ëª¨ë¸ë§',
        'min_factors': 3,
        'max_factors': 7
    },
    'central_composite': {
        'name': 'ì¤‘ì‹¬ í•©ì„± ì„¤ê³„ (CCD)',
        'description': '2ì°¨ ëª¨ë¸ì„ ìœ„í•œ í‘œì¤€ ì„¤ê³„',
        'pros': ['íšŒì „ ê°€ëŠ¥', 'ìˆœì°¨ì  ì‹¤í—˜ ê°€ëŠ¥'],
        'cons': ['ì¶•ì ì´ ë²”ìœ„ ë°–ì¼ ìˆ˜ ìˆìŒ'],
        'suitable_for': 'ìµœì í™” ì‹¤í—˜',
        'min_factors': 2,
        'max_factors': 6
    },
    'latin_hypercube': {
        'name': 'ë¼í‹´ í•˜ì´í¼íë¸Œ ìƒ˜í”Œë§',
        'description': 'ê³µê°„ ì¶©ì§„ ì„¤ê³„',
        'pros': ['ê· ë“±í•œ ê³µê°„ íƒìƒ‰', 'ëª¨ë¸ ë¬´ê´€'],
        'cons': ['í†µê³„ì  íŠ¹ì„± ë¶€ì¡±'],
        'suitable_for': 'ì»´í“¨í„° ì‹¤í—˜, ì‹œë®¬ë ˆì´ì…˜',
        'min_factors': 1,
        'max_factors': 100
    },
    'taguchi': {
        'name': 'ë‹¤êµ¬ì¹˜ ì„¤ê³„',
        'description': 'ê°•ê±´ ì„¤ê³„ë¥¼ ìœ„í•œ ì§êµ ë°°ì—´',
        'pros': ['ì¡ìŒ ì¸ì ê³ ë ¤', 'ê°•ê±´ì„±'],
        'cons': ['ìƒí˜¸ì‘ìš© ì œí•œì '],
        'suitable_for': 'í’ˆì§ˆ ê°œì„ , ê°•ê±´ ì„¤ê³„',
        'min_factors': 2,
        'max_factors': 50
    },
    'mixture': {
        'name': 'í˜¼í•©ë¬¼ ì„¤ê³„',
        'description': 'ì„±ë¶„ í•©ì´ ì¼ì •í•œ ì‹¤í—˜',
        'pros': ['í˜¼í•©ë¬¼ íŠ¹í™”', 'ì œì•½ ì¡°ê±´ ì²˜ë¦¬'],
        'cons': ['íŠ¹ìˆ˜í•œ ë¶„ì„ í•„ìš”'],
        'suitable_for': 'ì¡°ì„± ìµœì í™”',
        'min_factors': 3,
        'max_factors': 10
    },
    'optimal': {
        'name': 'ìµœì  ì„¤ê³„ (D-optimal)',
        'description': 'ì •ë³´ëŸ‰ì„ ìµœëŒ€í™”í•˜ëŠ” ì„¤ê³„',
        'pros': ['ìœ ì—°í•œ ì„¤ê³„', 'ì œì•½ ì¡°ê±´ ì²˜ë¦¬'],
        'cons': ['ê³„ì‚° ì§‘ì•½ì '],
        'suitable_for': 'ë¹„í‘œì¤€ ìƒí™©',
        'min_factors': 1,
        'max_factors': 20
    },
    'adaptive': {
        'name': 'ì ì‘í˜• ì„¤ê³„',
        'description': 'ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ì‹¤í—˜ì  ê²°ì •',
        'pros': ['íš¨ìœ¨ì  íƒìƒ‰', 'ì‹¤ì‹œê°„ ìµœì í™”'],
        'cons': ['ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜'],
        'suitable_for': 'ê³ ê°€ ì‹¤í—˜, ì‹¤ì‹œê°„ ìµœì í™”',
        'min_factors': 1,
        'max_factors': 10
    },
    'sequential': {
        'name': 'ìˆœì°¨ì  ì„¤ê³„',
        'description': 'ë‹¨ê³„ë³„ë¡œ ì •ë°€ë„ë¥¼ ë†’ì´ëŠ” ì„¤ê³„',
        'pros': ['ë¦¬ìŠ¤í¬ ê°ì†Œ', 'ë‹¨ê³„ì  ê°œì„ '],
        'cons': ['ì‹œê°„ ì†Œìš”'],
        'suitable_for': 'ì¥ê¸° í”„ë¡œì íŠ¸',
        'min_factors': 1,
        'max_factors': 20
    },
    'space_filling': {
        'name': 'ê³µê°„ ì¶©ì§„ ì„¤ê³„',
        'description': 'ì‹¤í—˜ ì˜ì—­ì„ ê· ë“±í•˜ê²Œ ì»¤ë²„',
        'pros': ['ëª¨ë¸ ë…ë¦½ì ', 'íƒìƒ‰ì '],
        'cons': ['í†µê³„ì  ìµœì ì„± ë¶€ì¡±'],
        'suitable_for': 'ë¯¸ì§€ ì‹œìŠ¤í…œ íƒìƒ‰',
        'min_factors': 1,
        'max_factors': 50
    },
    'bayesian': {
        'name': 'ë² ì´ì§€ì•ˆ ìµœì í™”',
        'description': 'í™•ë¥  ëª¨ë¸ ê¸°ë°˜ ìˆœì°¨ì  ì„¤ê³„',
        'pros': ['ë§¤ìš° íš¨ìœ¨ì ', 'ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”'],
        'cons': ['ê³„ì‚° ë³µì¡', 'ì´ˆê¸° ë°ì´í„° í•„ìš”'],
        'suitable_for': 'ê³ ë¹„ìš© ì‹¤í—˜',
        'min_factors': 1,
        'max_factors': 20
    }
}

# ê³ ë¶„ì ìœ í˜• (í™•ì¥ ë° ìœ ì—°í™”)
POLYMER_CATEGORIES = {
    'base_types': {
        'thermoplastic': {
            'name': 'ì—´ê°€ì†Œì„± ê³ ë¶„ì',
            'description': 'ê°€ì—´ ì‹œ ì—°í™”ë˜ê³  ëƒ‰ê° ì‹œ ê²½í™”ë˜ëŠ” ê³ ë¶„ì',
            'subcategories': ['ë²”ìš©', 'ì—”ì§€ë‹ˆì–´ë§', 'ìŠˆí¼ ì—”ì§€ë‹ˆì–´ë§'],
            'examples': ['PE', 'PP', 'PS', 'PVC', 'PET', 'PA', 'PC', 'PMMA', 'POM', 'PEEK', 'PPS', 'PSU'],
            'typical_properties': ['ë…¹ëŠ”ì ', 'ìœ ë¦¬ì „ì´ì˜¨ë„', 'ìš©ìœµì§€ìˆ˜', 'ì¸ì¥ê°•ë„', 'ì‹ ìœ¨', 'ì¶©ê²©ê°•ë„', 'ê²½ë„'],
            'processing_methods': ['ì‚¬ì¶œì„±í˜•', 'ì••ì¶œ', 'ë¸”ë¡œìš°ì„±í˜•', 'ì—´ì„±í˜•', '3Dí”„ë¦°íŒ…']
        },
        'thermosetting': {
            'name': 'ì—´ê²½í™”ì„± ê³ ë¶„ì',
            'description': 'ê°€ì—´ ì‹œ í™”í•™ë°˜ì‘ìœ¼ë¡œ ê²½í™”ë˜ëŠ” ê³ ë¶„ì',
            'subcategories': ['ì—í­ì‹œ', 'í´ë¦¬ì—ìŠ¤í„°', 'í˜ë†€', 'í´ë¦¬ìš°ë ˆíƒ„'],
            'examples': ['Epoxy', 'UP', 'VE', 'Phenolic', 'PU', 'Silicone', 'BMI', 'PI'],
            'typical_properties': ['ê²½í™”ì‹œê°„', 'ê²½í™”ì˜¨ë„', 'ê°€êµë°€ë„', 'ê²½ë„', 'ë‚´ì—´ì„±', 'ì ‘ì°©ê°•ë„', 'ìˆ˜ì¶•ë¥ '],
            'processing_methods': ['RTM', 'SMC', 'BMC', 'í•¸ë“œë ˆì´ì—…', 'í•„ë¼ë©˜íŠ¸ì™€ì¸ë”©', 'ì˜¤í† í´ë ˆì´ë¸Œ']
        },
        'elastomer': {
            'name': 'íƒ„ì„±ì²´',
            'description': 'ê³ ë¬´ì™€ ê°™ì€ íƒ„ì„±ì„ ê°€ì§„ ê³ ë¶„ì',
            'subcategories': ['ì²œì—°ê³ ë¬´', 'í•©ì„±ê³ ë¬´', 'ì—´ê°€ì†Œì„± íƒ„ì„±ì²´'],
            'examples': ['NR', 'SBR', 'NBR', 'EPDM', 'Silicone', 'TPE', 'TPU', 'TPV', 'FKM'],
            'typical_properties': ['ê²½ë„', 'ì¸ì¥ê°•ë„', 'ì‹ ìœ¨', 'ë°˜ë°œíƒ„ì„±', 'ì••ì¶•ì˜êµ¬ë³€í˜•', 'ì¸ì—´ê°•ë„', 'ë‚´ë§ˆëª¨ì„±'],
            'processing_methods': ['ì»´íŒŒìš´ë”©', 'ìº˜ë¦°ë”ë§', 'ì••ì¶œ', 'ì‚¬ì¶œì„±í˜•', 'ê°€í™©']
        },
        'biopolymer': {
            'name': 'ë°”ì´ì˜¤ ê³ ë¶„ì',
            'description': 'ìƒë¬¼ ìœ ë˜ ë˜ëŠ” ìƒë¶„í•´ì„± ê³ ë¶„ì',
            'subcategories': ['ì²œì—° ê³ ë¶„ì', 'ë°”ì´ì˜¤ ê¸°ë°˜', 'ìƒë¶„í•´ì„±'],
            'examples': ['PLA', 'PHA', 'PBS', 'Starch', 'Cellulose', 'Chitosan', 'Alginate', 'Collagen'],
            'typical_properties': ['ìƒë¶„í•´ì„±', 'ìƒì²´ì í•©ì„±', 'ê¸°ê³„ì ê°•ë„', 'ê°€ê³µì„±', 'ì•ˆì •ì„±', 'ìˆ˜ë¶„í¡ìˆ˜ìœ¨', 'ê²°ì •í™”ë„'],
            'processing_methods': ['ìš©ì•¡ìºìŠ¤íŒ…', 'ì „ê¸°ë°©ì‚¬', '3Dë°”ì´ì˜¤í”„ë¦°íŒ…', 'ì••ì¶œ', 'ì‚¬ì¶œì„±í˜•']
        },
        'conducting': {
            'name': 'ì „ë„ì„± ê³ ë¶„ì',
            'description': 'ì „ê¸° ì „ë„ì„±ì„ ê°€ì§„ íŠ¹ìˆ˜ ê³ ë¶„ì',
            'subcategories': ['ë³¸ì§ˆì „ë„ì„±', 'ë³µí•©ì „ë„ì„±'],
            'examples': ['PANI', 'PPy', 'PEDOT', 'PTh', 'PAc', 'P3HT', 'Graphene composite'],
            'typical_properties': ['ì „ê¸°ì „ë„ë„', 'ë„í•‘ë ˆë²¨', 'ì•ˆì •ì„±', 'ê°€ê³µì„±', 'ê´‘í•™íŠ¹ì„±', 'ìºë¦¬ì–´ ì´ë™ë„', 'ì¼í•¨ìˆ˜'],
            'processing_methods': ['ì „ê¸°ì¤‘í•©', 'í™”í•™ì¤‘í•©', 'ìŠ¤í•€ì½”íŒ…', 'ì‰í¬ì ¯í”„ë¦°íŒ…', 'ì¦ì°©']
        },
        'composite': {
            'name': 'ë³µí•©ì¬ë£Œ',
            'description': 'ê°•í™”ì¬ì™€ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ êµ¬ì„±ëœ ì¬ë£Œ',
            'subcategories': ['ì„¬ìœ ê°•í™”', 'ì…ìê°•í™”', 'ë‚˜ë…¸ë³µí•©ì¬'],
            'examples': ['CFRP', 'GFRP', 'AFRP', 'CNT composite', 'Graphene composite', 'Clay nanocomposite'],
            'typical_properties': ['ì¸ì¥ê°•ë„', 'êµ´ê³¡ê°•ë„', 'ì¶©ê²©ê°•ë„', 'ê³„ë©´ì ‘ì°©ë ¥', 'ë¶„ì‚°ë„', 'ì„¬ìœ í•¨ëŸ‰', 'ê³µê·¹ë¥ '],
            'processing_methods': ['í”„ë¦¬í”„ë ˆê·¸', 'RTM', 'VARTM', 'í•„ë¼ë©˜íŠ¸ì™€ì¸ë”©', 'AFP', '3Dí”„ë¦°íŒ…']
        },
        'inorganic': {
            'name': 'ë¬´ê¸° ê³ ë¶„ì',
            'description': 'íƒ„ì†Œ ëŒ€ì‹  ë‹¤ë¥¸ ì›ì†Œê°€ ì£¼ì‚¬ìŠ¬ì„ ì´ë£¨ëŠ” ê³ ë¶„ì',
            'subcategories': ['ì‹¤ë¦¬ì½˜ê³„', 'ì¸ê³„', 'ë¶•ì†Œê³„'],
            'examples': ['Silicone', 'Phosphazene', 'Polysilane', 'Polysiloxane', 'Polyphosphate', 'Sol-gel'],
            'typical_properties': ['ë‚´ì—´ì„±', 'í™”í•™ì ì•ˆì •ì„±', 'ê¸°ê³„ì íŠ¹ì„±', 'ê´‘í•™íŠ¹ì„±', 'ìœ ì „íŠ¹ì„±', 'ì—´íŒ½ì°½ê³„ìˆ˜'],
            'processing_methods': ['ì¡¸ê²”', 'CVD', 'ìŠ¤í•€ì½”íŒ…', 'ë”¥ì½”íŒ…', 'ìŠ¤í”„ë ˆì´']
        }
    },
    'special_types': {
        'smart': {
            'name': 'ìŠ¤ë§ˆíŠ¸ ê³ ë¶„ì',
            'description': 'ì™¸ë¶€ ìê·¹ì— ë°˜ì‘í•˜ëŠ” ê³ ë¶„ì',
            'types': ['í˜•ìƒê¸°ì–µ', 'ìê°€ì¹˜ìœ ', 'ìê·¹ì‘ë‹µì„±'],
            'stimuli': ['ì˜¨ë„', 'pH', 'ë¹›', 'ì „ê¸°', 'ìê¸°ì¥']
        },
        'functional': {
            'name': 'ê¸°ëŠ¥ì„± ê³ ë¶„ì',
            'description': 'íŠ¹ìˆ˜ ê¸°ëŠ¥ì„ ê°€ì§„ ê³ ë¶„ì',
            'types': ['ì˜ë£Œìš©', 'ê´‘í•™ìš©', 'ì „ìì¬ë£Œìš©', 'ë¶„ë¦¬ë§‰ìš©']
        }
    }
}

# API ìƒíƒœ
class APIStatus(Enum):
    """API ì—°ê²° ìƒíƒœ"""
    ONLINE = "ğŸŸ¢ ì˜¨ë¼ì¸"
    OFFLINE = "ğŸ”´ ì˜¤í”„ë¼ì¸"
    ERROR = "âš ï¸ ì˜¤ë¥˜"
    RATE_LIMITED = "â±ï¸ ì†ë„ ì œí•œ"
    UNAUTHORIZED = "ğŸ” ì¸ì¦ í•„ìš”"
    MAINTENANCE = "ğŸ”§ ìœ ì§€ë³´ìˆ˜"

# ì‚¬ìš©ì ë ˆë²¨
class UserLevel(Enum):
    """ì‚¬ìš©ì ìˆ™ë ¨ë„"""
    BEGINNER = (1, "ğŸŒ± ì´ˆë³´ì", "ìƒì„¸í•œ ì„¤ëª…ê³¼ ê°€ì´ë“œ ì œê³µ", 0.9)
    INTERMEDIATE = (2, "ğŸŒ¿ ì¤‘ê¸‰ì", "ì„ íƒì§€ì™€ ê¶Œì¥ì‚¬í•­ ì œê³µ", 0.7)
    ADVANCED = (3, "ğŸŒ³ ê³ ê¸‰ì", "ììœ ë¡œìš´ ì„¤ì •ê³¼ ê³ ê¸‰ ê¸°ëŠ¥", 0.3)
    EXPERT = (4, "ğŸ“ ì „ë¬¸ê°€", "ì™„ì „í•œ ì œì–´ì™€ ì»¤ìŠ¤í„°ë§ˆì´ì§•", 0.1)
    
    def __init__(self, level, icon, description, help_ratio):
        self.level = level
        self.icon = icon
        self.description = description
        self.help_ratio = help_ratio  # ë„ì›€ë§ í‘œì‹œ ë¹„ìœ¨

# ì‹¤í—˜ ìƒíƒœ
class ExperimentStatus(Enum):
    """ì‹¤í—˜ ì§„í–‰ ìƒíƒœ"""
    PLANNED = "ğŸ“‹ ê³„íšë¨"
    IN_PROGRESS = "ğŸ”¬ ì§„í–‰ì¤‘"
    COMPLETED = "âœ… ì™„ë£Œ"
    FAILED = "âŒ ì‹¤íŒ¨"
    PAUSED = "â¸ï¸ ì¼ì‹œì •ì§€"
    CANCELLED = "ğŸš« ì·¨ì†Œë¨"

# ë¶„ì„ ìœ í˜•
class AnalysisType(Enum):
    """í†µê³„ ë¶„ì„ ìœ í˜•"""
    DESCRIPTIVE = "ê¸°ìˆ í†µê³„"
    ANOVA = "ë¶„ì‚°ë¶„ì„"
    REGRESSION = "íšŒê·€ë¶„ì„"
    RSM = "ë°˜ì‘í‘œë©´ë¶„ì„"
    OPTIMIZATION = "ìµœì í™”"
    PCA = "ì£¼ì„±ë¶„ë¶„ì„"
    CORRELATION = "ìƒê´€ë¶„ì„"
    TIME_SERIES = "ì‹œê³„ì—´ë¶„ì„"
    MACHINE_LEARNING = "ê¸°ê³„í•™ìŠµ"

# Polymer-doe-platform - Part 2
# ==================== íƒ€ì… ì •ì˜ ====================
T = TypeVar('T')
FactorType = Union[float, int, str, bool]
ResponseType = Union[float, int]

# ==================== ë°ì´í„° í´ë˜ìŠ¤ ====================
@dataclass
class ExperimentFactor:
    """ì‹¤í—˜ ì¸ì ì •ì˜ (í™•ì¥íŒ)"""
    name: str
    unit: str = ""
    min_value: float = 0.0
    max_value: float = 100.0
    levels: List[float] = field(default_factory=list)
    categorical: bool = False
    categories: List[str] = field(default_factory=list)
    description: str = ""
    constraints: List[str] = field(default_factory=list)
    importance: float = 1.0  # ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
    cost: float = 1.0  # ë¹„ìš© ê°€ì¤‘ì¹˜
    difficulty: float = 1.0  # ì‹¤í—˜ ë‚œì´ë„
    tolerance: float = 0.01  # í—ˆìš© ì˜¤ì°¨
    controllable: bool = True  # ì œì–´ ê°€ëŠ¥ ì—¬ë¶€
    noise_factor: bool = False  # ì¡ìŒ ì¸ì ì—¬ë¶€
    transformation: Optional[str] = None  # ë³€í™˜ í•¨ìˆ˜ (log, sqrt ë“±)
    
    def validate(self) -> Tuple[bool, List[str]]:
        """ì¸ì ìœ íš¨ì„± ê²€ì¦"""
        errors = []
        
        if not self.name:
            errors.append("ì¸ì ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if not self.categorical:
            if self.min_value >= self.max_value:
                errors.append(f"{self.name}: ìµœì†Œê°’ì´ ìµœëŒ€ê°’ë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.")
            
            if self.levels:
                for level in self.levels:
                    if level < self.min_value or level > self.max_value:
                        errors.append(f"{self.name}: ìˆ˜ì¤€ {level}ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
        else:
            if not self.categories:
                errors.append(f"{self.name}: ë²”ì£¼í˜• ì¸ìëŠ” ì¹´í…Œê³ ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return len(errors) == 0, errors
    
    def get_levels(self, n_levels: int = None) -> List[FactorType]:
        """ìˆ˜ì¤€ ëª©ë¡ ë°˜í™˜"""
        if self.categorical:
            return self.categories[:n_levels] if n_levels else self.categories
        
        if self.levels:
            return self.levels[:n_levels] if n_levels else self.levels
        
        # ìˆ˜ì¤€ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ìƒì„±
        if n_levels:
            if n_levels == 2:
                return [self.min_value, self.max_value]
            else:
                return np.linspace(self.min_value, self.max_value, n_levels).tolist()
        
        return [self.min_value, self.max_value]
    
    def apply_transformation(self, value: float) -> float:
        """ë³€í™˜ ì ìš©"""
        if not self.transformation or self.categorical:
            return value
        
        if self.transformation == 'log':
            return np.log(value) if value > 0 else 0
        elif self.transformation == 'sqrt':
            return np.sqrt(value) if value >= 0 else 0
        elif self.transformation == 'inverse':
            return 1 / value if value != 0 else float('inf')
        elif self.transformation == 'square':
            return value ** 2
        
        return value

@dataclass
class ExperimentResponse:
    """ë°˜ì‘ ë³€ìˆ˜ ì •ì˜ (í™•ì¥íŒ)"""
    name: str
    unit: str = ""
    target_value: Optional[float] = None
    minimize: bool = False
    maximize: bool = False
    weight: float = 1.0
    specification_limits: Tuple[Optional[float], Optional[float]] = (None, None)
    transformation: Optional[str] = None
    measurement_error: float = 0.0  # ì¸¡ì • ì˜¤ì°¨
    cost_per_measurement: float = 0.0  # ì¸¡ì • ë¹„ìš©
    measurement_time: float = 0.0  # ì¸¡ì • ì‹œê°„ (ë¶„)
    critical: bool = False  # í•µì‹¬ ë°˜ì‘ ì—¬ë¶€
    
    def is_within_spec(self, value: float) -> bool:
        """ê·œê²© ë‚´ ì—¬ë¶€ í™•ì¸"""
        lower, upper = self.specification_limits
        
        if lower is not None and value < lower:
            return False
        if upper is not None and value > upper:
            return False
        
        return True
    
    def calculate_desirability(self, value: float) -> float:
        """ë°”ëŒì§í•¨ ì§€ìˆ˜ ê³„ì‚° (0-1)"""
        if self.target_value is not None:
            # ëª©í‘œê°’ì— ê°€ê¹Œìš¸ìˆ˜ë¡ 1
            deviation = abs(value - self.target_value)
            return np.exp(-deviation / abs(self.target_value))
        
        elif self.minimize:
            lower, upper = self.specification_limits
            if lower is None:
                return 0.5  # ì •ë³´ ë¶€ì¡±
            
            if value <= lower:
                return 1.0
            elif upper is not None and value >= upper:
                return 0.0
            else:
                return (upper - value) / (upper - lower) if upper else 0.5
        
        elif self.maximize:
            lower, upper = self.specification_limits
            if upper is None:
                return 0.5  # ì •ë³´ ë¶€ì¡±
            
            if value >= upper:
                return 1.0
            elif lower is not None and value <= lower:
                return 0.0
            else:
                return (value - lower) / (upper - lower) if lower else 0.5
        
        return 0.5  # ê¸°ë³¸ê°’

@dataclass
class ProjectInfo:
    """í”„ë¡œì íŠ¸ ì •ë³´ (í™•ì¥íŒ)"""
    id: str
    name: str
    description: str
    polymer_type: str
    polymer_system: Dict[str, Any]
    objectives: List[str]
    constraints: List[str]
    created_at: datetime
    updated_at: datetime
    owner: str
    collaborators: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    version: int = 1
    parent_project_id: Optional[str] = None
    status: str = "active"
    budget: Optional[float] = None
    deadline: Optional[datetime] = None
    notes: str = ""
    attachments: List[str] = field(default_factory=list)
    custom_fields: Dict[str, Any] = field(default_factory=dict)
    
    def add_collaborator(self, user_id: str):
        """í˜‘ì—…ì ì¶”ê°€"""
        if user_id not in self.collaborators:
            self.collaborators.append(user_id)
            self.updated_at = datetime.now()
    
    def add_tag(self, tag: str):
        """íƒœê·¸ ì¶”ê°€"""
        if tag not in self.tags:
            self.tags.append(tag)
            self.updated_at = datetime.now()
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

@dataclass
class ExperimentData:
    """ì‹¤í—˜ ë°ì´í„° (í™•ì¥íŒ)"""
    id: str
    project_id: str
    design_matrix: pd.DataFrame
    results: Optional[pd.DataFrame] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    status: ExperimentStatus = ExperimentStatus.PLANNED
    run_order: Optional[List[int]] = None
    block_structure: Optional[Dict[str, List[int]]] = None
    replicates: Dict[int, List[int]] = field(default_factory=dict)
    conditions: Dict[str, Any] = field(default_factory=dict)
    operator: Optional[str] = None
    equipment: Optional[str] = None
    notes: Dict[int, str] = field(default_factory=dict)
    
    def get_completed_runs(self) -> List[int]:
        """ì™„ë£Œëœ ì‹¤í—˜ ë²ˆí˜¸ ëª©ë¡"""
        if self.results is None:
            return []
        
        return self.results[~self.results.isnull().any(axis=1)].index.tolist()
    
    def get_progress(self) -> float:
        """ì§„í–‰ë¥  ê³„ì‚° (0-100%)"""
        total_runs = len(self.design_matrix)
        completed_runs = len(self.get_completed_runs())
        
        return (completed_runs / total_runs * 100) if total_runs > 0 else 0

@dataclass
class AIResponse:
    """AI ì‘ë‹µ ë°ì´í„°"""
    success: bool
    content: str
    model: str
    tokens_used: int = 0
    response_time: float = 0.0
    confidence: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None

@dataclass
class LearningRecord:
    """í•™ìŠµ ê¸°ë¡ ë°ì´í„°"""
    id: str
    timestamp: datetime
    user_id: str
    user_level: UserLevel
    action_type: str
    action_details: Dict[str, Any]
    outcome: Dict[str, Any]
    quality_score: float
    context: Dict[str, Any] = field(default_factory=dict)
    feedback: Optional[str] = None

# ==================== ì˜ˆì™¸ ì²˜ë¦¬ ====================
class PolymerDOEException(Exception):
    """í”Œë«í¼ ê¸°ë³¸ ì˜ˆì™¸"""
    def __init__(self, message: str, error_code: str = None, details: Dict[str, Any] = None):
        super().__init__(message)
        self.error_code = error_code
        self.details = details or {}
        self.timestamp = datetime.now()
        
        # ë¡œê¹…
        logger.error(f"Exception: {error_code} - {message}", extra=self.details)

class APIException(PolymerDOEException):
    """API ê´€ë ¨ ì˜ˆì™¸"""
    pass

class ValidationException(PolymerDOEException):
    """ê²€ì¦ ì‹¤íŒ¨ ì˜ˆì™¸"""
    pass

class DataException(PolymerDOEException):
    """ë°ì´í„° ê´€ë ¨ ì˜ˆì™¸"""
    pass

class DesignException(PolymerDOEException):
    """ì‹¤í—˜ ì„¤ê³„ ê´€ë ¨ ì˜ˆì™¸"""
    pass

class AnalysisException(PolymerDOEException):
    """ë¶„ì„ ê´€ë ¨ ì˜ˆì™¸"""
    pass

# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================
def timeit(func: Callable) -> Callable:
    """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        start = time.time()
        result = await func(*args, **kwargs)
        end = time.time()
        logger.debug(f"{func.__name__} ì‹¤í–‰ ì‹œê°„: {end - start:.3f}ì´ˆ")
        return result
    
    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        logger.debug(f"{func.__name__} ì‹¤í–‰ ì‹œê°„: {end - start:.3f}ì´ˆ")
        return result
    
    if asyncio.iscoroutinefunction(func):
        return async_wrapper
    else:
        return sync_wrapper

def retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0):
    """ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            attempt = 0
            current_delay = delay
            
            while attempt < max_attempts:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    attempt += 1
                    if attempt >= max_attempts:
                        logger.error(f"{func.__name__} ì‹¤íŒ¨ (ì‹œë„ {attempt}/{max_attempts}): {e}")
                        raise
                    
                    logger.warning(f"{func.__name__} ì¬ì‹œë„ {attempt}/{max_attempts} ({current_delay}ì´ˆ ëŒ€ê¸°)")
                    await asyncio.sleep(current_delay)
                    current_delay *= backoff
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            attempt = 0
            current_delay = delay
            
            while attempt < max_attempts:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    attempt += 1
                    if attempt >= max_attempts:
                        logger.error(f"{func.__name__} ì‹¤íŒ¨ (ì‹œë„ {attempt}/{max_attempts}): {e}")
                        raise
                    
                    logger.warning(f"{func.__name__} ì¬ì‹œë„ {attempt}/{max_attempts} ({current_delay}ì´ˆ ëŒ€ê¸°)")
                    time.sleep(current_delay)
                    current_delay *= backoff
        
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

def validate_input(value: Any, 
                  min_val: float = None, 
                  max_val: float = None,
                  allowed_values: List = None,
                  value_type: type = None) -> Tuple[bool, Optional[str]]:
    """ì…ë ¥ê°’ ê²€ì¦"""
    if value is None:
        return False, "ê°’ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    if value_type and not isinstance(value, value_type):
        return False, f"íƒ€ì…ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. {value_type.__name__}ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    
    if min_val is not None and value < min_val:
        return False, f"ìµœì†Œê°’ {min_val} ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    
    if max_val is not None and value > max_val:
        return False, f"ìµœëŒ€ê°’ {max_val} ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤."
    
    if allowed_values is not None and value not in allowed_values:
        return False, f"í—ˆìš©ëœ ê°’ì´ ì•„ë‹™ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {allowed_values}"
    
    return True, None

def generate_unique_id(prefix: str = "EXP") -> str:
    """ê³ ìœ  ID ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    random_part = hashlib.md5(f"{timestamp}{uuid.uuid4()}".encode()).hexdigest()[:6]
    return f"{prefix}_{timestamp}_{random_part}"

def safe_float_conversion(value: Any, default: float = 0.0) -> float:
    """ì•ˆì „í•œ float ë³€í™˜"""
    if value is None:
        return default
    
    try:
        # ë¬¸ìì—´ì¸ ê²½ìš° ì‰¼í‘œ ì œê±°
        if isinstance(value, str):
            value = value.replace(',', '')
        
        return float(value)
    except (ValueError, TypeError):
        logger.warning(f"Float ë³€í™˜ ì‹¤íŒ¨: {value}")
        return default

def format_number(value: float, 
                 decimals: int = 2, 
                 use_scientific: bool = True,
                 threshold: float = 1e6) -> str:
    """ìˆ«ì í¬ë§·íŒ…"""
    if pd.isna(value):
        return "N/A"
    
    if use_scientific and (abs(value) >= threshold or (abs(value) < 1e-3 and value != 0)):
        return f"{value:.{decimals}e}"
    else:
        return f"{value:,.{decimals}f}"

def sanitize_filename(filename: str) -> str:
    """íŒŒì¼ëª… ì •ë¦¬"""
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        filename = filename.replace(char, '_')
    
    # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ
    filename = filename.replace(' ', '_')
    
    # ê¸¸ì´ ì œí•œ
    max_length = 255
    if len(filename) > max_length:
        name, ext = os.path.splitext(filename)
        filename = name[:max_length - len(ext)] + ext
    
    return filename

@lru_cache(maxsize=128)
def calculate_hash(data: str) -> str:
    """ë°ì´í„° í•´ì‹œ ê³„ì‚° (ìºì‹±)"""
    return hashlib.sha256(data.encode()).hexdigest()

def create_backup(data: Any, backup_dir: str = "backups") -> str:
    """ë°ì´í„° ë°±ì—… ìƒì„±"""
    os.makedirs(backup_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = os.path.join(backup_dir, f"backup_{timestamp}.pkl")
    
    with open(backup_file, 'wb') as f:
        pickle.dump(data, f)
    
    logger.info(f"ë°±ì—… ìƒì„±: {backup_file}")
    return backup_file

def restore_backup(backup_file: str) -> Any:
    """ë°±ì—… ë³µì›"""
    if not os.path.exists(backup_file):
        raise FileNotFoundError(f"ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {backup_file}")
    
    with open(backup_file, 'rb') as f:
        data = pickle.load(f)
    
    logger.info(f"ë°±ì—… ë³µì›: {backup_file}")
    return data

class ProgressTracker:
    """ì§„í–‰ ìƒí™© ì¶”ì ê¸°"""
    def __init__(self, total: int, description: str = "Processing"):
        self.total = total
        self.current = 0
        self.description = description
        self.start_time = time.time()
        self.last_update = self.start_time
        
    def update(self, increment: int = 1):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        self.current += increment
        current_time = time.time()
        
        # 0.5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
        if current_time - self.last_update > 0.5:
            self.last_update = current_time
            self._display_progress()
    
    def _display_progress(self):
        """ì§„í–‰ë¥  í‘œì‹œ"""
        if self.total == 0:
            return
        
        progress = self.current / self.total
        elapsed = time.time() - self.start_time
        eta = elapsed / progress - elapsed if progress > 0 else 0
        
        bar_length = 30
        filled_length = int(bar_length * progress)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        st.progress(progress)
        st.text(f"{self.description}: {bar} {progress*100:.1f}% (ETA: {eta:.1f}s)")
    
    def finish(self):
        """ì™„ë£Œ"""
        self.current = self.total
        self._display_progress()
        elapsed = time.time() - self.start_time
        st.success(f"{self.description} ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")

# ==================== ì´ˆë³´ìë¥¼ ìœ„í•œ ë„ì›€ë§ ì‹œìŠ¤í…œ ====================
class HelpSystem:
    """ìƒí™©ë³„ ë„ì›€ë§ ì œê³µ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.help_database = {
            'factor_selection': {
                'title': 'ğŸ¯ ì‹¤í—˜ ì¸ìë€?',
                'basic': """
                ì‹¤í—˜ ì¸ì(Factor)ëŠ” ì‹¤í—˜ ê²°ê³¼ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ë³€ìˆ˜ì…ë‹ˆë‹¤.
                
                ì˜ˆì‹œ:
                - ğŸŒ¡ï¸ **ì˜¨ë„**: ë°˜ì‘ ì†ë„ì— ì˜í–¥
                - â±ï¸ **ì‹œê°„**: ë°˜ì‘ ì™„ì„±ë„ì— ì˜í–¥
                - ğŸ§ª **ë†ë„**: ìƒì„±ë¬¼ ì–‘ì— ì˜í–¥
                """,
                'detailed': """
                ### ì¸ì ì„ íƒ ì‹œ ê³ ë ¤ì‚¬í•­
                
                1. **ì œì–´ ê°€ëŠ¥ì„±**: ì‹¤í—˜ ì¤‘ ì •í™•íˆ ì¡°ì ˆí•  ìˆ˜ ìˆë‚˜ìš”?
                2. **ì¸¡ì • ê°€ëŠ¥ì„±**: ì •í™•íˆ ì¸¡ì •í•  ìˆ˜ ìˆë‚˜ìš”?
                3. **ì˜í–¥ë ¥**: ê²°ê³¼ì— ì‹¤ì œë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ë‚˜ìš”?
                4. **ë…ë¦½ì„±**: ë‹¤ë¥¸ ì¸ìì™€ ë…ë¦½ì ì¸ê°€ìš”?
                
                ğŸ’¡ **íŒ**: ì²˜ìŒì—ëŠ” 2-3ê°œì˜ í•µì‹¬ ì¸ìë¡œ ì‹œì‘í•˜ì„¸ìš”!
                """,
                'examples': [
                    "ê³ ë¶„ì í•©ì„±: ì˜¨ë„, ì‹œê°„, ì´‰ë§¤ ë†ë„",
                    "ë³µí•©ì¬ë£Œ: ì„¬ìœ  í•¨ëŸ‰, ê²½í™” ì˜¨ë„, ì••ë ¥",
                    "ì½”íŒ…: ë‘ê»˜, ê±´ì¡° ì‹œê°„, ìš©ë§¤ ë¹„ìœ¨"
                ]
            },
            'design_method': {
                'title': 'ğŸ“Š ì‹¤í—˜ ì„¤ê³„ ë°©ë²• ì„ íƒ',
                'basic': """
                ì‹¤í—˜ ì„¤ê³„ëŠ” ì–´ë–¤ ì¡°ê±´ì—ì„œ ì‹¤í—˜í• ì§€ ì •í•˜ëŠ” ê³„íšì…ë‹ˆë‹¤.
                
                ì£¼ìš” ë°©ë²•:
                - **ì™„ì „ ìš”ì¸**: ëª¨ë“  ì¡°í•© (ì •í™•í•˜ì§€ë§Œ ì‹¤í—˜ ë§ìŒ)
                - **ë¶€ë¶„ ìš”ì¸**: ì¼ë¶€ ì¡°í•© (íš¨ìœ¨ì )
                - **ë°˜ì‘í‘œë©´**: ìµœì ì  ì°¾ê¸° (ê³ ê¸‰)
                """,
                'detailed': """
                ### ì„¤ê³„ ë°©ë²•ë³„ íŠ¹ì§•
                
                #### ğŸ”· ì™„ì „ ìš”ì¸ ì„¤ê³„
                - **ì¥ì **: ëª¨ë“  ì •ë³´ íšë“, ì´í•´í•˜ê¸° ì‰¬ì›€
                - **ë‹¨ì **: ì‹¤í—˜ ìˆ˜ê°€ ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€
                - **ì‚¬ìš© ì‹œê¸°**: ì¸ìê°€ ì ê³ (2-4ê°œ) ì •í™•í•œ ë¶„ì„ í•„ìš”í•  ë•Œ
                
                #### ğŸ”¶ ë¶€ë¶„ ìš”ì¸ ì„¤ê³„
                - **ì¥ì **: ì‹¤í—˜ ìˆ˜ í¬ê²Œ ê°ì†Œ
                - **ë‹¨ì **: ì¼ë¶€ ìƒí˜¸ì‘ìš© ì •ë³´ ì†ì‹¤
                - **ì‚¬ìš© ì‹œê¸°**: ìŠ¤í¬ë¦¬ë‹, ë§ì€ ì¸ì(5ê°œ ì´ìƒ)
                
                #### ğŸ”´ Box-Behnken ì„¤ê³„
                - **ì¥ì **: 2ì°¨ ëª¨ë¸, ê·¹ê°’ íšŒí”¼
                - **ë‹¨ì **: 3ê°œ ì´ìƒ ì¸ì í•„ìš”
                - **ì‚¬ìš© ì‹œê¸°**: ìµœì í™”, ê³¡ì„  ê´€ê³„
                """,
                'quiz': [
                    {
                        'question': "ì¸ìê°€ 2ê°œì´ê³  ê°ê° 3ìˆ˜ì¤€ì¼ ë•Œ, ì™„ì „ ìš”ì¸ ì„¤ê³„ì˜ ì‹¤í—˜ ìˆ˜ëŠ”?",
                        'answer': "9ê°œ (3 Ã— 3)",
                        'explanation': "ê° ì¸ìì˜ ìˆ˜ì¤€ì„ ê³±í•©ë‹ˆë‹¤."
                    }
                ]
            },
            'response_variable': {
                'title': 'ğŸ“ˆ ë°˜ì‘ ë³€ìˆ˜ë€?',
                'basic': """
                ë°˜ì‘ ë³€ìˆ˜(Response)ëŠ” ì‹¤í—˜ì—ì„œ ì¸¡ì •í•˜ëŠ” ê²°ê³¼ê°’ì…ë‹ˆë‹¤.
                
                ì˜ˆì‹œ:
                - ğŸ’ª **ì¸ì¥ê°•ë„**: ì¬ë£Œì˜ ê°•ë„
                - ğŸ“ **ì‹ ìœ¨**: ëŠ˜ì–´ë‚˜ëŠ” ì •ë„
                - ğŸŒ¡ï¸ **ìœ ë¦¬ì „ì´ì˜¨ë„**: ë¬¼ì„± ë³€í™” ì˜¨ë„
                """,
                'detailed': """
                ### ì¢‹ì€ ë°˜ì‘ ë³€ìˆ˜ì˜ ì¡°ê±´
                
                1. **ì •ëŸ‰ì **: ìˆ«ìë¡œ ì¸¡ì • ê°€ëŠ¥
                2. **ì¬í˜„ì„±**: ê°™ì€ ì¡°ê±´ì—ì„œ ë¹„ìŠ·í•œ ê°’
                3. **ë¯¼ê°ì„±**: ì¸ì ë³€í™”ì— ë°˜ì‘
                4. **ê´€ë ¨ì„±**: ì—°êµ¬ ëª©ì ê³¼ ì§ê²°
                
                ### ë°˜ì‘ ë³€ìˆ˜ ìœ í˜•
                - **ëª©í‘œê°’**: íŠ¹ì • ê°’ì— ë§ì¶”ê¸° (ì˜ˆ: pH 7.0)
                - **ìµœëŒ€í™”**: í´ìˆ˜ë¡ ì¢‹ìŒ (ì˜ˆ: ê°•ë„)
                - **ìµœì†Œí™”**: ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ (ì˜ˆ: ë¶ˆëŸ‰ë¥ )
                """
            },
            'analysis': {
                'title': 'ğŸ“Š ê²°ê³¼ ë¶„ì„ ì´í•´í•˜ê¸°',
                'basic': """
                ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ìì˜ ì˜í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
                
                ì£¼ìš” ë¶„ì„:
                - **ì£¼íš¨ê³¼**: ê° ì¸ìì˜ ì˜í–¥
                - **ìƒí˜¸ì‘ìš©**: ì¸ìë“¤ì˜ ë³µí•© ì˜í–¥
                - **ìµœì  ì¡°ê±´**: ê°€ì¥ ì¢‹ì€ ì„¤ì •
                """,
                'detailed': """
                ### í†µê³„ ìš©ì–´ ì‰½ê²Œ ì´í•´í•˜ê¸°
                
                #### ğŸ“Œ p-value (ìœ ì˜í™•ë¥ )
                - **p < 0.05**: "ìš°ì—°ì´ ì•„ë‹ˆë‹¤!" âœ…
                - **p â‰¥ 0.05**: "ìš°ì—°ì¼ ìˆ˜ë„..." âŒ
                - ì‘ì„ìˆ˜ë¡ ì¸ìì˜ ì˜í–¥ì´ í™•ì‹¤í•¨
                
                #### ğŸ“Œ RÂ² (ê²°ì •ê³„ìˆ˜)
                - ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€
                - 0~1 ì‚¬ì´ ê°’ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
                - 0.8 ì´ìƒì´ë©´ ëŒ€ì²´ë¡œ ì–‘í˜¸
                
                #### ğŸ“Œ ì£¼íš¨ê³¼ ê·¸ë˜í”„
                - ê¸°ìš¸ê¸°ê°€ ê¸‰í• ìˆ˜ë¡ ì˜í–¥ì´ í¼
                - í‰í‰í•˜ë©´ ì˜í–¥ì´ ì ìŒ
                
                #### ğŸ“Œ ìƒí˜¸ì‘ìš© ê·¸ë˜í”„
                - ì„ ì´ í‰í–‰: ìƒí˜¸ì‘ìš© ì—†ìŒ
                - ì„ ì´ êµì°¨: ìƒí˜¸ì‘ìš© ìˆìŒ
                """
            }
        }
        
        self.tooltips = {
            'factor': "ê²°ê³¼ì— ì˜í–¥ì„ ì£¼ëŠ” ì‹¤í—˜ ì¡°ê±´",
            'response': "ì¸¡ì •í•˜ë ¤ëŠ” ì‹¤í—˜ ê²°ê³¼",
            'level': "ì¸ìê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’",
            'replicate': "ê°™ì€ ì¡°ê±´ì˜ ë°˜ë³µ ì‹¤í—˜",
            'block': "ì™¸ë¶€ ì˜í–¥ì„ ì¤„ì´ëŠ” ì‹¤í—˜ ê·¸ë£¹",
            'randomization': "ìˆœì„œ íš¨ê³¼ë¥¼ ì—†ì• ëŠ” ë¬´ì‘ìœ„ ë°°ì¹˜",
            'center_point': "ì¤‘ê°„ ì¡°ê±´ì—ì„œì˜ ì¶”ê°€ ì‹¤í—˜",
            'resolution': "êµ¬ë³„ ê°€ëŠ¥í•œ íš¨ê³¼ì˜ ìˆ˜ì¤€",
            'confounding': "íš¨ê³¼ë¥¼ êµ¬ë³„í•  ìˆ˜ ì—†ëŠ” ìƒíƒœ",
            'orthogonal': "ì¸ìë“¤ì´ ë…ë¦½ì ì¸ ì„¤ê³„"
        }
    
    def get_help(self, topic: str, level: str = 'basic') -> str:
        """ë„ì›€ë§ ë‚´ìš© ë°˜í™˜"""
        if topic in self.help_database:
            help_content = self.help_database[topic]
            
            if level == 'basic':
                return f"## {help_content['title']}\n{help_content['basic']}"
            elif level == 'detailed':
                return f"## {help_content['title']}\n{help_content['basic']}\n{help_content['detailed']}"
            elif level == 'examples' and 'examples' in help_content:
                examples = '\n'.join([f"- {ex}" for ex in help_content['examples']])
                return f"### ğŸ“ ì˜ˆì‹œ\n{examples}"
        
        return "ë„ì›€ë§ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def get_tooltip(self, term: str) -> str:
        """íˆ´íŒ ë°˜í™˜"""
        return self.tooltips.get(term, "")
    
    def show_help_button(self, topic: str, key: str = None):
        """ë„ì›€ë§ ë²„íŠ¼ í‘œì‹œ"""
        if st.button("â“ ë„ì›€ë§", key=key):
            st.info(self.get_help(topic, 'basic'))
            
            if st.button("ğŸ“– ë” ìì„¸íˆ", key=f"{key}_more"):
                st.info(self.get_help(topic, 'detailed'))
    
    def show_contextual_help(self, context: str, user_level: UserLevel):
        """ìƒí™©ë³„ ë„ì›€ë§ í‘œì‹œ"""
        # ì´ˆë³´ìëŠ” ìë™ìœ¼ë¡œ ë„ì›€ë§ í‘œì‹œ
        if user_level == UserLevel.BEGINNER:
            with st.expander("ğŸ’¡ ë„ì›€ë§", expanded=True):
                st.markdown(self.get_help(context, 'basic'))
        
        # ì¤‘ê¸‰ìëŠ” ë²„íŠ¼ìœ¼ë¡œ í‘œì‹œ
        elif user_level == UserLevel.INTERMEDIATE:
            self.show_help_button(context)

# ==================== ìºì‹œ ì‹œìŠ¤í…œ ====================
class CacheManager:
    """íš¨ìœ¨ì ì¸ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, cache_dir: str = ".cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        self.memory_cache = {}
        self.cache_stats = defaultdict(lambda: {'hits': 0, 'misses': 0})
    
    def _get_cache_key(self, func_name: str, *args, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{func_name}:{args}:{sorted(kwargs.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if key in self.memory_cache:
            self.cache_stats[key]['hits'] += 1
            return self.memory_cache[key]
        
        # ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    value = pickle.load(f)
                
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                self.memory_cache[key] = value
                self.cache_stats[key]['hits'] += 1
                return value
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        self.cache_stats[key]['misses'] += 1
        return None
    
    def set(self, key: str, value: Any, ttl: int = 3600):
        """ìºì‹œì— ê°’ ì €ì¥"""
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
        self.memory_cache[key] = value
        
        # ë””ìŠ¤í¬ ìºì‹œì— ì €ì¥
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(value, f)
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def invalidate(self, pattern: str = None):
        """ìºì‹œ ë¬´íš¨í™”"""
        if pattern:
            # íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ìºì‹œ ì‚­ì œ
            keys_to_remove = [k for k in self.memory_cache.keys() if pattern in k]
            for key in keys_to_remove:
                del self.memory_cache[key]
                
                cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
                if os.path.exists(cache_file):
                    os.remove(cache_file)
        else:
            # ì „ì²´ ìºì‹œ ì‚­ì œ
            self.memory_cache.clear()
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)
    
    def get_stats(self) -> Dict[str, Dict[str, int]]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        return dict(self.cache_stats)

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
cache_manager = CacheManager()

def cached(ttl: int = 3600):
    """ìºì‹± ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = cache_manager._get_cache_key(func.__name__, *args, **kwargs)
            
            # ìºì‹œ í™•ì¸
            cached_value = cache_manager.get(cache_key)
            if cached_value is not None:
                return cached_value
            
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ìºì‹œ ì €ì¥
            cache_manager.set(cache_key, result, ttl)
            
            return result
        
        return wrapper
    return decorator

# ==================== ì„¤ì • ê´€ë¦¬ ====================
class ConfigManager:
    """ì„¤ì • ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    DEFAULT_CONFIG = {
        'app': {
            'name': 'ê³ ë¶„ì ì‹¤í—˜ ì„¤ê³„ í”Œë«í¼',
            'version': VERSION,
            'theme': 'light',
            'language': 'ko',
            'timezone': 'Asia/Seoul'
        },
        'experiment': {
            'default_confidence_level': 0.95,
            'default_power': 0.8,
            'max_factors': 20,
            'max_runs': 1000,
            'auto_save': True,
            'save_interval': 300  # 5ë¶„
        },
        'analysis': {
            'significance_level': 0.05,
            'outlier_threshold': 3.0,  # í‘œì¤€í¸ì°¨
            'min_r_squared': 0.7,
            'cross_validation_folds': 5
        },
        'visualization': {
            'plot_style': 'seaborn',
            'color_palette': 'viridis',
            'figure_dpi': 150,
            'interactive_plots': True,
            'animation_speed': 1.0
        },
        'ai': {
            'default_model': 'gemini',
            'temperature': 0.7,
            'max_tokens': 2000,
            'timeout': 30,
            'retry_attempts': 3,
            'consensus_threshold': 0.7
        },
        'database': {
            'backup_enabled': True,
            'backup_interval': 86400,  # 24ì‹œê°„
            'max_backups': 7,
            'compression': True
        },
        'notifications': {
            'email_enabled': False,
            'slack_enabled': False,
            'desktop_enabled': True
        }
    }
    
    def __init__(self, config_file: str = "config.json"):
        self.config_file = config_file
        self.config = self.load_config()
    
    def load_config(self) -> Dict[str, Any]:
        """ì„¤ì • ë¡œë“œ"""
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                
                # ê¸°ë³¸ ì„¤ì •ê³¼ ë³‘í•©
                return self._merge_configs(self.DEFAULT_CONFIG, user_config)
            except Exception as e:
                logger.error(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return self.DEFAULT_CONFIG.copy()
    
    def save_config(self):
        """ì„¤ì • ì €ì¥"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            
            logger.info("ì„¤ì • ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get(self, path: str, default: Any = None) -> Any:
        """ì„¤ì • ê°’ ê°€ì ¸ì˜¤ê¸° (ì  í‘œê¸°ë²• ì§€ì›)"""
        keys = path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value
    
    def set(self, path: str, value: Any):
        """ì„¤ì • ê°’ ì„¤ì •"""
        keys = path.split('.')
        config = self.config
        
        for key in keys[:-1]:
            if key not in config:
                config[key] = {}
            config = config[key]
        
        config[keys[-1]] = value
        self.save_config()
    
    def _merge_configs(self, default: Dict, user: Dict) -> Dict:
        """ì„¤ì • ë³‘í•© (ì¬ê·€ì )"""
        result = default.copy()
        
        for key, value in user.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._merge_configs(result[key], value)
            else:
                result[key] = value
        
        return result

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
config_manager = ConfigManager()

# Polymer-doe-platform - Part 3
# ==================== ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ ====================
class EventType(Enum):
    """ì´ë²¤íŠ¸ ìœ í˜•"""
    PROJECT_CREATED = "project_created"
    PROJECT_UPDATED = "project_updated"
    EXPERIMENT_STARTED = "experiment_started"
    EXPERIMENT_COMPLETED = "experiment_completed"
    ANALYSIS_COMPLETED = "analysis_completed"
    ERROR_OCCURRED = "error_occurred"
    USER_ACTION = "user_action"
    SYSTEM_EVENT = "system_event"

@dataclass
class Event:
    """ì´ë²¤íŠ¸ ë°ì´í„°"""
    type: EventType
    timestamp: datetime
    data: Dict[str, Any]
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class EventBus:
    """ì´ë²¤íŠ¸ ë²„ìŠ¤ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.subscribers: Dict[EventType, List[Callable]] = defaultdict(list)
        self.event_queue: queue.Queue = queue.Queue()
        self.event_history: deque = deque(maxlen=1000)
        self.running = False
        self.worker_thread = None
    
    def subscribe(self, event_type: EventType, callback: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        self.subscribers[event_type].append(callback)
        logger.debug(f"êµ¬ë… ì¶”ê°€: {event_type.value} -> {callback.__name__}")
    
    def unsubscribe(self, event_type: EventType, callback: Callable):
        """êµ¬ë… í•´ì œ"""
        if callback in self.subscribers[event_type]:
            self.subscribers[event_type].remove(callback)
    
    def publish(self, event: Event):
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        self.event_queue.put(event)
        self.event_history.append(event)
        
        # ì¦‰ì‹œ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°
        if event.type == EventType.ERROR_OCCURRED:
            self._process_event(event)
    
    def start(self):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œì‘"""
        if not self.running:
            self.running = True
            self.worker_thread = threading.Thread(target=self._worker, daemon=True)
            self.worker_thread.start()
            logger.info("ì´ë²¤íŠ¸ ë²„ìŠ¤ ì‹œì‘")
    
    def stop(self):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ì§€"""
        self.running = False
        if self.worker_thread:
            self.worker_thread.join()
        logger.info("ì´ë²¤íŠ¸ ë²„ìŠ¤ ì¤‘ì§€")
    
    def _worker(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤"""
        while self.running:
            try:
                event = self.event_queue.get(timeout=1)
                self._process_event(event)
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"ì´ë²¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def _process_event(self, event: Event):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        for callback in self.subscribers[event.type]:
            try:
                callback(event)
            except Exception as e:
                logger.error(f"ì´ë²¤íŠ¸ ì½œë°± ì˜¤ë¥˜: {callback.__name__} - {e}")
    
    def get_history(self, event_type: EventType = None, limit: int = 100) -> List[Event]:
        """ì´ë²¤íŠ¸ ê¸°ë¡ ì¡°íšŒ"""
        history = list(self.event_history)
        
        if event_type:
            history = [e for e in history if e.type == event_type]
        
        return history[-limit:]

# ì „ì—­ ì´ë²¤íŠ¸ ë²„ìŠ¤
event_bus = EventBus()

# ==================== ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ====================
class DatabaseManager:
    """í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 db_type: str = "sqlite",
                 db_path: str = "polymer_doe.db",
                 backup_enabled: bool = True):
        self.db_type = db_type
        self.db_path = db_path
        self.backup_enabled = backup_enabled
        self.connection_pool = []
        self.lock = threading.Lock()
        
        self._init_database()
        
        # ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬
        if backup_enabled:
            self._schedule_backups()
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        if self.db_type == "sqlite":
            self._init_sqlite()
        elif self.db_type == "mongodb" and MONGODB_AVAILABLE:
            self._init_mongodb()
        else:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DB íƒ€ì…: {self.db_type}")
            self.db_type = "sqlite"
            self._init_sqlite()
    
    def _init_sqlite(self):
        """SQLite ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # í”„ë¡œì íŠ¸ í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS projects (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    description TEXT,
                    polymer_type TEXT,
                    polymer_system TEXT,
                    objectives TEXT,
                    constraints TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    owner TEXT,
                    collaborators TEXT,
                    tags TEXT,
                    version INTEGER DEFAULT 1,
                    parent_project_id TEXT,
                    status TEXT DEFAULT 'active',
                    budget REAL,
                    deadline TIMESTAMP,
                    notes TEXT,
                    attachments TEXT,
                    custom_fields TEXT,
                    data BLOB
                )
            """)
            
            # ì‹¤í—˜ ë°ì´í„° í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS experiments (
                    id TEXT PRIMARY KEY,
                    project_id TEXT,
                    design_matrix TEXT,
                    results TEXT,
                    metadata TEXT,
                    created_at TIMESTAMP,
                    status TEXT,
                    run_order TEXT,
                    block_structure TEXT,
                    conditions TEXT,
                    operator TEXT,
                    equipment TEXT,
                    notes TEXT,
                    data BLOB,
                    FOREIGN KEY (project_id) REFERENCES projects (id)
                )
            """)
            
            # ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS analysis_results (
                    id TEXT PRIMARY KEY,
                    experiment_id TEXT,
                    analysis_type TEXT,
                    results TEXT,
                    plots TEXT,
                    statistics TEXT,
                    created_at TIMESTAMP,
                    parameters TEXT,
                    quality_metrics TEXT,
                    FOREIGN KEY (experiment_id) REFERENCES experiments (id)
                )
            """)
            
            # ì‚¬ìš©ì í™œë™ ë¡œê·¸
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS activity_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT,
                    action TEXT,
                    details TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    session_id TEXT,
                    ip_address TEXT,
                    user_agent TEXT
                )
            """)
            
            # í•™ìŠµ ë°ì´í„° í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS learning_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    user_id TEXT,
                    user_level TEXT,
                    action_type TEXT,
                    action_details TEXT,
                    outcome TEXT,
                    quality_score REAL,
                    context TEXT,
                    feedback TEXT
                )
            """)
            
            # AI ìƒí˜¸ì‘ìš© ë¡œê·¸
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS ai_interactions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    user_id TEXT,
                    prompt TEXT,
                    response TEXT,
                    model TEXT,
                    tokens_used INTEGER,
                    response_time REAL,
                    quality_rating REAL,
                    feedback TEXT
                )
            """)
            
            # í˜‘ì—… ë°ì´í„°
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS collaborations (
                    id TEXT PRIMARY KEY,
                    project_id TEXT,
                    type TEXT,  -- comment, review, suggestion
                    user_id TEXT,
                    content TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    parent_id TEXT,  -- for threaded discussions
                    status TEXT,
                    metadata TEXT,
                    FOREIGN KEY (project_id) REFERENCES projects (id)
                )
            """)
            
            # í…œí”Œë¦¿ ì €ì¥ì†Œ
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS templates (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    description TEXT,
                    type TEXT,  -- project, experiment, analysis
                    content TEXT,
                    created_by TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    usage_count INTEGER DEFAULT 0,
                    rating REAL,
                    tags TEXT,
                    public BOOLEAN DEFAULT FALSE
                )
            """)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_projects_owner ON projects(owner)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_experiments_project ON experiments(project_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_activity_user ON activity_log(user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_learning_user ON learning_data(user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_ai_user ON ai_interactions(user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_collaborations_project ON collaborations(project_id)")
            
            conn.commit()
            logger.info("SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    @contextmanager
    def get_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        conn = None
        try:
            if self.connection_pool:
                conn = self.connection_pool.pop()
            else:
                if self.db_type == "sqlite":
                    conn = sqlite3.connect(self.db_path)
                    conn.row_factory = sqlite3.Row
            
            yield conn
            
        finally:
            if conn:
                if len(self.connection_pool) < 5:
                    self.connection_pool.append(conn)
                else:
                    conn.close()
    
    def execute_query(self, query: str, params: tuple = None) -> List[Dict]:
        """ì¿¼ë¦¬ ì‹¤í–‰"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            if query.strip().upper().startswith('SELECT'):
                columns = [desc[0] for desc in cursor.description]
                return [dict(zip(columns, row)) for row in cursor.fetchall()]
            else:
                conn.commit()
                return []
    
    def save_project(self, project: ProjectInfo) -> bool:
        """í”„ë¡œì íŠ¸ ì €ì¥"""
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                # JSON ì§ë ¬í™”
                polymer_system = json.dumps(project.polymer_system)
                objectives = json.dumps(project.objectives)
                constraints = json.dumps(project.constraints)
                collaborators = json.dumps(project.collaborators)
                tags = json.dumps(project.tags)
                attachments = json.dumps(project.attachments)
                custom_fields = json.dumps(project.custom_fields)
                
                # ì „ì²´ ê°ì²´ ì§ë ¬í™” (ë°±ì—…ìš©)
                data = pickle.dumps(project)
                
                cursor.execute("""
                    INSERT OR REPLACE INTO projects 
                    (id, name, description, polymer_type, polymer_system, 
                     objectives, constraints, created_at, updated_at, 
                     owner, collaborators, tags, version, parent_project_id,
                     status, budget, deadline, notes, attachments, 
                     custom_fields, data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    project.id, project.name, project.description,
                    project.polymer_type, polymer_system, objectives,
                    constraints, project.created_at, project.updated_at,
                    project.owner, collaborators, tags, project.version,
                    project.parent_project_id, project.status, project.budget,
                    project.deadline, project.notes, attachments,
                    custom_fields, data
                ))
                
                conn.commit()
                
                # ì´ë²¤íŠ¸ ë°œí–‰
                event_bus.publish(Event(
                    type=EventType.PROJECT_CREATED,
                    timestamp=datetime.now(),
                    data={'project_id': project.id, 'name': project.name},
                    user_id=project.owner
                ))
                
                return True
                
        except Exception as e:
            logger.error(f"í”„ë¡œì íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_project(self, project_id: str) -> Optional[ProjectInfo]:
        """í”„ë¡œì íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            results = self.execute_query(
                "SELECT data FROM projects WHERE id = ?",
                (project_id,)
            )
            
            if results:
                return pickle.loads(results[0]['data'])
            
            return None
            
        except Exception as e:
            logger.error(f"í”„ë¡œì íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def search_projects(self, 
                       owner: str = None,
                       tags: List[str] = None,
                       polymer_type: str = None,
                       status: str = None,
                       limit: int = 100) -> List[ProjectInfo]:
        """í”„ë¡œì íŠ¸ ê²€ìƒ‰"""
        query = "SELECT data FROM projects WHERE 1=1"
        params = []
        
        if owner:
            query += " AND owner = ?"
            params.append(owner)
        
        if polymer_type:
            query += " AND polymer_type = ?"
            params.append(polymer_type)
        
        if status:
            query += " AND status = ?"
            params.append(status)
        
        if tags:
            # íƒœê·¸ëŠ” JSON ë°°ì—´ë¡œ ì €ì¥ë˜ì–´ ìˆìŒ
            for tag in tags:
                query += " AND tags LIKE ?"
                params.append(f'%"{tag}"%')
        
        query += " ORDER BY updated_at DESC LIMIT ?"
        params.append(limit)
        
        results = self.execute_query(query, tuple(params))
        
        projects = []
        for row in results:
            try:
                project = pickle.loads(row['data'])
                projects.append(project)
            except:
                continue
        
        return projects
    
    def save_experiment(self, experiment: ExperimentData) -> bool:
        """ì‹¤í—˜ ë°ì´í„° ì €ì¥"""
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                # ì§ë ¬í™”
                design_matrix = experiment.design_matrix.to_json()
                results = experiment.results.to_json() if experiment.results is not None else None
                metadata = json.dumps(experiment.metadata)
                run_order = json.dumps(experiment.run_order)
                block_structure = json.dumps(experiment.block_structure)
                conditions = json.dumps(experiment.conditions)
                notes = json.dumps(experiment.notes)
                data = pickle.dumps(experiment)
                
                cursor.execute("""
                    INSERT OR REPLACE INTO experiments 
                    (id, project_id, design_matrix, results, metadata,
                     created_at, status, run_order, block_structure,
                     conditions, operator, equipment, notes, data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    experiment.id, experiment.project_id, design_matrix,
                    results, metadata, experiment.created_at, 
                    experiment.status.value, run_order, block_structure,
                    conditions, experiment.operator, experiment.equipment,
                    notes, data
                ))
                
                conn.commit()
                
                # ì´ë²¤íŠ¸ ë°œí–‰
                event_bus.publish(Event(
                    type=EventType.EXPERIMENT_STARTED,
                    timestamp=datetime.now(),
                    data={'experiment_id': experiment.id, 'project_id': experiment.project_id}
                ))
                
                return True
                
        except Exception as e:
            logger.error(f"ì‹¤í—˜ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def save_analysis_result(self, 
                           experiment_id: str,
                           analysis_type: str,
                           results: Dict[str, Any],
                           plots: List[str] = None,
                           parameters: Dict[str, Any] = None) -> bool:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            analysis_id = generate_unique_id("ANALYSIS")
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            quality_metrics = self._calculate_quality_metrics(results)
            
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute("""
                    INSERT INTO analysis_results
                    (id, experiment_id, analysis_type, results, plots,
                     statistics, created_at, parameters, quality_metrics)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    analysis_id, experiment_id, analysis_type,
                    json.dumps(results), json.dumps(plots or []),
                    json.dumps(results.get('statistics', {})),
                    datetime.now(), json.dumps(parameters or {}),
                    json.dumps(quality_metrics)
                ))
                
                conn.commit()
                
                # ì´ë²¤íŠ¸ ë°œí–‰
                event_bus.publish(Event(
                    type=EventType.ANALYSIS_COMPLETED,
                    timestamp=datetime.now(),
                    data={
                        'analysis_id': analysis_id,
                        'experiment_id': experiment_id,
                        'type': analysis_type
                    }
                ))
                
                return True
                
        except Exception as e:
            logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def log_activity(self, 
                    user_id: str,
                    action: str,
                    details: Dict[str, Any],
                    session_id: str = None):
        """í™œë™ ë¡œê·¸ ê¸°ë¡"""
        try:
            self.execute_query("""
                INSERT INTO activity_log 
                (user_id, action, details, session_id)
                VALUES (?, ?, ?, ?)
            """, (user_id, action, json.dumps(details), session_id))
            
        except Exception as e:
            logger.error(f"í™œë™ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    def save_learning_record(self, record: LearningRecord):
        """í•™ìŠµ ê¸°ë¡ ì €ì¥"""
        try:
            self.execute_query("""
                INSERT INTO learning_data
                (timestamp, user_id, user_level, action_type,
                 action_details, outcome, quality_score, context, feedback)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                record.timestamp, record.user_id, record.user_level.name,
                record.action_type, json.dumps(record.action_details),
                json.dumps(record.outcome), record.quality_score,
                json.dumps(record.context), record.feedback
            ))
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_learning_recommendations(self, 
                                   user_id: str,
                                   context: str,
                                   limit: int = 5) -> List[Dict[str, Any]]:
        """í•™ìŠµ ê¸°ë°˜ ì¶”ì²œ"""
        # ìœ ì‚¬í•œ ì»¨í…ìŠ¤íŠ¸ì˜ ì„±ê³µ ì‚¬ë¡€ ê²€ìƒ‰
        query = """
            SELECT action_details, outcome, quality_score
            FROM learning_data
            WHERE user_id = ? AND action_type = ? AND quality_score > 0.7
            ORDER BY quality_score DESC, timestamp DESC
            LIMIT ?
        """
        
        results = self.execute_query(query, (user_id, context, limit * 2))
        
        recommendations = []
        for row in results[:limit]:
            try:
                recommendations.append({
                    'action': json.loads(row['action_details']),
                    'outcome': json.loads(row['outcome']),
                    'score': row['quality_score']
                })
            except:
                continue
        
        return recommendations
    
    def save_template(self, 
                     name: str,
                     template_type: str,
                     content: Dict[str, Any],
                     user_id: str,
                     description: str = "",
                     tags: List[str] = None,
                     public: bool = False) -> str:
        """í…œí”Œë¦¿ ì €ì¥"""
        template_id = generate_unique_id("TEMPLATE")
        
        try:
            self.execute_query("""
                INSERT INTO templates
                (id, name, description, type, content, created_by,
                 created_at, updated_at, tags, public)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                template_id, name, description, template_type,
                json.dumps(content), user_id, datetime.now(),
                datetime.now(), json.dumps(tags or []), public
            ))
            
            return template_id
            
        except Exception as e:
            logger.error(f"í…œí”Œë¦¿ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def load_template(self, template_id: str) -> Optional[Dict[str, Any]]:
        """í…œí”Œë¦¿ ë¶ˆëŸ¬ì˜¤ê¸°"""
        results = self.execute_query(
            "SELECT * FROM templates WHERE id = ?",
            (template_id,)
        )
        
        if results:
            template = results[0]
            template['content'] = json.loads(template['content'])
            template['tags'] = json.loads(template['tags'])
            
            # ì‚¬ìš© íšŸìˆ˜ ì¦ê°€
            self.execute_query(
                "UPDATE templates SET usage_count = usage_count + 1 WHERE id = ?",
                (template_id,)
            )
            
            return template
        
        return None
    
    def search_templates(self,
                        template_type: str = None,
                        tags: List[str] = None,
                        public_only: bool = True,
                        limit: int = 50) -> List[Dict[str, Any]]:
        """í…œí”Œë¦¿ ê²€ìƒ‰"""
        query = "SELECT * FROM templates WHERE 1=1"
        params = []
        
        if template_type:
            query += " AND type = ?"
            params.append(template_type)
        
        if public_only:
            query += " AND public = 1"
        
        if tags:
            for tag in tags:
                query += " AND tags LIKE ?"
                params.append(f'%"{tag}"%')
        
        query += " ORDER BY usage_count DESC, rating DESC LIMIT ?"
        params.append(limit)
        
        results = self.execute_query(query, tuple(params))
        
        templates = []
        for row in results:
            template = dict(row)
            template['content'] = json.loads(template['content'])
            template['tags'] = json.loads(template['tags'])
            templates.append(template)
        
        return templates
    
    def _calculate_quality_metrics(self, results: Dict[str, Any]) -> Dict[str, float]:
        """ë¶„ì„ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        
        # RÂ² ê°’
        if 'r_squared' in results:
            metrics['r_squared'] = results['r_squared']
        
        # p-value ê¸°ë°˜ ì‹ ë¢°ë„
        if 'p_values' in results:
            p_values = results['p_values']
            if isinstance(p_values, dict):
                significant_count = sum(1 for p in p_values.values() if p < 0.05)
                metrics['significance_ratio'] = significant_count / len(p_values) if p_values else 0
        
        # ì”ì°¨ ë¶„ì„
        if 'residuals' in results:
            residuals = results['residuals']
            if isinstance(residuals, dict):
                metrics['residual_normality'] = residuals.get('normality_p_value', 0)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        quality_score = 0
        weights = {'r_squared': 0.4, 'significance_ratio': 0.3, 'residual_normality': 0.3}
        
        for metric, weight in weights.items():
            if metric in metrics:
                quality_score += metrics[metric] * weight
        
        metrics['overall_quality'] = quality_score
        
        return metrics
    
    def backup_database(self) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = "backups"
        os.makedirs(backup_dir, exist_ok=True)
        
        if self.db_type == "sqlite":
            backup_file = os.path.join(backup_dir, f"polymer_doe_{timestamp}.db")
            
            try:
                import shutil
                shutil.copy2(self.db_path, backup_file)
                
                # ì••ì¶•
                if config_manager.get('database.compression'):
                    import gzip
                    with open(backup_file, 'rb') as f_in:
                        with gzip.open(f"{backup_file}.gz", 'wb') as f_out:
                            f_out.writelines(f_in)
                    
                    os.remove(backup_file)
                    backup_file = f"{backup_file}.gz"
                
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ: {backup_file}")
                
                # ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ
                self._cleanup_old_backups(backup_dir)
                
                return backup_file
                
            except Exception as e:
                logger.error(f"ë°±ì—… ì‹¤íŒ¨: {e}")
                return None
    
    def _cleanup_old_backups(self, backup_dir: str):
        """ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ"""
        max_backups = config_manager.get('database.max_backups', 7)
        
        backups = sorted([
            f for f in os.listdir(backup_dir) 
            if f.startswith('polymer_doe_') and (f.endswith('.db') or f.endswith('.db.gz'))
        ])
        
        if len(backups) > max_backups:
            for old_backup in backups[:-max_backups]:
                try:
                    os.remove(os.path.join(backup_dir, old_backup))
                    logger.info(f"ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {old_backup}")
                except:
                    pass
    
    def _schedule_backups(self):
        """ë°±ì—… ìŠ¤ì¼€ì¤„ë§"""
        def backup_task():
            while self.backup_enabled:
                interval = config_manager.get('database.backup_interval', 86400)
                time.sleep(interval)
                self.backup_database()
        
        backup_thread = threading.Thread(target=backup_task, daemon=True)
        backup_thread.start()

# ì „ì—­ ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
db_manager = DatabaseManager()

# ==================== í˜‘ì—… ì‹œìŠ¤í…œ ====================
class CollaborationType(Enum):
    """í˜‘ì—… ìœ í˜•"""
    COMMENT = "comment"
    REVIEW = "review"
    SUGGESTION = "suggestion"
    APPROVAL = "approval"
    QUESTION = "question"

@dataclass
class Collaboration:
    """í˜‘ì—… ë°ì´í„°"""
    id: str
    project_id: str
    type: CollaborationType
    user_id: str
    content: str
    created_at: datetime
    updated_at: datetime
    parent_id: Optional[str] = None
    status: str = "active"
    metadata: Dict[str, Any] = field(default_factory=dict)
    reactions: Dict[str, List[str]] = field(default_factory=dict)  # emoji -> user_ids

class CollaborationManager:
    """í˜‘ì—… ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db = db_manager
        self.active_sessions: Dict[str, List[str]] = defaultdict(list)  # project_id -> user_ids
    
    def add_collaboration(self, 
                         project_id: str,
                         collab_type: CollaborationType,
                         user_id: str,
                         content: str,
                         parent_id: str = None) -> str:
        """í˜‘ì—… í•­ëª© ì¶”ê°€"""
        collab_id = generate_unique_id("COLLAB")
        
        collaboration = Collaboration(
            id=collab_id,
            project_id=project_id,
            type=collab_type,
            user_id=user_id,
            content=content,
            created_at=datetime.now(),
            updated_at=datetime.now(),
            parent_id=parent_id
        )
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self.db.execute_query("""
            INSERT INTO collaborations
            (id, project_id, type, user_id, content, created_at,
             updated_at, parent_id, status, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            collab_id, project_id, collab_type.value, user_id,
            content, collaboration.created_at, collaboration.updated_at,
            parent_id, collaboration.status, json.dumps(collaboration.metadata)
        ))
        
        # ì‹¤ì‹œê°„ ì•Œë¦¼ (í™œì„± ì‚¬ìš©ìì—ê²Œ)
        self._notify_collaborators(project_id, collaboration)
        
        return collab_id
    
    def get_collaborations(self, 
                          project_id: str,
                          collab_type: CollaborationType = None,
                          parent_id: str = None) -> List[Collaboration]:
        """í˜‘ì—… í•­ëª© ì¡°íšŒ"""
        query = "SELECT * FROM collaborations WHERE project_id = ?"
        params = [project_id]
        
        if collab_type:
            query += " AND type = ?"
            params.append(collab_type.value)
        
        if parent_id is not None:
            query += " AND parent_id = ?"
            params.append(parent_id)
        elif parent_id is None:
            query += " AND parent_id IS NULL"
        
        query += " ORDER BY created_at DESC"
        
        results = self.db.execute_query(query, tuple(params))
        
        collaborations = []
        for row in results:
            collab = Collaboration(
                id=row['id'],
                project_id=row['project_id'],
                type=CollaborationType(row['type']),
                user_id=row['user_id'],
                content=row['content'],
                created_at=row['created_at'],
                updated_at=row['updated_at'],
                parent_id=row['parent_id'],
                status=row['status'],
                metadata=json.loads(row['metadata']) if row['metadata'] else {}
            )
            collaborations.append(collab)
        
        return collaborations
    
    def update_collaboration(self, 
                           collab_id: str,
                           content: str = None,
                           status: str = None) -> bool:
        """í˜‘ì—… í•­ëª© ìˆ˜ì •"""
        updates = []
        params = []
        
        if content is not None:
            updates.append("content = ?")
            params.append(content)
        
        if status is not None:
            updates.append("status = ?")
            params.append(status)
        
        if not updates:
            return False
        
        updates.append("updated_at = ?")
        params.append(datetime.now())
        
        params.append(collab_id)
        
        query = f"UPDATE collaborations SET {', '.join(updates)} WHERE id = ?"
        
        try:
            self.db.execute_query(query, tuple(params))
            return True
        except:
            return False
    
    def add_reaction(self, collab_id: str, user_id: str, emoji: str):
        """ë°˜ì‘ ì¶”ê°€"""
        # í˜„ì¬ ë°˜ì‘ ê°€ì ¸ì˜¤ê¸°
        results = self.db.execute_query(
            "SELECT metadata FROM collaborations WHERE id = ?",
            (collab_id,)
        )
        
        if results:
            metadata = json.loads(results[0]['metadata']) if results[0]['metadata'] else {}
            reactions = metadata.get('reactions', {})
            
            if emoji not in reactions:
                reactions[emoji] = []
            
            if user_id not in reactions[emoji]:
                reactions[emoji].append(user_id)
            
            metadata['reactions'] = reactions
            
            # ì—…ë°ì´íŠ¸
            self.db.execute_query(
                "UPDATE collaborations SET metadata = ? WHERE id = ?",
                (json.dumps(metadata), collab_id)
            )
    
    def join_session(self, project_id: str, user_id: str):
        """í˜‘ì—… ì„¸ì…˜ ì°¸ê°€"""
        if user_id not in self.active_sessions[project_id]:
            self.active_sessions[project_id].append(user_id)
            logger.info(f"ì‚¬ìš©ì {user_id}ê°€ í”„ë¡œì íŠ¸ {project_id} ì„¸ì…˜ì— ì°¸ê°€")
    
    def leave_session(self, project_id: str, user_id: str):
        """í˜‘ì—… ì„¸ì…˜ ë‚˜ê°€ê¸°"""
        if user_id in self.active_sessions[project_id]:
            self.active_sessions[project_id].remove(user_id)
            logger.info(f"ì‚¬ìš©ì {user_id}ê°€ í”„ë¡œì íŠ¸ {project_id} ì„¸ì…˜ì—ì„œ ë‚˜ê°")
    
    def get_active_users(self, project_id: str) -> List[str]:
        """í™œì„± ì‚¬ìš©ì ëª©ë¡"""
        return self.active_sessions.get(project_id, [])
    
    def _notify_collaborators(self, project_id: str, collaboration: Collaboration):
        """í˜‘ì—…ìì—ê²Œ ì•Œë¦¼"""
        active_users = self.get_active_users(project_id)
        
        for user_id in active_users:
            if user_id != collaboration.user_id:
                # ì‹¤ì‹œê°„ ì•Œë¦¼ ì „ì†¡ (WebSocket ë“± ì‚¬ìš© ì‹œ)
                logger.info(f"ì•Œë¦¼: {user_id}ì—ê²Œ ìƒˆ {collaboration.type.value} ì•Œë¦¼")

# ==================== API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ (í™•ì¥) ====================
class APIKeyManager:
    """API í‚¤ë¥¼ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if 'api_keys' not in st.session_state:
            st.session_state.api_keys = {}
        if 'api_keys_initialized' not in st.session_state:
            st.session_state.api_keys_initialized = False
        
        # API ì„¤ì • ì •ì˜ (í™•ì¥)
        self.api_configs = {
            # AI APIs
            'openai': {
                'name': 'OpenAI',
                'env_key': 'OPENAI_API_KEY',
                'required': False,
                'test_endpoint': 'https://api.openai.com/v1/models',
                'category': 'ai',
                'description': 'GPT ëª¨ë¸ì„ ì‚¬ìš©í•œ ê³ ê¸‰ ì–¸ì–´ ì²˜ë¦¬',
                'features': ['í…ìŠ¤íŠ¸ ìƒì„±', 'ì½”ë“œ ìƒì„±', 'ë¶„ì„', 'ë²ˆì—­'],
                'rate_limit': {'rpm': 3500, 'tpm': 90000},
                'models': ['gpt-4', 'gpt-3.5-turbo', 'text-embedding-ada-002']
            },
            'gemini': {
                'name': 'Google Gemini',
                'env_key': 'GOOGLE_API_KEY',
                'required': False,
                'test_endpoint': 'https://generativelanguage.googleapis.com/v1beta/models',
                'category': 'ai',
                'description': 'Googleì˜ ìµœì‹  AI ëª¨ë¸',
                'features': ['ë‹¤ì¤‘ ëª¨ë‹¬', 'ê¸´ ì»¨í…ìŠ¤íŠ¸', 'ì¶”ë¡ ', 'ì°½ì˜ì„±'],
                'rate_limit': {'rpm': 60, 'rpd': 1500},
                'models': ['gemini-pro', 'gemini-pro-vision']
            },
            'anthropic': {
                'name': 'Anthropic Claude',
                'env_key': 'ANTHROPIC_API_KEY',
                'required': False,
                'test_endpoint': 'https://api.anthropic.com/v1/messages',
                'category': 'ai',
                'description': 'Claude AI ëª¨ë¸',
                'features': ['ê¸´ ì»¨í…ìŠ¤íŠ¸', 'ì•ˆì „ì„±', 'ì¶”ë¡ ', 'ì½”ë”©'],
                'rate_limit': {'rpm': 50, 'tpm': 100000},
                'models': ['claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku']
            },
            # ... ê¸°ì¡´ APIë“¤ ...
            
            # Database APIs (í™•ì¥)
            'materials_project': {
                'name': 'Materials Project',
                'env_key': 'MP_API_KEY',
                'required': False,
                'test_endpoint': 'https://api.materialsproject.org',
                'category': 'database',
                'description': 'ì¬ë£Œ ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤',
                'features': ['ì¬ë£Œ íŠ¹ì„±', 'ê³„ì‚° ë°ì´í„°', 'êµ¬ì¡° ì •ë³´'],
                'rate_limit': {'rpd': 1000}
            },
            'polymer_database': {
                'name': 'PoLyInfo',
                'env_key': 'POLYINFO_API_KEY',
                'required': False,
                'test_endpoint': 'https://polymer.nims.go.jp/api',
                'category': 'database',
                'description': 'ê³ ë¶„ì ë¬¼ì„± ë°ì´í„°ë² ì´ìŠ¤',
                'features': ['ê³ ë¶„ì ë¬¼ì„±', 'í™”í•™ êµ¬ì¡°', 'ê°€ê³µ ì¡°ê±´'],
                'rate_limit': {'rpd': 500}
            },
            'chemspider': {
                'name': 'ChemSpider',
                'env_key': 'CHEMSPIDER_API_KEY',
                'required': False,
                'test_endpoint': 'https://api.rsc.org/compounds/v1',
                'category': 'database',
                'description': 'í™”í•™ êµ¬ì¡° ë°ì´í„°ë² ì´ìŠ¤',
                'features': ['í™”í•™ êµ¬ì¡°', 'ë¬¼ì„± ì˜ˆì¸¡', 'InChI/SMILES'],
                'rate_limit': {'rpm': 15}
            }
        }
        
        self.rate_limiters = {}
        self.initialize_keys()
    
    def initialize_keys(self):
        """API í‚¤ ì´ˆê¸°í™”"""
        if not st.session_state.api_keys_initialized:
            # 1. Streamlit secretsì—ì„œ ë¡œë“œ
            self._load_from_secrets()
            
            # 2. í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ
            self._load_from_env()
            
            # 3. ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ (ê°œë°œìš©)
            self._load_from_file()
            
            # 4. ì‚¬ìš©ì ì…ë ¥ í‚¤ ë¡œë“œ
            self._load_user_keys()
            
            # 5. Rate limiter ì´ˆê¸°í™”
            self._init_rate_limiters()
            
            st.session_state.api_keys_initialized = True
            logger.info("API í‚¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_rate_limiters(self):
        """Rate limiter ì´ˆê¸°í™”"""
        for api_id, config in self.api_configs.items():
            if 'rate_limit' in config:
                self.rate_limiters[api_id] = RateLimiter(
                    api_id,
                    config['rate_limit']
                )
    
    def _load_user_keys(self):
        """ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ ë¡œë“œ"""
        if 'user_api_keys' in st.session_state:
            for key_id, value in st.session_state.user_api_keys.items():
                if value and key_id not in st.session_state.api_keys:
                    st.session_state.api_keys[key_id] = value
    
    def validate_and_set_key(self, key_id: str, key: str) -> Tuple[bool, str]:
        """API í‚¤ ê²€ì¦ ë° ì„¤ì •"""
        # í˜•ì‹ ê²€ì¦
        if not self.validate_key_format(key_id, key):
            return False, "API í‚¤ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
        
        # ì‹¤ì œ ì—°ê²° í…ŒìŠ¤íŠ¸
        test_result = self.test_api_connection(key_id, key)
        
        if test_result['status'] == 'success':
            self.set_key(key_id, key)
            return True, test_result['message']
        else:
            return False, test_result['message']
    
    @retry(max_attempts=3, delay=1.0)
    async def call_api_with_limit(self, api_id: str, api_call: Callable, *args, **kwargs):
        """Rate limitingì´ ì ìš©ëœ API í˜¸ì¶œ"""
        if api_id in self.rate_limiters:
            await self.rate_limiters[api_id].acquire()
        
        try:
            return await api_call(*args, **kwargs)
        except Exception as e:
            logger.error(f"API í˜¸ì¶œ ì‹¤íŒ¨ ({api_id}): {e}")
            raise

# Polymer-doe-platform - Part 4
# ==================== Rate Limiter ====================
class RateLimiter:
    """API í˜¸ì¶œ ì†ë„ ì œí•œê¸°"""
    
    def __init__(self, api_id: str, limits: Dict[str, int]):
        self.api_id = api_id
        self.limits = limits  # {'rpm': 60, 'rpd': 1500, 'tpm': 10000}
        self.calls = defaultdict(lambda: deque(maxlen=10000))
        self.lock = threading.Lock()
    
    async def acquire(self):
        """í˜¸ì¶œ ê¶Œí•œ íšë“"""
        while not self._can_make_request():
            await asyncio.sleep(0.1)
        
        self._record_request()
    
    def _can_make_request(self) -> bool:
        """ìš”ì²­ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        now = datetime.now()
        
        with self.lock:
            # ë¶„ë‹¹ ì œí•œ (rpm)
            if 'rpm' in self.limits:
                minute_ago = now - timedelta(minutes=1)
                recent_calls = [t for t in self.calls['minute'] if t > minute_ago]
                if len(recent_calls) >= self.limits['rpm']:
                    return False
            
            # ì¼ì¼ ì œí•œ (rpd)
            if 'rpd' in self.limits:
                day_ago = now - timedelta(days=1)
                recent_calls = [t for t in self.calls['day'] if t > day_ago]
                if len(recent_calls) >= self.limits['rpd']:
                    return False
            
            # í† í° ì œí•œ (tpm)
            if 'tpm' in self.limits:
                # í† í° ìˆ˜ëŠ” ë³„ë„ë¡œ ì¶”ì  í•„ìš”
                pass
        
        return True
    
    def _record_request(self):
        """ìš”ì²­ ê¸°ë¡"""
        now = datetime.now()
        
        with self.lock:
            self.calls['minute'].append(now)
            self.calls['day'].append(now)

# ==================== API ëª¨ë‹ˆí„° (í™•ì¥) ====================
class APIMonitor:
    """API ìƒíƒœ ëª¨ë‹ˆí„°ë§ (í™•ì¥íŒ)"""
    
    def __init__(self):
        if 'api_status' not in st.session_state:
            st.session_state.api_status = {}
        if 'api_metrics' not in st.session_state:
            st.session_state.api_metrics = defaultdict(lambda: {
                'total_calls': 0,
                'successful_calls': 0,
                'failed_calls': 0,
                'total_response_time': 0,
                'total_tokens': 0,
                'total_cost': 0.0,
                'last_call': None,
                'errors': [],
                'hourly_calls': defaultdict(int),
                'daily_success_rate': [],
                'response_times': deque(maxlen=100),
                'token_usage': deque(maxlen=100)
            })
        
        self.cost_per_token = {
            'openai': {'input': 0.00003, 'output': 0.00006},  # GPT-4 ê¸°ì¤€
            'gemini': {'input': 0.00001, 'output': 0.00002},
            'anthropic': {'input': 0.00003, 'output': 0.00015}
        }
    
    def update_status(self, api_name: str, status: APIStatus, 
                     response_time: float = 0, error_msg: str = None,
                     tokens: Dict[str, int] = None):
        """API ìƒíƒœ ì—…ë°ì´íŠ¸ (í™•ì¥)"""
        st.session_state.api_status[api_name] = {
            'status': status,
            'last_update': datetime.now(),
            'response_time': response_time,
            'error': error_msg
        }
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        metrics = st.session_state.api_metrics[api_name]
        metrics['total_calls'] += 1
        metrics['last_call'] = datetime.now()
        
        # ì‹œê°„ë³„ í˜¸ì¶œ ê¸°ë¡
        current_hour = datetime.now().strftime("%Y-%m-%d %H:00")
        metrics['hourly_calls'][current_hour] += 1
        
        if status == APIStatus.ONLINE:
            metrics['successful_calls'] += 1
            metrics['total_response_time'] += response_time
            metrics['response_times'].append(response_time)
            
            # í† í° ë° ë¹„ìš© ê³„ì‚°
            if tokens:
                total_tokens = tokens.get('input', 0) + tokens.get('output', 0)
                metrics['total_tokens'] += total_tokens
                metrics['token_usage'].append(total_tokens)
                
                # ë¹„ìš© ê³„ì‚°
                if api_name in self.cost_per_token:
                    cost = (tokens.get('input', 0) * self.cost_per_token[api_name]['input'] +
                           tokens.get('output', 0) * self.cost_per_token[api_name]['output'])
                    metrics['total_cost'] += cost
        else:
            metrics['failed_calls'] += 1
            if error_msg:
                metrics['errors'].append({
                    'time': datetime.now(),
                    'error': error_msg
                })
                metrics['errors'] = metrics['errors'][-10:]
        
        # ì¼ë³„ ì„±ê³µë¥  ì—…ë°ì´íŠ¸
        self._update_daily_success_rate(api_name)
    
    def _update_daily_success_rate(self, api_name: str):
        """ì¼ë³„ ì„±ê³µë¥  ì—…ë°ì´íŠ¸"""
        metrics = st.session_state.api_metrics[api_name]
        today = datetime.now().date()
        success_rate = self.get_success_rate(api_name)
        
        if metrics['daily_success_rate']:
            last_entry = metrics['daily_success_rate'][-1]
            if last_entry['date'] == today:
                last_entry['rate'] = success_rate
            else:
                metrics['daily_success_rate'].append({
                    'date': today,
                    'rate': success_rate
                })
        else:
            metrics['daily_success_rate'].append({
                'date': today,
                'rate': success_rate
            })
        
        metrics['daily_success_rate'] = metrics['daily_success_rate'][-30:]
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° ì¤€ë¹„"""
        dashboard_data = {
            'summary': {
                'total_apis': len(api_key_manager.api_configs),
                'online_apis': 0,
                'total_calls': 0,
                'total_cost': 0.0,
                'avg_response_time': 0.0
            },
            'apis': {},
            'trends': {
                'hourly_calls': defaultdict(int),
                'daily_costs': defaultdict(float)
            }
        }
        
        # APIë³„ ë°ì´í„° ìˆ˜ì§‘
        for api_name in api_key_manager.api_configs:
            status = self.get_status(api_name)
            metrics = self.get_metrics(api_name)
            
            if status == APIStatus.ONLINE:
                dashboard_data['summary']['online_apis'] += 1
            
            dashboard_data['summary']['total_calls'] += metrics['total_calls']
            dashboard_data['summary']['total_cost'] += metrics['total_cost']
            
            dashboard_data['apis'][api_name] = {
                'status': status,
                'metrics': metrics,
                'success_rate': self.get_success_rate(api_name),
                'avg_response_time': self.get_average_response_time(api_name)
            }
            
            # íŠ¸ë Œë“œ ë°ì´í„°
            for hour, count in metrics['hourly_calls'].items():
                dashboard_data['trends']['hourly_calls'][hour] += count
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„
        total_time = sum(d['metrics']['total_response_time'] for d in dashboard_data['apis'].values())
        total_success = sum(d['metrics']['successful_calls'] for d in dashboard_data['apis'].values())
        
        if total_success > 0:
            dashboard_data['summary']['avg_response_time'] = total_time / total_success
        
        return dashboard_data
    
    def display_enhanced_dashboard(self):
        """í–¥ìƒëœ ìƒíƒœ ëŒ€ì‹œë³´ë“œ"""
        data = self.get_dashboard_data()
        
        # ìš”ì•½ ë©”íŠ¸ë¦­
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "í™œì„± API",
                f"{data['summary']['online_apis']}/{data['summary']['total_apis']}",
                delta=f"{data['summary']['online_apis']/data['summary']['total_apis']*100:.0f}%"
            )
        
        with col2:
            st.metric(
                "ì´ í˜¸ì¶œ ìˆ˜",
                f"{data['summary']['total_calls']:,}",
                delta=f"+{data['summary']['total_calls']}"
            )
        
        with col3:
            st.metric(
                "í‰ê·  ì‘ë‹µì‹œê°„",
                f"{data['summary']['avg_response_time']:.2f}s"
            )
        
        with col4:
            st.metric(
                "ì´ ë¹„ìš©",
                f"${data['summary']['total_cost']:.4f}"
            )
        
        # ì‹œê°„ë³„ íŠ¸ë Œë“œ ì°¨íŠ¸
        if data['trends']['hourly_calls']:
            fig = self._create_trend_chart(data['trends']['hourly_calls'])
            st.plotly_chart(fig, use_container_width=True)
        
        # APIë³„ ìƒì„¸ ì •ë³´
        st.markdown("### APIë³„ ìƒì„¸ ì •ë³´")
        
        for api_name, api_data in data['apis'].items():
            if api_data['metrics']['total_calls'] > 0:
                with st.expander(f"{api_key_manager.api_configs[api_name]['name']} ({api_name})"):
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("ìƒíƒœ", api_data['status'].value)
                        st.metric("ì„±ê³µë¥ ", f"{api_data['success_rate']:.1f}%")
                    
                    with col2:
                        st.metric("í‰ê·  ì‘ë‹µì‹œê°„", f"{api_data['avg_response_time']:.2f}s")
                        st.metric("ì´ í† í°", f"{api_data['metrics']['total_tokens']:,}")
                    
                    with col3:
                        st.metric("ì´ í˜¸ì¶œ", api_data['metrics']['total_calls'])
                        st.metric("ë¹„ìš©", f"${api_data['metrics']['total_cost']:.4f}")
                    
                    # ìµœê·¼ ì—ëŸ¬
                    if api_data['metrics']['errors']:
                        st.markdown("#### ìµœê·¼ ì—ëŸ¬")
                        for error in api_data['metrics']['errors'][-3:]:
                            st.error(f"{error['time'].strftime('%H:%M:%S')} - {error['error']}")
    
    def _create_trend_chart(self, hourly_data: Dict[str, int]) -> go.Figure:
        """íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„±"""
        # ë°ì´í„° ì •ë ¬
        sorted_hours = sorted(hourly_data.keys())
        hours = [h.split()[-1] for h in sorted_hours]
        counts = [hourly_data[h] for h in sorted_hours]
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=hours,
            y=counts,
            mode='lines+markers',
            name='API í˜¸ì¶œ ìˆ˜',
            line=dict(color='#667eea', width=2),
            marker=dict(size=8)
        ))
        
        fig.update_layout(
            title="ì‹œê°„ë³„ API í˜¸ì¶œ íŠ¸ë Œë“œ",
            xaxis_title="ì‹œê°„",
            yaxis_title="í˜¸ì¶œ ìˆ˜",
            hovermode='x unified',
            template='plotly_white'
        )
        
        return fig

# ì „ì—­ API ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
api_monitor = APIMonitor()

# ==================== ë²ˆì—­ ì„œë¹„ìŠ¤ (í™•ì¥) ====================
class TranslationService:
    """ë‹¤êµ­ì–´ ë²ˆì—­ ì„œë¹„ìŠ¤ (í™•ì¥íŒ)"""
    
    def __init__(self):
        self.translator = None
        self.available = False
        self.cache = {}
        self.supported_languages = SUPPORTED_LANGUAGES
        self.language_detector = None
        
        # ê¸°ìˆ  ìš©ì–´ ì‚¬ì „
        self.technical_terms = {
            'ko': {
                'polymer': 'ê³ ë¶„ì',
                'experiment': 'ì‹¤í—˜',
                'design': 'ì„¤ê³„',
                'factor': 'ì¸ì',
                'response': 'ë°˜ì‘',
                'optimization': 'ìµœì í™”',
                'analysis': 'ë¶„ì„',
                'thermoplastic': 'ì—´ê°€ì†Œì„±',
                'thermosetting': 'ì—´ê²½í™”ì„±',
                'elastomer': 'íƒ„ì„±ì²´',
                'composite': 'ë³µí•©ì¬ë£Œ'
            },
            'en': {
                'ê³ ë¶„ì': 'polymer',
                'ì‹¤í—˜': 'experiment',
                'ì„¤ê³„': 'design',
                'ì¸ì': 'factor',
                'ë°˜ì‘': 'response',
                'ìµœì í™”': 'optimization',
                'ë¶„ì„': 'analysis',
                'ì—´ê°€ì†Œì„±': 'thermoplastic',
                'ì—´ê²½í™”ì„±': 'thermosetting',
                'íƒ„ì„±ì²´': 'elastomer',
                'ë³µí•©ì¬ë£Œ': 'composite'
            }
        }
        
        self._initialize()
    
    def _initialize(self):
        """ë²ˆì—­ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        if TRANSLATION_AVAILABLE:
            try:
                self.translator = Translator()
                self.available = True
                logger.info("ë²ˆì—­ ì„œë¹„ìŠ¤ í™œì„±í™”")
                
                # ì–¸ì–´ ê°ì§€ê¸° ì´ˆê¸°í™”
                if NLP_AVAILABLE:
                    import spacy
                    try:
                        self.language_detector = spacy.load("xx_ent_wiki_sm")
                    except:
                        pass
                        
            except Exception as e:
                logger.error(f"ë²ˆì—­ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€ (ê°œì„ )"""
        if not self.available or not text:
            return 'en'
        
        try:
            # langdetect ì‚¬ìš©
            detected = langdetect.detect(text)
            
            # ì‹ ë¢°ë„ í™•ì¸
            probs = langdetect.detect_langs(text)
            if probs and probs[0].prob > 0.9:
                return detected
            
            # ë¶ˆí™•ì‹¤í•œ ê²½ìš° ì¶”ê°€ ê²€ì¦
            if self.language_detector:
                # spaCyë¥¼ ì‚¬ìš©í•œ ì¶”ê°€ ê²€ì¦
                doc = self.language_detector(text)
                if doc.lang_:
                    return doc.lang_
            
            return detected
            
        except Exception as e:
            logger.warning(f"ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            return 'en'
    
    def translate(self, text: str, target_lang: str = 'ko', 
                 source_lang: str = None, preserve_terms: bool = True) -> str:
        """í…ìŠ¤íŠ¸ ë²ˆì—­ (ê°œì„ )"""
        if not self.available or not text:
            return text
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{text}_{source_lang}_{target_lang}_{preserve_terms}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            if source_lang is None:
                source_lang = self.detect_language(text)
            
            if source_lang == target_lang:
                return text
            
            # ê¸°ìˆ  ìš©ì–´ ë³´í˜¸
            protected_text = text
            replacements = {}
            
            if preserve_terms and source_lang in self.technical_terms:
                terms = self.technical_terms[source_lang]
                for term, translation in terms.items():
                    if term in protected_text:
                        placeholder = f"__TERM_{len(replacements)}__"
                        protected_text = protected_text.replace(term, placeholder)
                        replacements[placeholder] = self.technical_terms.get(
                            target_lang, {}
                        ).get(term, translation)
            
            # ë²ˆì—­
            result = self.translator.translate(
                protected_text,
                src=source_lang,
                dest=target_lang
            )
            
            translated_text = result.text
            
            # ë³´í˜¸ëœ ìš©ì–´ ë³µì›
            for placeholder, term in replacements.items():
                translated_text = translated_text.replace(placeholder, term)
            
            self.cache[cache_key] = translated_text
            return translated_text
            
        except Exception as e:
            logger.error(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text
    
    def translate_dataframe(self, df: pd.DataFrame, columns: List[str], 
                          target_lang: str = 'ko', 
                          preserve_terms: bool = True) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ ë²ˆì—­ (ê°œì„ )"""
        if not self.available:
            return df
        
        df_translated = df.copy()
        
        # ì§„í–‰ë¥  í‘œì‹œ
        progress_bar = st.progress(0)
        total_cells = len(columns) * len(df)
        current = 0
        
        for col in columns:
            if col in df.columns:
                translated_col = f"{col}_{target_lang}"
                df_translated[translated_col] = ""
                
                for idx, value in df[col].items():
                    if pd.notna(value):
                        df_translated.at[idx, translated_col] = self.translate(
                            str(value), target_lang, preserve_terms=preserve_terms
                        )
                    
                    current += 1
                    progress_bar.progress(current / total_cells)
        
        progress_bar.empty()
        return df_translated
    
    def create_multilingual_report(self, report_content: str, 
                                 languages: List[str] = ['en', 'ko', 'ja']) -> Dict[str, str]:
        """ë‹¤êµ­ì–´ ë³´ê³ ì„œ ìƒì„±"""
        reports = {}
        
        for lang in languages:
            if lang in self.supported_languages:
                reports[lang] = self.translate(
                    report_content, 
                    target_lang=lang,
                    preserve_terms=True
                )
        
        return reports

# ì „ì—­ ë²ˆì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
translation_service = TranslationService()

# ==================== ê³ ê¸‰ ì‹¤í—˜ ì„¤ê³„ ì—”ì§„ ====================
class AdvancedDesignEngine:
    """ê³ ê¸‰ ì‹¤í—˜ ì„¤ê³„ ì—”ì§„"""
    
    def __init__(self):
        self.base_engine = ExperimentDesignEngine()
        self.ml_models = {}
        self.design_history = []
        
    def generate_adaptive_design(self, 
                               factors: List[ExperimentFactor],
                               responses: List[ExperimentResponse],
                               initial_data: pd.DataFrame = None,
                               budget: int = 50,
                               strategy: str = 'expected_improvement') -> pd.DataFrame:
        """ì ì‘í˜• ì‹¤í—˜ ì„¤ê³„ ìƒì„±"""
        
        # ì´ˆê¸° ì„¤ê³„
        if initial_data is None:
            # ì´ˆê¸° ì‹¤í—˜ì  ìƒì„± (LHS ë˜ëŠ” ì‘ì€ factorial)
            n_initial = min(len(factors) * 4, budget // 3)
            initial_design = self.base_engine.generate_design(
                factors, 
                method='latin_hypercube',
                n_samples=n_initial
            )
        else:
            initial_design = initial_data
            n_initial = len(initial_data)
        
        # ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
        bounds = []
        for factor in factors:
            if not factor.categorical:
                bounds.append((factor.min_value, factor.max_value))
        
        # ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ëª¨ë¸ ìƒì„±
        kernel = Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)
        gp_model = GaussianProcessRegressor(
            kernel=kernel,
            alpha=1e-6,
            normalize_y=True,
            n_restarts_optimizer=10
        )
        
        # ìˆœì°¨ì  ì„¤ê³„
        current_design = initial_design.copy()
        remaining_budget = budget - n_initial
        
        for i in range(remaining_budget):
            # ë‹¤ìŒ ì‹¤í—˜ì  ì„ íƒ
            next_point = self._select_next_point(
                current_design,
                factors,
                gp_model,
                bounds,
                strategy
            )
            
            # ì„¤ê³„ì— ì¶”ê°€
            new_row = pd.DataFrame([next_point])
            new_row['Run'] = len(current_design) + 1
            new_row['Adaptive'] = True
            
            current_design = pd.concat([current_design, new_row], ignore_index=True)
        
        return current_design
    
    def _select_next_point(self, 
                          current_design: pd.DataFrame,
                          factors: List[ExperimentFactor],
                          gp_model: GaussianProcessRegressor,
                          bounds: List[Tuple[float, float]],
                          strategy: str) -> Dict[str, Any]:
        """ë‹¤ìŒ ì‹¤í—˜ì  ì„ íƒ"""
        
        # í˜„ì¬ ë°ì´í„°ë¡œ GP ëª¨ë¸ í•™ìŠµ (ì‹œë®¬ë ˆì´ì…˜)
        X = current_design[[f.name for f in factors if not f.categorical]].values
        
        # ê°€ìƒì˜ ë°˜ì‘ê°’ ìƒì„± (ì‹¤ì œë¡œëŠ” ì‹¤í—˜ ê²°ê³¼ ì‚¬ìš©)
        y = np.random.randn(len(X))  # ì‹¤ì œ êµ¬í˜„ì‹œ ì‹¤í—˜ ê²°ê³¼ ì‚¬ìš©
        
        if len(X) > 0:
            gp_model.fit(X, y)
        
        # íšë“ í•¨ìˆ˜ ìµœì í™”
        if strategy == 'expected_improvement':
            acquisition_func = self._expected_improvement
        elif strategy == 'upper_confidence_bound':
            acquisition_func = self._upper_confidence_bound
        else:
            acquisition_func = self._probability_of_improvement
        
        # ìµœì í™”
        result = differential_evolution(
            lambda x: -acquisition_func(x.reshape(1, -1), gp_model, y.max() if len(y) > 0 else 0),
            bounds,
            seed=42,
            maxiter=100
        )
        
        # ë‹¤ìŒ í¬ì¸íŠ¸ ìƒì„±
        next_point = {}
        continuous_idx = 0
        
        for factor in factors:
            if factor.categorical:
                # ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ëœë¤ ì„ íƒ
                next_point[factor.name] = np.random.choice(factor.categories)
            else:
                next_point[factor.name] = result.x[continuous_idx]
                continuous_idx += 1
        
        return next_point
    
    def _expected_improvement(self, X: np.ndarray, gp_model: GaussianProcessRegressor, 
                            y_best: float, xi: float = 0.01) -> np.ndarray:
        """Expected Improvement íšë“ í•¨ìˆ˜"""
        mu, sigma = gp_model.predict(X, return_std=True)
        
        with np.errstate(divide='warn'):
            Z = (mu - y_best - xi) / sigma
            ei = (mu - y_best - xi) * stats.norm.cdf(Z) + sigma * stats.norm.pdf(Z)
            ei[sigma == 0.0] = 0.0
        
        return ei
    
    def _upper_confidence_bound(self, X: np.ndarray, gp_model: GaussianProcessRegressor,
                               beta: float = 2.0) -> np.ndarray:
        """Upper Confidence Bound íšë“ í•¨ìˆ˜"""
        mu, sigma = gp_model.predict(X, return_std=True)
        return mu + beta * sigma
    
    def _probability_of_improvement(self, X: np.ndarray, gp_model: GaussianProcessRegressor,
                                  y_best: float, xi: float = 0.01) -> np.ndarray:
        """Probability of Improvement íšë“ í•¨ìˆ˜"""
        mu, sigma = gp_model.predict(X, return_std=True)
        
        with np.errstate(divide='warn'):
            Z = (mu - y_best - xi) / sigma
            pi = stats.norm.cdf(Z)
            pi[sigma == 0.0] = 0.0
        
        return pi
    
    def generate_mixture_design(self, 
                              components: List[str],
                              constraints: Dict[str, Tuple[float, float]] = None,
                              include_process_vars: List[ExperimentFactor] = None,
                              design_type: str = 'simplex_lattice',
                              degree: int = 3) -> pd.DataFrame:
        """í˜¼í•©ë¬¼ ì‹¤í—˜ ì„¤ê³„ ìƒì„±"""
        
        n_components = len(components)
        
        if design_type == 'simplex_lattice':
            # Simplex-Lattice ì„¤ê³„
            points = self._generate_simplex_lattice(n_components, degree)
        elif design_type == 'simplex_centroid':
            # Simplex-Centroid ì„¤ê³„
            points = self._generate_simplex_centroid(n_components)
        elif design_type == 'extreme_vertices':
            # Extreme Vertices ì„¤ê³„
            points = self._generate_extreme_vertices(n_components, constraints)
        else:
            # ê¸°ë³¸: ê· ë“± ë¶„í¬
            points = self._generate_uniform_mixture(n_components, 20)
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        design = pd.DataFrame(points, columns=components)
        
        # ì œì•½ ì¡°ê±´ í™•ì¸ ë° í•„í„°ë§
        if constraints:
            valid_rows = []
            for idx, row in design.iterrows():
                valid = True
                for comp, (min_val, max_val) in constraints.items():
                    if comp in row:
                        if row[comp] < min_val or row[comp] > max_val:
                            valid = False
                            break
                if valid:
                    valid_rows.append(idx)
            
            design = design.loc[valid_rows].reset_index(drop=True)
        
        # ê³µì • ë³€ìˆ˜ ì¶”ê°€
        if include_process_vars:
            process_design = self.base_engine.generate_design(
                include_process_vars,
                method='full_factorial'
            )
            
            # í˜¼í•©ë¬¼ x ê³µì • ë³€ìˆ˜ ì¡°í•©
            n_mixture = len(design)
            n_process = len(process_design)
            
            expanded_design = pd.DataFrame()
            
            for i in range(n_mixture):
                for j in range(n_process):
                    row = pd.concat([
                        design.iloc[i],
                        process_design.iloc[j].drop('Run')
                    ])
                    expanded_design = expanded_design.append(row, ignore_index=True)
            
            design = expanded_design
        
        # Run ë²ˆí˜¸ ì¶”ê°€
        design.insert(0, 'Run', range(1, len(design) + 1))
        
        return design
    
    def _generate_simplex_lattice(self, n_components: int, degree: int) -> np.ndarray:
        """Simplex-Lattice í¬ì¸íŠ¸ ìƒì„±"""
        points = []
        
        # ê° ë ˆë²¨ì—ì„œì˜ ê°€ëŠ¥í•œ ì¡°í•© ìƒì„±
        def generate_combinations(n, q, current=[]):
            if len(current) == n:
                if sum(current) == q:
                    points.append([x/q for x in current])
                return
            
            for i in range(q - sum(current) + 1):
                generate_combinations(n, q, current + [i])
        
        generate_combinations(n_components, degree, [])
        
        return np.array(points)
    
    def _generate_simplex_centroid(self, n_components: int) -> np.ndarray:
        """Simplex-Centroid í¬ì¸íŠ¸ ìƒì„±"""
        points = []
        
        # ì •ì 
        for i in range(n_components):
            point = [0] * n_components
            point[i] = 1
            points.append(point)
        
        # ëª¨ë“  ë¶€ë¶„ì§‘í•©ì˜ ì¤‘ì‹¬ì 
        from itertools import combinations
        
        for r in range(2, n_components + 1):
            for combo in combinations(range(n_components), r):
                point = [0] * n_components
                for idx in combo:
                    point[idx] = 1 / len(combo)
                points.append(point)
        
        return np.array(points)
    
    def _generate_extreme_vertices(self, n_components: int, 
                                 constraints: Dict[str, Tuple[float, float]]) -> np.ndarray:
        """Extreme Vertices ì„¤ê³„"""
        # ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê·¹ì  ì°¾ê¸°
        # ê°„ë‹¨í•œ êµ¬í˜„: ê·¸ë¦¬ë“œ ì„œì¹˜
        points = []
        
        # ê° ì„±ë¶„ì˜ ë²”ìœ„
        ranges = []
        for i in range(n_components):
            comp_name = f"Component_{i+1}"
            if comp_name in constraints:
                ranges.append(constraints[comp_name])
            else:
                ranges.append((0, 1))
        
        # ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ ìƒì„±
        n_points_per_dim = 5
        grid_points = []
        
        for min_val, max_val in ranges:
            grid_points.append(np.linspace(min_val, max_val, n_points_per_dim))
        
        # ê°€ëŠ¥í•œ ì¡°í•© ì¤‘ í•©ì´ 1ì¸ ê²ƒë§Œ ì„ íƒ
        from itertools import product
        
        for combo in product(*grid_points):
            if abs(sum(combo) - 1.0) < 0.01:  # í—ˆìš© ì˜¤ì°¨
                points.append(list(combo))
        
        return np.array(points) if points else np.array([[1/n_components] * n_components])
    
    def _generate_uniform_mixture(self, n_components: int, n_points: int) -> np.ndarray:
        """ê· ë“± í˜¼í•©ë¬¼ í¬ì¸íŠ¸ ìƒì„±"""
        points = []
        
        for _ in range(n_points):
            # Dirichlet ë¶„í¬ ì‚¬ìš©
            point = np.random.dirichlet(np.ones(n_components))
            points.append(point)
        
        return np.array(points)

# ==================== ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ====================
class MLPredictionSystem:
    """ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.model_performance = {}
        
    def train_models(self, 
                    X: pd.DataFrame, 
                    y: pd.Series,
                    model_types: List[str] = ['rf', 'gb', 'xgb', 'nn'],
                    cv_folds: int = 5) -> Dict[str, Dict[str, float]]:
        """ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        X_scaled, scaler = self._preprocess_data(X)
        self.scalers['main'] = scaler
        
        results = {}
        
        for model_type in model_types:
            logger.info(f"í•™ìŠµ ì¤‘: {model_type}")
            
            # ëª¨ë¸ ìƒì„±
            model = self._create_model(model_type, X.shape[1])
            
            # êµì°¨ ê²€ì¦
            cv_scores = cross_val_score(
                model, X_scaled, y,
                cv=cv_folds,
                scoring='r2',
                n_jobs=-1
            )
            
            # ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ
            model.fit(X_scaled, y)
            
            # ì˜ˆì¸¡ ë° í‰ê°€
            y_pred = model.predict(X_scaled)
            
            metrics = {
                'r2': r2_score(y, y_pred),
                'mse': mean_squared_error(y, y_pred),
                'mae': mean_absolute_error(y, y_pred),
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            # ëª¨ë¸ ì €ì¥
            self.models[model_type] = model
            self.model_performance[model_type] = metrics
            results[model_type] = metrics
            
            # íŠ¹ì„± ì¤‘ìš”ë„
            if hasattr(model, 'feature_importances_'):
                self.feature_importance[model_type] = dict(
                    zip(X.columns, model.feature_importances_)
                )
        
        return results
    
    def _preprocess_data(self, X: pd.DataFrame) -> Tuple[np.ndarray, StandardScaler]:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        X_encoded = pd.get_dummies(X, drop_first=True)
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_encoded)
        
        return X_scaled, scaler
    
    def _create_model(self, model_type: str, n_features: int):
        """ëª¨ë¸ ìƒì„±"""
        if model_type == 'rf':
            return RandomForestRegressor(
                n_estimators=100,
                max_depth=None,
                min_samples_split=2,
                min_samples_leaf=1,
                random_state=42,
                n_jobs=-1
            )
        
        elif model_type == 'gb':
            return GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=3,
                random_state=42
            )
        
        elif model_type == 'xgb':
            return xgb.XGBRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=3,
                random_state=42,
                n_jobs=-1
            )
        
        elif model_type == 'nn':
            return MLPRegressor(
                hidden_layer_sizes=(n_features * 2, n_features),
                activation='relu',
                solver='adam',
                alpha=0.0001,
                max_iter=1000,
                random_state=42
            )
        
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    def predict(self, X: pd.DataFrame, model_type: str = None, 
               return_uncertainty: bool = False) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        
        if model_type is None:
            # ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ ì„ íƒ
            model_type = max(self.model_performance.items(), 
                           key=lambda x: x[1]['cv_mean'])[0]
        
        if model_type not in self.models:
            raise ValueError(f"Model {model_type} not trained")
        
        # ì „ì²˜ë¦¬
        X_encoded = pd.get_dummies(X, drop_first=True)
        X_scaled = self.scalers['main'].transform(X_encoded)
        
        model = self.models[model_type]
        
        if return_uncertainty:
            if model_type == 'rf':
                # Random Forestì˜ ê²½ìš° ê°œë³„ íŠ¸ë¦¬ ì˜ˆì¸¡ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
                predictions = []
                for tree in model.estimators_:
                    predictions.append(tree.predict(X_scaled))
                
                predictions = np.array(predictions)
                mean_pred = predictions.mean(axis=0)
                std_pred = predictions.std(axis=0)
                
                return mean_pred, std_pred
            else:
                # ë‹¤ë¥¸ ëª¨ë¸ì˜ ê²½ìš° ê¸°ë³¸ ì˜ˆì¸¡ë§Œ
                pred = model.predict(X_scaled)
                return pred, np.zeros_like(pred)
        else:
            return model.predict(X_scaled)
    
    def explain_predictions(self, X: pd.DataFrame, model_type: str = None) -> pd.DataFrame:
        """ì˜ˆì¸¡ ì„¤ëª… (SHAP values ê³„ì‚°)"""
        try:
            import shap
            
            if model_type is None:
                model_type = max(self.model_performance.items(), 
                               key=lambda x: x[1]['cv_mean'])[0]
            
            model = self.models[model_type]
            X_encoded = pd.get_dummies(X, drop_first=True)
            X_scaled = self.scalers['main'].transform(X_encoded)
            
            # SHAP ì„¤ëª…ì ìƒì„±
            if model_type in ['rf', 'gb', 'xgb']:
                explainer = shap.TreeExplainer(model)
            else:
                explainer = shap.KernelExplainer(model.predict, X_scaled[:100])
            
            # SHAP values ê³„ì‚°
            shap_values = explainer.shap_values(X_scaled)
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            shap_df = pd.DataFrame(
                shap_values,
                columns=X_encoded.columns,
                index=X.index
            )
            
            return shap_df
            
        except ImportError:
            logger.warning("SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
    
    def optimize_hyperparameters(self, X: pd.DataFrame, y: pd.Series, 
                               model_type: str, n_trials: int = 50) -> Dict[str, Any]:
        """ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        try:
            import optuna
            
            X_scaled, _ = self._preprocess_data(X)
            
            def objective(trial):
                # ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜
                if model_type == 'rf':
                    params = {
                        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                        'max_depth': trial.suggest_int('max_depth', 3, 20),
                        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
                    }
                    model = RandomForestRegressor(**params, random_state=42)
                
                elif model_type == 'xgb':
                    params = {
                        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                        'max_depth': trial.suggest_int('max_depth', 3, 10),
                        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)
                    }
                    model = xgb.XGBRegressor(**params, random_state=42)
                
                else:
                    return 0
                
                # êµì°¨ ê²€ì¦
                scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
                return scores.mean()
            
            # ìµœì í™” ì‹¤í–‰
            study = optuna.create_study(direction='maximize')
            study.optimize(objective, n_trials=n_trials)
            
            return {
                'best_params': study.best_params,
                'best_score': study.best_value,
                'optimization_history': study.trials_dataframe()
            }
            
        except ImportError:
            logger.warning("Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}

# Polymer-doe-platform - Part 5
# ==================== í†µê³„ ë¶„ì„ ì—”ì§„ (í™•ì¥) ====================
class AdvancedStatisticalAnalyzer:
    """ê³ ê¸‰ í†µê³„ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self):
        self.basic_analyzer = StatisticalAnalyzer()
        self.results_cache = {}
        
    def comprehensive_analysis(self, 
                             design: pd.DataFrame, 
                             results: pd.DataFrame,
                             responses: List[ExperimentResponse],
                             alpha: float = 0.05) -> Dict[str, Any]:
        """ì¢…í•© í†µê³„ ë¶„ì„"""
        
        analysis_results = {
            'timestamp': datetime.now(),
            'design_info': self._analyze_design_properties(design),
            'descriptive': {},
            'inferential': {},
            'regression': {},
            'diagnostics': {},
            'recommendations': []
        }
        
        # ê° ë°˜ì‘ ë³€ìˆ˜ë³„ ë¶„ì„
        for response in responses:
            if response.name not in results.columns:
                continue
            
            response_data = results[response.name].dropna()
            
            # 1. ê¸°ìˆ  í†µê³„
            analysis_results['descriptive'][response.name] = self._enhanced_descriptive_stats(
                response_data, response
            )
            
            # 2. ì •ê·œì„± ë° ë¶„í¬ ê²€ì •
            analysis_results['diagnostics'][response.name] = self._distribution_tests(
                response_data
            )
            
            # 3. ANOVA ë° íš¨ê³¼ ë¶„ì„
            analysis_results['inferential'][response.name] = self._comprehensive_anova(
                design, response_data, alpha
            )
            
            # 4. íšŒê·€ ë¶„ì„
            analysis_results['regression'][response.name] = self._advanced_regression(
                design, response_data, response
            )
            
            # 5. ìµœì í™” ê¶Œì¥ì‚¬í•­
            recommendations = self._generate_recommendations(
                analysis_results, response
            )
            analysis_results['recommendations'].extend(recommendations)
        
        # ë‹¤ì¤‘ ë°˜ì‘ ë¶„ì„
        if len(responses) > 1:
            analysis_results['multi_response'] = self._multi_response_analysis(
                design, results, responses
            )
        
        return analysis_results
    
    def _analyze_design_properties(self, design: pd.DataFrame) -> Dict[str, Any]:
        """ì„¤ê³„ íŠ¹ì„± ë¶„ì„"""
        factor_cols = [col for col in design.columns if col not in ['Run', 'Block', 'Adaptive']]
        
        properties = {
            'n_runs': len(design),
            'n_factors': len(factor_cols),
            'factors': factor_cols,
            'design_type': self._identify_design_type(design),
            'balance': self._check_balance(design, factor_cols),
            'orthogonality': self._check_orthogonality(design, factor_cols),
            'power': self._calculate_statistical_power(design)
        }
        
        return properties
    
    def _identify_design_type(self, design: pd.DataFrame) -> str:
        """ì„¤ê³„ ìœ í˜• ì‹ë³„"""
        n_runs = len(design)
        factor_cols = [col for col in design.columns if col not in ['Run', 'Block', 'Adaptive']]
        n_factors = len(factor_cols)
        
        # ê° ì¸ìì˜ ìˆ˜ì¤€ ìˆ˜ í™•ì¸
        levels = []
        for col in factor_cols:
            levels.append(len(design[col].unique()))
        
        # ì™„ì „ ìš”ì¸ ì„¤ê³„ í™•ì¸
        expected_full = np.prod(levels)
        if n_runs == expected_full:
            return "Full Factorial"
        
        # ë¶€ë¶„ ìš”ì¸ ì„¤ê³„ í™•ì¸
        if all(l == 2 for l in levels) and n_runs < expected_full:
            resolution = self._estimate_resolution(design, factor_cols)
            return f"Fractional Factorial (Resolution {resolution})"
        
        # ì¤‘ì‹¬ì  í¬í•¨ í™•ì¸
        center_points = []
        for col in factor_cols:
            if design[col].dtype in ['float64', 'int64']:
                mid_value = (design[col].min() + design[col].max()) / 2
                center_points.append(len(design[design[col] == mid_value]))
        
        if min(center_points) >= 3:
            if n_runs == 2**n_factors + 2*n_factors + min(center_points):
                return "Central Composite Design"
            elif self._is_box_behnken(design, factor_cols):
                return "Box-Behnken Design"
        
        # Plackett-Burman í™•ì¸
        pb_runs = [4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48]
        if n_runs in pb_runs and all(l == 2 for l in levels):
            return "Plackett-Burman Design"
        
        # ê¸°íƒ€
        if 'Adaptive' in design.columns and design['Adaptive'].any():
            return "Adaptive/Sequential Design"
        
        return "Custom Design"
    
    def _check_balance(self, design: pd.DataFrame, factor_cols: List[str]) -> Dict[str, bool]:
        """ê· í˜•ì„± í™•ì¸"""
        balance_info = {}
        
        for col in factor_cols:
            value_counts = design[col].value_counts()
            is_balanced = value_counts.std() / value_counts.mean() < 0.1 if len(value_counts) > 1 else True
            balance_info[col] = is_balanced
        
        balance_info['overall'] = all(balance_info.values())
        return balance_info
    
    def _check_orthogonality(self, design: pd.DataFrame, factor_cols: List[str]) -> float:
        """ì§êµì„± í™•ì¸"""
        # ìˆ˜ì¹˜í˜• ì¸ìë§Œ ì„ íƒ
        numeric_cols = [col for col in factor_cols if design[col].dtype in ['float64', 'int64']]
        
        if len(numeric_cols) < 2:
            return 1.0
        
        # ì½”ë“œí™” (-1, 1)
        coded_design = design[numeric_cols].copy()
        for col in numeric_cols:
            min_val = coded_design[col].min()
            max_val = coded_design[col].max()
            if max_val > min_val:
                coded_design[col] = 2 * (coded_design[col] - min_val) / (max_val - min_val) - 1
        
        # ìƒê´€ í–‰ë ¬
        corr_matrix = coded_design.corr()
        
        # ë¹„ëŒ€ê° ì›ì†Œì˜ í‰ê·  ì ˆëŒ€ê°’
        n = len(corr_matrix)
        off_diagonal_sum = 0
        count = 0
        
        for i in range(n):
            for j in range(i + 1, n):
                off_diagonal_sum += abs(corr_matrix.iloc[i, j])
                count += 1
        
        orthogonality = 1 - (off_diagonal_sum / count) if count > 0 else 1.0
        return orthogonality
    
    def _calculate_statistical_power(self, design: pd.DataFrame) -> Dict[str, float]:
        """í†µê³„ì  ê²€ì •ë ¥ ê³„ì‚°"""
        n = len(design)
        factor_cols = [col for col in design.columns if col not in ['Run', 'Block', 'Adaptive']]
        k = len(factor_cols)
        
        # ì£¼íš¨ê³¼ ê²€ì •ë ¥
        effect_size = 0.25  # Cohen's f
        alpha = 0.05
        
        # ë¹„ì¤‘ì‹¬ ëª¨ìˆ˜
        lambda_main = n * effect_size**2
        
        # F ë¶„í¬ ì„ê³„ê°’
        df1_main = k
        df2_main = n - k - 1
        
        power_results = {}
        
        if df2_main > 0:
            f_crit_main = stats.f.ppf(1 - alpha, df1_main, df2_main)
            power_main = 1 - stats.ncf.cdf(f_crit_main, df1_main, df2_main, lambda_main)
            power_results['main_effects'] = power_main
        
        # 2ì°¨ ìƒí˜¸ì‘ìš© ê²€ì •ë ¥
        if k >= 2:
            df1_int = k * (k - 1) // 2
            df2_int = n - df1_int - k - 1
            
            if df2_int > 0:
                lambda_int = n * (effect_size/2)**2  # ìƒí˜¸ì‘ìš©ì€ ì£¼íš¨ê³¼ì˜ ì ˆë°˜ìœ¼ë¡œ ê°€ì •
                f_crit_int = stats.f.ppf(1 - alpha, df1_int, df2_int)
                power_int = 1 - stats.ncf.cdf(f_crit_int, df1_int, df2_int, lambda_int)
                power_results['interactions'] = power_int
        
        return power_results
    
    def _enhanced_descriptive_stats(self, data: pd.Series, 
                                  response: ExperimentResponse) -> Dict[str, Any]:
        """í–¥ìƒëœ ê¸°ìˆ  í†µê³„"""
        stats_dict = {
            'count': len(data),
            'mean': data.mean(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max(),
            'range': data.max() - data.min(),
            'cv': (data.std() / data.mean() * 100) if data.mean() != 0 else np.inf,
            'q1': data.quantile(0.25),
            'median': data.median(),
            'q3': data.quantile(0.75),
            'iqr': data.quantile(0.75) - data.quantile(0.25),
            'skewness': stats.skew(data),
            'kurtosis': stats.kurtosis(data),
            'outliers': self._detect_outliers(data)
        }
        
        # ëª©í‘œê°’ê³¼ì˜ ë¹„êµ
        if response.target_value is not None:
            stats_dict['target_deviation'] = {
                'mean_deviation': abs(data.mean() - response.target_value),
                'percent_in_spec': self._calculate_in_spec_percentage(data, response),
                'process_capability': self._calculate_capability_indices(data, response)
            }
        
        # ì‹ ë¢°êµ¬ê°„
        confidence_level = 0.95
        stats_dict['confidence_interval'] = stats.t.interval(
            confidence_level,
            len(data) - 1,
            loc=data.mean(),
            scale=stats.sem(data)
        )
        
        return stats_dict
    
    def _detect_outliers(self, data: pd.Series) -> Dict[str, List[float]]:
        """ì´ìƒì¹˜ ê²€ì¶œ"""
        outliers = {}
        
        # IQR ë°©ë²•
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        iqr_outliers = data[(data < lower_bound) | (data > upper_bound)].tolist()
        outliers['iqr_method'] = iqr_outliers
        
        # Z-score ë°©ë²•
        z_scores = np.abs(stats.zscore(data))
        z_outliers = data[z_scores > 3].tolist()
        outliers['z_score_method'] = z_outliers
        
        # Modified Z-score (MAD ê¸°ë°˜)
        median = data.median()
        mad = np.median(np.abs(data - median))
        modified_z_scores = 0.6745 * (data - median) / mad if mad > 0 else np.zeros_like(data)
        mad_outliers = data[np.abs(modified_z_scores) > 3.5].tolist()
        outliers['mad_method'] = mad_outliers
        
        # Isolation Forest
        try:
            from sklearn.ensemble import IsolationForest
            
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            outlier_labels = iso_forest.fit_predict(data.values.reshape(-1, 1))
            iso_outliers = data[outlier_labels == -1].tolist()
            outliers['isolation_forest'] = iso_outliers
        except:
            pass
        
        return outliers
    
    def _calculate_in_spec_percentage(self, data: pd.Series, 
                                    response: ExperimentResponse) -> float:
        """ê·œê²© ë‚´ ë¹„ìœ¨ ê³„ì‚°"""
        lower, upper = response.specification_limits
        
        if lower is None and upper is None:
            return 100.0
        
        in_spec = data.copy()
        
        if lower is not None:
            in_spec = in_spec[in_spec >= lower]
        
        if upper is not None:
            in_spec = in_spec[in_spec <= upper]
        
        return len(in_spec) / len(data) * 100
    
    def _calculate_capability_indices(self, data: pd.Series, 
                                    response: ExperimentResponse) -> Dict[str, float]:
        """ê³µì •ëŠ¥ë ¥ì§€ìˆ˜ ê³„ì‚°"""
        lower, upper = response.specification_limits
        
        if lower is None and upper is None:
            return {}
        
        indices = {}
        
        # ê¸°ë³¸ í†µê³„
        mean = data.mean()
        std = data.std()
        
        if std == 0:
            return {'error': 'Standard deviation is zero'}
        
        # Cp (ì ì¬ ëŠ¥ë ¥)
        if lower is not None and upper is not None:
            cp = (upper - lower) / (6 * std)
            indices['Cp'] = cp
        
        # Cpk (ì‹¤ì œ ëŠ¥ë ¥)
        if lower is not None and upper is not None:
            cpu = (upper - mean) / (3 * std)
            cpl = (mean - lower) / (3 * std)
            cpk = min(cpu, cpl)
            indices['Cpk'] = cpk
            indices['Cpu'] = cpu
            indices['Cpl'] = cpl
        elif lower is not None:
            cpl = (mean - lower) / (3 * std)
            indices['Cpl'] = cpl
        elif upper is not None:
            cpu = (upper - mean) / (3 * std)
            indices['Cpu'] = cpu
        
        # Cpm (ëª©í‘œê°’ ê³ ë ¤)
        if response.target_value is not None:
            target = response.target_value
            
            if lower is not None and upper is not None:
                tau = np.sqrt(std**2 + (mean - target)**2)
                cpm = (upper - lower) / (6 * tau)
                indices['Cpm'] = cpm
        
        # ì˜ˆìƒ ë¶ˆëŸ‰ë¥  (ppm)
        if lower is not None and upper is not None:
            z_lower = (lower - mean) / std
            z_upper = (upper - mean) / std
            
            p_lower = stats.norm.cdf(z_lower)
            p_upper = 1 - stats.norm.cdf(z_upper)
            
            ppm = (p_lower + p_upper) * 1e6
            indices['expected_ppm'] = ppm
        
        return indices
    
    def _distribution_tests(self, data: pd.Series) -> Dict[str, Any]:
        """ë¶„í¬ ê²€ì •"""
        tests = {}
        
        # ì •ê·œì„± ê²€ì •
        # Shapiro-Wilk
        if len(data) >= 3:
            shapiro_stat, shapiro_p = stats.shapiro(data)
            tests['shapiro_wilk'] = {
                'statistic': shapiro_stat,
                'p_value': shapiro_p,
                'normal': shapiro_p > 0.05
            }
        
        # Anderson-Darling
        if len(data) >= 7:
            anderson_result = stats.anderson(data)
            tests['anderson_darling'] = {
                'statistic': anderson_result.statistic,
                'critical_values': dict(zip(
                    anderson_result.significance_level,
                    anderson_result.critical_values
                )),
                'normal': anderson_result.statistic < anderson_result.critical_values[2]  # 5% level
            }
        
        # Kolmogorov-Smirnov
        ks_stat, ks_p = stats.kstest(data, 'norm', args=(data.mean(), data.std()))
        tests['kolmogorov_smirnov'] = {
            'statistic': ks_stat,
            'p_value': ks_p,
            'normal': ks_p > 0.05
        }
        
        # ë‹¤ë¥¸ ë¶„í¬ ì í•©ë„ ê²€ì •
        distributions = {
            'exponential': stats.expon,
            'lognormal': stats.lognorm,
            'weibull': stats.weibull_min,
            'gamma': stats.gamma
        }
        
        best_fit = {'distribution': 'normal', 'aic': float('inf')}
        
        for dist_name, dist_func in distributions.items():
            try:
                # íŒŒë¼ë¯¸í„° ì¶”ì •
                params = dist_func.fit(data)
                
                # Log-likelihood
                log_likelihood = np.sum(dist_func.logpdf(data, *params))
                
                # AIC
                k = len(params)
                aic = 2 * k - 2 * log_likelihood
                
                if aic < best_fit['aic']:
                    best_fit = {
                        'distribution': dist_name,
                        'aic': aic,
                        'parameters': params
                    }
            except:
                continue
        
        tests['best_fit_distribution'] = best_fit
        
        return tests
    
    def _comprehensive_anova(self, design: pd.DataFrame, response_data: pd.Series, 
                           alpha: float = 0.05) -> Dict[str, Any]:
        """ì¢…í•© ANOVA ë¶„ì„"""
        results = {
            'main_effects': {},
            'interactions': {},
            'model_adequacy': {},
            'post_hoc': {}
        }
        
        # ì¸ì ì‹ë³„
        factor_cols = [col for col in design.columns 
                      if col not in ['Run', 'Block', 'Adaptive'] and col in design.columns]
        
        # ê° ì¸ìë³„ ì£¼íš¨ê³¼
        for factor in factor_cols:
            groups = []
            levels = design[factor].unique()
            
            for level in levels:
                mask = design[factor] == level
                if mask.sum() > 0:
                    groups.append(response_data[mask].values)
            
            if len(groups) >= 2:
                # One-way ANOVA
                f_stat, p_value = stats.f_oneway(*groups)
                
                # íš¨ê³¼ í¬ê¸°
                ss_between = sum(len(g) * (np.mean(g) - response_data.mean())**2 for g in groups)
                ss_total = sum((response_data - response_data.mean())**2)
                eta_squared = ss_between / ss_total if ss_total > 0 else 0
                
                # ê²€ì •ë ¥
                effect_size = np.sqrt(eta_squared / (1 - eta_squared)) if eta_squared < 1 else np.inf
                df1 = len(groups) - 1
                df2 = len(response_data) - len(groups)
                
                if df2 > 0:
                    lambda_nc = len(response_data) * effect_size**2
                    f_crit = stats.f.ppf(1 - alpha, df1, df2)
                    power = 1 - stats.ncf.cdf(f_crit, df1, df2, lambda_nc)
                else:
                    power = 0
                
                results['main_effects'][factor] = {
                    'f_statistic': f_stat,
                    'p_value': p_value,
                    'significant': p_value < alpha,
                    'eta_squared': eta_squared,
                    'power': power,
                    'levels': list(levels),
                    'means': {str(level): np.mean(groups[i]) for i, level in enumerate(levels)}
                }
                
                # ì‚¬í›„ ê²€ì • (Tukey HSD)
                if p_value < alpha and len(groups) > 2:
                    results['post_hoc'][factor] = self._tukey_hsd(groups, levels, alpha)
        
        # 2ì°¨ ìƒí˜¸ì‘ìš©
        if len(factor_cols) >= 2:
            from itertools import combinations
            
            for f1, f2 in combinations(factor_cols, 2):
                interaction_key = f"{f1}*{f2}"
                
                # 2-way ANOVAë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
                try:
                    interaction_results = self._two_way_anova(
                        design, response_data, f1, f2, alpha
                    )
                    
                    if interaction_results:
                        results['interactions'][interaction_key] = interaction_results
                except:
                    continue
        
        # ëª¨ë¸ ì í•©ë„
        results['model_adequacy'] = self._assess_model_adequacy(
            design, response_data, factor_cols
        )
        
        return results
    
    def _tukey_hsd(self, groups: List[np.ndarray], levels: List[Any], 
                  alpha: float = 0.05) -> List[Dict[str, Any]]:
        """Tukey HSD ì‚¬í›„ ê²€ì •"""
        from statsmodels.stats.multicomp import pairwise_tukeyhsd
        
        # ë°ì´í„° ì¤€ë¹„
        data_list = []
        group_list = []
        
        for i, (group, level) in enumerate(zip(groups, levels)):
            data_list.extend(group)
            group_list.extend([str(level)] * len(group))
        
        # Tukey HSD
        tukey_result = pairwise_tukeyhsd(data_list, group_list, alpha=alpha)
        
        # ê²°ê³¼ ì •ë¦¬
        comparisons = []
        for i in range(len(tukey_result.reject)):
            comparisons.append({
                'group1': tukey_result.groupsunique[tukey_result._results_table[i+1][0]],
                'group2': tukey_result.groupsunique[tukey_result._results_table[i+1][1]],
                'mean_diff': tukey_result._results_table[i+1][2],
                'p_adj': tukey_result._results_table[i+1][5],
                'reject': tukey_result.reject[i]
            })
        
        return comparisons
    
    def _two_way_anova(self, design: pd.DataFrame, response_data: pd.Series,
                      factor1: str, factor2: str, alpha: float = 0.05) -> Dict[str, Any]:
        """ì´ì› ë¶„ì‚°ë¶„ì„"""
        import statsmodels.api as sm
        from statsmodels.formula.api import ols
        
        # ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„
        anova_df = design[[factor1, factor2]].copy()
        anova_df['response'] = response_data
        
        # ëª¨ë¸ ì í•©
        formula = f'response ~ C({factor1}) + C({factor2}) + C({factor1}):C({factor2})'
        model = ols(formula, data=anova_df).fit()
        
        # ANOVA í…Œì´ë¸”
        anova_table = sm.stats.anova_lm(model, typ=2)
        
        # ê²°ê³¼ ì •ë¦¬
        interaction_row = f'C({factor1}):C({factor2})'
        
        if interaction_row in anova_table.index:
            return {
                'f_statistic': anova_table.loc[interaction_row, 'F'],
                'p_value': anova_table.loc[interaction_row, 'PR(>F)'],
                'significant': anova_table.loc[interaction_row, 'PR(>F)'] < alpha,
                'sum_sq': anova_table.loc[interaction_row, 'sum_sq'],
                'mean_sq': anova_table.loc[interaction_row, 'mean_sq']
            }
        
        return None
    
    def _assess_model_adequacy(self, design: pd.DataFrame, response_data: pd.Series,
                              factor_cols: List[str]) -> Dict[str, Any]:
        """ëª¨ë¸ ì í•©ë„ í‰ê°€"""
        from sklearn.linear_model import LinearRegression
        
        # ì„¤ê³„ í–‰ë ¬ ì¤€ë¹„
        X = pd.get_dummies(design[factor_cols], drop_first=True)
        y = response_data.values
        
        # ì„ í˜• ëª¨ë¸ ì í•©
        model = LinearRegression()
        model.fit(X, y)
        
        # ì˜ˆì¸¡ê°’ê³¼ ì”ì°¨
        y_pred = model.predict(X)
        residuals = y - y_pred
        
        # ì í•©ë„ ì§€í‘œ
        r_squared = r2_score(y, y_pred)
        adj_r_squared = 1 - (1 - r_squared) * (len(y) - 1) / (len(y) - X.shape[1] - 1)
        
        # ì”ì°¨ ë¶„ì„
        residual_analysis = {
            'mean': residuals.mean(),
            'std': residuals.std(),
            'normality': stats.shapiro(residuals)[1] if len(residuals) >= 3 else None,
            'homoscedasticity': self._breusch_pagan_test(X, residuals),
            'autocorrelation': self._durbin_watson(residuals)
        }
        
        # Lack of Fit ê²€ì •
        lack_of_fit = self._lack_of_fit_test(design, response_data, factor_cols)
        
        return {
            'r_squared': r_squared,
            'adjusted_r_squared': adj_r_squared,
            'residual_analysis': residual_analysis,
            'lack_of_fit': lack_of_fit
        }
    
    def _breusch_pagan_test(self, X: pd.DataFrame, residuals: np.ndarray) -> Dict[str, float]:
        """Breusch-Pagan ì´ë¶„ì‚°ì„± ê²€ì •"""
        # ì”ì°¨ ì œê³±
        residuals_squared = residuals ** 2
        
        # ë³´ì¡° íšŒê·€
        aux_model = LinearRegression()
        aux_model.fit(X, residuals_squared)
        aux_pred = aux_model.predict(X)
        
        # LM í†µê³„ëŸ‰
        n = len(residuals)
        rss = np.sum((residuals_squared - aux_pred) ** 2)
        tss = np.sum((residuals_squared - residuals_squared.mean()) ** 2)
        r_squared_aux = 1 - rss / tss if tss > 0 else 0
        
        lm_statistic = n * r_squared_aux
        p_value = 1 - stats.chi2.cdf(lm_statistic, X.shape[1])
        
        return {
            'statistic': lm_statistic,
            'p_value': p_value,
            'homoscedastic': p_value > 0.05
        }
    
    def _durbin_watson(self, residuals: np.ndarray) -> float:
        """Durbin-Watson ìê¸°ìƒê´€ ê²€ì •"""
        diff = np.diff(residuals)
        dw = np.sum(diff ** 2) / np.sum(residuals ** 2)
        return dw
    
    def _lack_of_fit_test(self, design: pd.DataFrame, response_data: pd.Series,
                         factor_cols: List[str]) -> Dict[str, Any]:
        """ì í•©ê²°ì—¬ ê²€ì •"""
        # ë°˜ë³µì‹¤í—˜ ì°¾ê¸°
        design_with_response = design[factor_cols].copy()
        design_with_response['response'] = response_data
        
        # ê·¸ë£¹í™”
        grouped = design_with_response.groupby(factor_cols)
        
        pure_error_ss = 0
        pure_error_df = 0
        
        for name, group in grouped:
            if len(group) > 1:
                group_mean = group['response'].mean()
                pure_error_ss += np.sum((group['response'] - group_mean) ** 2)
                pure_error_df += len(group) - 1
        
        if pure_error_df == 0:
            return {'test_possible': False, 'reason': 'No replicates found'}
        
        # ì „ì²´ ì˜¤ì°¨
        total_mean = response_data.mean()
        total_ss = np.sum((response_data - total_mean) ** 2)
        
        # ëª¨ë¸ ì í•©
        X = pd.get_dummies(design[factor_cols], drop_first=True)
        model = LinearRegression()
        model.fit(X, response_data)
        y_pred = model.predict(X)
        
        residual_ss = np.sum((response_data - y_pred) ** 2)
        
        # Lack of fit
        lack_of_fit_ss = residual_ss - pure_error_ss
        lack_of_fit_df = len(response_data) - X.shape[1] - 1 - pure_error_df
        
        if lack_of_fit_df <= 0:
            return {'test_possible': False, 'reason': 'Insufficient degrees of freedom'}
        
        # F ê²€ì •
        f_statistic = (lack_of_fit_ss / lack_of_fit_df) / (pure_error_ss / pure_error_df)
        p_value = 1 - stats.f.cdf(f_statistic, lack_of_fit_df, pure_error_df)
        
        return {
            'test_possible': True,
            'f_statistic': f_statistic,
            'p_value': p_value,
            'adequate_fit': p_value > 0.05,
            'pure_error_df': pure_error_df,
            'lack_of_fit_df': lack_of_fit_df
        }
    
    def _advanced_regression(self, design: pd.DataFrame, response_data: pd.Series,
                           response: ExperimentResponse) -> Dict[str, Any]:
        """ê³ ê¸‰ íšŒê·€ ë¶„ì„"""
        factor_cols = [col for col in design.columns 
                      if col not in ['Run', 'Block', 'Adaptive']]
        
        # ë‹¤í•­ì‹ ì°¨ìˆ˜ ê²°ì •
        poly_degree = self._determine_polynomial_degree(design, response_data, factor_cols)
        
        # ëª¨ë¸ êµ¬ì¶•
        from sklearn.preprocessing import PolynomialFeatures
        
        X = design[factor_cols]
        X_encoded = pd.get_dummies(X, drop_first=True)
        
        # ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„±
        if poly_degree > 1:
            poly = PolynomialFeatures(degree=poly_degree, include_bias=False)
            X_poly = poly.fit_transform(X_encoded)
            feature_names = poly.get_feature_names_out(X_encoded.columns)
        else:
            X_poly = X_encoded.values
            feature_names = X_encoded.columns.tolist()
        
        # ì—¬ëŸ¬ íšŒê·€ ë°©ë²• ë¹„êµ
        models = {
            'linear': LinearRegression(),
            'ridge': Ridge(alpha=1.0),
            'lasso': Lasso(alpha=0.1),
            'elastic_net': ElasticNet(alpha=0.1, l1_ratio=0.5)
        }
        
        best_model = None
        best_score = -np.inf
        model_results = {}
        
        for name, model in models.items():
            try:
                # êµì°¨ ê²€ì¦
                cv_scores = cross_val_score(model, X_poly, response_data, cv=5, scoring='r2')
                
                # ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ
                model.fit(X_poly, response_data)
                y_pred = model.predict(X_poly)
                
                # í‰ê°€
                r2 = r2_score(response_data, y_pred)
                
                model_results[name] = {
                    'r2': r2,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'coefficients': dict(zip(feature_names, model.coef_)) if hasattr(model, 'coef_') else {}
                }
                
                if cv_scores.mean() > best_score:
                    best_score = cv_scores.mean()
                    best_model = name
                    
            except:
                continue
        
        # ìµœì¢… ëª¨ë¸ë¡œ ìƒì„¸ ë¶„ì„
        if best_model:
            final_model = models[best_model]
            final_model.fit(X_poly, response_data)
            
            # ë°˜ì‘í‘œë©´ ë°©ì •ì‹
            equation = self._generate_regression_equation(
                final_model, feature_names, poly_degree
            )
            
            # ìµœì  ì¡°ê±´ ì˜ˆì¸¡
            optimal_conditions = self._find_optimal_conditions(
                final_model, X_encoded, response, poly
            )
            
            model_results['best_model'] = best_model
            model_results['equation'] = equation
            model_results['optimal_conditions'] = optimal_conditions
        
        return model_results
    
    def _determine_polynomial_degree(self, design: pd.DataFrame, response_data: pd.Series,
                                   factor_cols: List[str]) -> int:
        """ì ì ˆí•œ ë‹¤í•­ì‹ ì°¨ìˆ˜ ê²°ì •"""
        n_samples = len(design)
        n_factors = len(factor_cols)
        
        # íœ´ë¦¬ìŠ¤í‹± ê·œì¹™
        if n_samples < 10:
            return 1
        elif n_samples < 20:
            return min(2, n_factors)
        else:
            # AIC ê¸°ë°˜ ì„ íƒ
            best_aic = np.inf
            best_degree = 1
            
            for degree in range(1, min(4, n_factors + 1)):
                try:
                    from sklearn.preprocessing import PolynomialFeatures
                    
                    X = pd.get_dummies(design[factor_cols], drop_first=True)
                    poly = PolynomialFeatures(degree=degree, include_bias=False)
                    X_poly = poly.fit_transform(X)
                    
                    if X_poly.shape[1] >= n_samples:
                        break
                    
                    model = LinearRegression()
                    model.fit(X_poly, response_data)
                    y_pred = model.predict(X_poly)
                    
                    # AIC ê³„ì‚°
                    rss = np.sum((response_data - y_pred) ** 2)
                    k = X_poly.shape[1] + 1  # íŒŒë¼ë¯¸í„° ìˆ˜
                    
                    if n_samples > k + 1:
                        aic = n_samples * np.log(rss / n_samples) + 2 * k
                        
                        if aic < best_aic:
                            best_aic = aic
                            best_degree = degree
                except:
                    break
            
            return best_degree
    
    def _generate_regression_equation(self, model, feature_names: List[str], 
                                    degree: int) -> str:
        """íšŒê·€ ë°©ì •ì‹ ìƒì„±"""
        equation_parts = []
        
        # ì ˆí¸
        if hasattr(model, 'intercept_'):
            equation_parts.append(f"{model.intercept_:.4f}")
        
        # ê³„ìˆ˜
        if hasattr(model, 'coef_'):
            for feature, coef in zip(feature_names, model.coef_):
                if abs(coef) > 1e-6:
                    sign = "+" if coef > 0 else "-"
                    
                    if equation_parts or sign == "-":
                        equation_parts.append(f" {sign} {abs(coef):.4f}*{feature}")
                    else:
                        equation_parts.append(f"{coef:.4f}*{feature}")
        
        return "Y = " + "".join(equation_parts)
    
    def _find_optimal_conditions(self, model, X_encoded: pd.DataFrame, 
                               response: ExperimentResponse,
                               poly_transformer=None) -> Dict[str, Any]:
        """ìµœì  ì¡°ê±´ ì°¾ê¸°"""
        bounds = []
        for col in X_encoded.columns:
            if X_encoded[col].dtype in ['float64', 'int64']:
                bounds.append((X_encoded[col].min(), X_encoded[col].max()))
            else:
                bounds.append((0, 1))  # ë”ë¯¸ ë³€ìˆ˜
        
        # ëª©ì  í•¨ìˆ˜
        def objective(x):
            if poly_transformer:
                x_poly = poly_transformer.transform(x.reshape(1, -1))
            else:
                x_poly = x.reshape(1, -1)
            
            pred = model.predict(x_poly)[0]
            
            # ìµœì í™” ë°©í–¥
            if response.maximize:
                return -pred
            elif response.minimize:
                return pred
            elif response.target_value is not None:
                return abs(pred - response.target_value)
            else:
                return pred
        
        # ìµœì í™”
        result = differential_evolution(objective, bounds, seed=42, maxiter=1000)
        
        # ìµœì  ì¡°ê±´
        optimal_x = result.x
        if poly_transformer:
            optimal_pred = model.predict(poly_transformer.transform(optimal_x.reshape(1, -1)))[0]
        else:
            optimal_pred = model.predict(optimal_x.reshape(1, -1))[0]
        
        optimal_conditions = dict(zip(X_encoded.columns, optimal_x))
        
        return {
            'conditions': optimal_conditions,
            'predicted_value': optimal_pred,
            'optimization_success': result.success,
            'iterations': result.nit
        }
    
    def _multi_response_analysis(self, design: pd.DataFrame, results: pd.DataFrame,
                               responses: List[ExperimentResponse]) -> Dict[str, Any]:
        """ë‹¤ì¤‘ ë°˜ì‘ ë¶„ì„"""
        multi_results = {
            'correlation_matrix': {},
            'desirability': {},
            'pareto_optimal': [],
            'compromise_solution': {}
        }
        
        # ìƒê´€ í–‰ë ¬
        response_names = [r.name for r in responses if r.name in results.columns]
        if len(response_names) >= 2:
            corr_matrix = results[response_names].corr()
            multi_results['correlation_matrix'] = corr_matrix.to_dict()
        
        # ì¢…í•© ë°”ëŒì§í•¨ ì§€ìˆ˜
        desirability_scores = []
        
        for idx, row in results.iterrows():
            individual_desirabilities = []
            
            for response in responses:
                if response.name in row:
                    d = response.calculate_desirability(row[response.name])
                    individual_desirabilities.append(d * response.weight)
            
            if individual_desirabilities:
                # ê¸°í•˜í‰ê· 
                overall_desirability = np.prod(individual_desirabilities) ** (1/len(individual_desirabilities))
                desirability_scores.append(overall_desirability)
            else:
                desirability_scores.append(0)
        
        multi_results['desirability']['scores'] = desirability_scores
        multi_results['desirability']['best_run'] = int(np.argmax(desirability_scores))
        multi_results['desirability']['best_score'] = max(desirability_scores)
        
        # Pareto ìµœì í•´
        pareto_front = self._find_pareto_front(results, responses)
        multi_results['pareto_optimal'] = pareto_front
        
        # íƒ€í˜‘í•´ (TOPSIS)
        compromise = self._topsis_analysis(results, responses)
        multi_results['compromise_solution'] = compromise
        
        return multi_results
    
    def _find_pareto_front(self, results: pd.DataFrame, 
                          responses: List[ExperimentResponse]) -> List[int]:
        """Pareto ìµœì í•´ ì°¾ê¸°"""
        # ëª©ì  í•¨ìˆ˜ ê°’ ì¶”ì¶œ
        objectives = []
        
        for response in responses:
            if response.name in results.columns:
                values = results[response.name].values
                
                # ìµœëŒ€í™”ëŠ” ìŒìˆ˜ë¡œ ë³€í™˜ (ìµœì†Œí™”ë¡œ í†µì¼)
                if response.maximize:
                    objectives.append(-values)
                else:
                    objectives.append(values)
        
        if not objectives:
            return []
        
        objectives = np.array(objectives).T
        n_points = len(objectives)
        
        # Pareto ì§€ë°° í™•ì¸
        pareto_front = []
        
        for i in range(n_points):
            dominated = False
            
            for j in range(n_points):
                if i != j:
                    # jê°€ ië¥¼ ì§€ë°°í•˜ëŠ”ì§€ í™•ì¸
                    if all(objectives[j] <= objectives[i]) and any(objectives[j] < objectives[i]):
                        dominated = True
                        break
            
            if not dominated:
                pareto_front.append(i)
        
        return pareto_front
    
    def _topsis_analysis(self, results: pd.DataFrame, 
                        responses: List[ExperimentResponse]) -> Dict[str, Any]:
        """TOPSIS ë‹¤ê¸°ì¤€ ì˜ì‚¬ê²°ì •"""
        # ê²°ì • í–‰ë ¬ êµ¬ì„±
        decision_matrix = []
        weights = []
        directions = []  # True: maximize, False: minimize
        
        for response in responses:
            if response.name in results.columns:
                decision_matrix.append(results[response.name].values)
                weights.append(response.weight)
                directions.append(response.maximize)
        
        if not decision_matrix:
            return {}
        
        decision_matrix = np.array(decision_matrix).T
        weights = np.array(weights)
        weights = weights / weights.sum()  # ì •ê·œí™”
        
        # ì •ê·œí™”
        norm_matrix = decision_matrix / np.sqrt((decision_matrix ** 2).sum(axis=0))
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        weighted_matrix = norm_matrix * weights
        
        # ì´ìƒì ì¸ í•´ì™€ ë°˜ì´ìƒì ì¸ í•´
        ideal_solution = []
        anti_ideal_solution = []
        
        for j, maximize in enumerate(directions):
            if maximize:
                ideal_solution.append(weighted_matrix[:, j].max())
                anti_ideal_solution.append(weighted_matrix[:, j].min())
            else:
                ideal_solution.append(weighted_matrix[:, j].min())
                anti_ideal_solution.append(weighted_matrix[:, j].max())
        
        ideal_solution = np.array(ideal_solution)
        anti_ideal_solution = np.array(anti_ideal_solution)
        
        # ê±°ë¦¬ ê³„ì‚°
        dist_to_ideal = np.sqrt(((weighted_matrix - ideal_solution) ** 2).sum(axis=1))
        dist_to_anti_ideal = np.sqrt(((weighted_matrix - anti_ideal_solution) ** 2).sum(axis=1))
        
        # ìƒëŒ€ì  ê·¼ì ‘ë„
        relative_closeness = dist_to_anti_ideal / (dist_to_ideal + dist_to_anti_ideal + 1e-10)
        
        # ìµœì í•´
        best_idx = np.argmax(relative_closeness)
        
        return {
            'best_run': int(best_idx),
            'closeness_scores': relative_closeness.tolist(),
            'ideal_solution': ideal_solution.tolist(),
            'anti_ideal_solution': anti_ideal_solution.tolist()
        }
    
    def _generate_recommendations(self, analysis_results: Dict[str, Any],
                                response: ExperimentResponse) -> List[str]:
        """ë¶„ì„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # í†µê³„ì  ìœ ì˜ì„± ê¸°ë°˜
        if response.name in analysis_results['inferential']:
            anova_results = analysis_results['inferential'][response.name]
            
            significant_factors = [
                factor for factor, result in anova_results['main_effects'].items()
                if result['significant']
            ]
            
            if significant_factors:
                recommendations.append(
                    f"âœ… {response.name}ì— ìœ ì˜í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¸ì: {', '.join(significant_factors)}"
                )
                
                # ìµœì  ìˆ˜ì¤€ ì¶”ì²œ
                for factor in significant_factors:
                    means = anova_results['main_effects'][factor]['means']
                    
                    if response.maximize:
                        best_level = max(means.items(), key=lambda x: x[1])
                        recommendations.append(
                            f"   â†’ {factor}ëŠ” {best_level[0]} ìˆ˜ì¤€ì—ì„œ ìµœëŒ€ê°’ ë‹¬ì„±"
                        )
                    elif response.minimize:
                        best_level = min(means.items(), key=lambda x: x[1])
                        recommendations.append(
                            f"   â†’ {factor}ëŠ” {best_level[0]} ìˆ˜ì¤€ì—ì„œ ìµœì†Œê°’ ë‹¬ì„±"
                        )
        
        # ëª¨ë¸ ì í•©ë„ ê¸°ë°˜
        if response.name in analysis_results['regression']:
            reg_results = analysis_results['regression'][response.name]
            
            if 'best_model' in reg_results:
                r2 = reg_results[reg_results['best_model']]['r2']
                
                if r2 < 0.7:
                    recommendations.append(
                        f"âš ï¸ ëª¨ë¸ ì„¤ëª…ë ¥ì´ ë‚®ìŒ (RÂ² = {r2:.3f}). ì¶”ê°€ ì¸ìë‚˜ ë¹„ì„ í˜• í•­ ê³ ë ¤ í•„ìš”"
                    )
                
                if 'optimal_conditions' in reg_results:
                    opt_cond = reg_results['optimal_conditions']
                    recommendations.append(
                        f"ğŸ¯ ì˜ˆì¸¡ ìµœì  ì¡°ê±´: {opt_cond['predicted_value']:.3f} {response.unit}"
                    )
        
        # ê³µì •ëŠ¥ë ¥ ê¸°ë°˜
        if response.name in analysis_results['descriptive']:
            desc_stats = analysis_results['descriptive'][response.name]
            
            if 'target_deviation' in desc_stats:
                capability = desc_stats['target_deviation'].get('process_capability', {})
                
                if 'Cpk' in capability:
                    cpk = capability['Cpk']
                    
                    if cpk < 1.0:
                        recommendations.append(
                            f"âŒ ê³µì •ëŠ¥ë ¥ ë¶€ì¡± (Cpk = {cpk:.2f}). ë³€ë™ ê°ì†Œ í•„ìš”"
                        )
                    elif cpk < 1.33:
                        recommendations.append(
                            f"âš ï¸ ê³µì •ëŠ¥ë ¥ ê°œì„  í•„ìš” (Cpk = {cpk:.2f})"
                        )
                    else:
                        recommendations.append(
                            f"âœ… ìš°ìˆ˜í•œ ê³µì •ëŠ¥ë ¥ (Cpk = {cpk:.2f})"
                        )
        
        return recommendations

# ==================== ì‹œê°í™” ì—”ì§„ (í™•ì¥) ====================
class EnhancedVisualizationEngine:
    """í–¥ìƒëœ ì‹œê°í™” ì—”ì§„"""
    
    def __init__(self):
        self.color_palettes = {
            'default': px.colors.qualitative.Plotly,
            'sequential': px.colors.sequential.Viridis,
            'diverging': px.colors.diverging.RdBu,
            'polymer': ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'],
            'professional': ['#003f5c', '#58508d', '#bc5090', '#ff6361', '#ffa600']
        }
        
        self.plot_templates = {
            'clean': dict(
                layout=go.Layout(
                    font=dict(family="Arial", size=12),
                    plot_bgcolor='white',
                    paper_bgcolor='white',
                    margin=dict(l=60, r=60, t=80, b=60)
                )
            ),
            'dark': dict(
                layout=go.Layout(
                    font=dict(family="Arial", size=12, color='white'),
                    plot_bgcolor='#1e1e1e',
                    paper_bgcolor='#1e1e1e',
                    margin=dict(l=60, r=60, t=80, b=60)
                )
            )
        }
    
    def create_main_effects_plot(self, analysis_results: Dict[str, Any],
                                response_name: str) -> go.Figure:
        """ì£¼íš¨ê³¼ í”Œë¡¯ ìƒì„±"""
        if response_name not in analysis_results['inferential']:
            return go.Figure()
        
        main_effects = analysis_results['inferential'][response_name]['main_effects']
        
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        n_factors = len(main_effects)
        cols = min(3, n_factors)
        rows = (n_factors + cols - 1) // cols
        
        fig = make_subplots(
            rows=rows, cols=cols,
            subplot_titles=list(main_effects.keys()),
            vertical_spacing=0.15,
            horizontal_spacing=0.1
        )
        
        # ê° ì¸ìë³„ í”Œë¡¯
        for idx, (factor, data) in enumerate(main_effects.items()):
            row = idx // cols + 1
            col = idx % cols + 1
            
            levels = list(data['means'].keys())
            means = list(data['means'].values())
            
            # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
            std_err = np.std(means) / np.sqrt(len(means))
            ci = 1.96 * std_err
            
            fig.add_trace(
                go.Scatter(
                    x=levels,
                    y=means,
                    mode='lines+markers',
                    marker=dict(size=10, color='#1f77b4'),
                    line=dict(width=2),
                    error_y=dict(
                        type='constant',
                        value=ci,
                        visible=True
                    ),
                    name=factor,
                    showlegend=False
                ),
                row=row, col=col
            )
            
            # ìœ ì˜ì„± í‘œì‹œ
            if data['significant']:
                fig.add_annotation(
                    x=levels[len(levels)//2],
                    y=max(means) + ci,
                    text="*",
                    showarrow=False,
                    font=dict(size=20, color='red'),
                    row=row, col=col
                )
        
        fig.update_layout(
            title=f"ì£¼íš¨ê³¼ í”Œë¡¯: {response_name}",
            height=300 * rows,
            showlegend=False,
            template='plotly_white'
        )
        
        # Yì¶• ë ˆì´ë¸”
        fig.update_yaxes(title_text=response_name)
        
        return fig
    
    def create_interaction_plot(self, design: pd.DataFrame, results: pd.DataFrame,
                              factor1: str, factor2: str, response: str) -> go.Figure:
        """ìƒí˜¸ì‘ìš© í”Œë¡¯ ìƒì„±"""
        # ë°ì´í„° ì¤€ë¹„
        plot_data = design[[factor1, factor2]].copy()
        plot_data[response] = results[response]
        
        # í‰ê·  ê³„ì‚°
        interaction_means = plot_data.groupby([factor1, factor2])[response].agg(['mean', 'std', 'count'])
        
        fig = go.Figure()
        
        # factor2ì˜ ê° ìˆ˜ì¤€ë³„ë¡œ ì„  ê·¸ë¦¬ê¸°
        for level2 in plot_data[factor2].unique():
            data_subset = interaction_means.xs(level2, level=1)
            
            # ì‹ ë¢°êµ¬ê°„
            ci = 1.96 * data_subset['std'] / np.sqrt(data_subset['count'])
            
            fig.add_trace(go.Scatter(
                x=data_subset.index,
                y=data_subset['mean'],
                mode='lines+markers',
                name=f"{factor2}={level2}",
                error_y=dict(
                    type='data',
                    array=ci,
                    visible=True
                )
            ))
        
        fig.update_layout(
            title=f"ìƒí˜¸ì‘ìš© í”Œë¡¯: {factor1} Ã— {factor2}",
            xaxis_title=factor1,
            yaxis_title=response,
            hovermode='x unified',
            template='plotly_white'
        )
        
        return fig
    
    def create_response_surface_3d(self, model, factor1_range: Tuple[float, float],
                                 factor2_range: Tuple[float, float],
                                 factor_names: List[str],
                                 response_name: str,
                                 other_factors: Dict[str, float] = None) -> go.Figure:
        """3D ë°˜ì‘í‘œë©´ í”Œë¡¯"""
        # ê·¸ë¦¬ë“œ ìƒì„±
        n_points = 50
        x = np.linspace(factor1_range[0], factor1_range[1], n_points)
        y = np.linspace(factor2_range[0], factor2_range[1], n_points)
        X, Y = np.meshgrid(x, y)
        
        # ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        prediction_points = []
        
        for i in range(n_points):
            for j in range(n_points):
                point = other_factors.copy() if other_factors else {}
                point[factor_names[0]] = X[i, j]
                point[factor_names[1]] = Y[i, j]
                prediction_points.append(point)
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        pred_df = pd.DataFrame(prediction_points)
        
        # ì˜ˆì¸¡
        Z = model.predict(pred_df).reshape(n_points, n_points)
        
        # 3D Surface plot
        fig = go.Figure(data=[
            go.Surface(
                x=x,
                y=y,
                z=Z,
                colorscale='Viridis',
                contours=dict(
                    z=dict(show=True, usecolormap=True, highlightcolor="limegreen", project_z=True)
                )
            )
        ])
        
        # ë ˆì´ì•„ì›ƒ
        fig.update_layout(
            title=f"ë°˜ì‘í‘œë©´: {response_name}",
            scene=dict(
                xaxis_title=factor_names[0],
                yaxis_title=factor_names[1],
                zaxis_title=response_name,
                camera=dict(
                    eye=dict(x=1.5, y=1.5, z=1.2)
                )
            ),
            width=800,
            height=600
        )
        
        return fig
    
    def create_contour_plot(self, model, factor1_range: Tuple[float, float],
                          factor2_range: Tuple[float, float],
                          factor_names: List[str],
                          response_name: str,
                          other_factors: Dict[str, float] = None,
                          show_optimum: bool = True) -> go.Figure:
        """ë“±ê³ ì„  í”Œë¡¯"""
        # ê·¸ë¦¬ë“œ ìƒì„±
        n_points = 100
        x = np.linspace(factor1_range[0], factor1_range[1], n_points)
        y = np.linspace(factor2_range[0], factor2_range[1], n_points)
        X, Y = np.meshgrid(x, y)
        
        # ì˜ˆì¸¡
        prediction_points = []
        for i in range(n_points):
            for j in range(n_points):
                point = other_factors.copy() if other_factors else {}
                point[factor_names[0]] = X[i, j]
                point[factor_names[1]] = Y[i, j]
                prediction_points.append(point)
        
        pred_df = pd.DataFrame(prediction_points)
        Z = model.predict(pred_df).reshape(n_points, n_points)
        
        # Contour plot
        fig = go.Figure()
        
        # Filled contour
        fig.add_trace(go.Contour(
            x=x,
            y=y,
            z=Z,
            colorscale='Viridis',
            contours=dict(
                coloring='heatmap',
                showlabels=True,
                labelfont=dict(size=12, color='white')
            ),
            colorbar=dict(title=response_name)
        ))
        
        # ìµœì ì  í‘œì‹œ
        if show_optimum:
            # ê°„ë‹¨í•œ ìµœì ì  ì°¾ê¸° (ê·¸ë¦¬ë“œ ì„œì¹˜)
            opt_idx = np.unravel_index(Z.argmax(), Z.shape)
            opt_x = X[opt_idx]
            opt_y = Y[opt_idx]
            opt_z = Z[opt_idx]
            
            fig.add_trace(go.Scatter(
                x=[opt_x],
                y=[opt_y],
                mode='markers',
                marker=dict(
                    size=15,
                    color='red',
                    symbol='star',
                    line=dict(color='white', width=2)
                ),
                name=f'ìµœì ì  ({opt_z:.2f})',
                showlegend=True
            ))
        
        fig.update_layout(
            title=f"ë“±ê³ ì„  í”Œë¡¯: {response_name}",
            xaxis_title=factor_names[0],
            yaxis_title=factor_names[1],
            template='plotly_white',
            width=700,
            height=600
        )
        
        return fig
    
    def create_pareto_chart(self, data: Dict[str, float], title: str = "Pareto Chart") -> go.Figure:
        """íŒŒë ˆí†  ì°¨íŠ¸"""
        # ë°ì´í„° ì •ë ¬
        sorted_items = sorted(data.items(), key=lambda x: abs(x[1]), reverse=True)
        
        categories = [item[0] for item in sorted_items]
        values = [abs(item[1]) for item in sorted_items]
        
        # ëˆ„ì  ë¹„ìœ¨ ê³„ì‚°
        total = sum(values)
        cumulative = []
        cum_sum = 0
        
        for val in values:
            cum_sum += val
            cumulative.append(cum_sum / total * 100)
        
        # ê·¸ë˜í”„ ìƒì„±
        fig = go.Figure()
        
        # ë§‰ëŒ€ ê·¸ë˜í”„
        fig.add_trace(go.Bar(
            x=categories,
            y=values,
            name='íš¨ê³¼',
            marker_color='lightblue',
            yaxis='y'
        ))
        
        # ëˆ„ì  ì„  ê·¸ë˜í”„
        fig.add_trace(go.Scatter(
            x=categories,
            y=cumulative,
            name='ëˆ„ì  %',
            mode='lines+markers',
            line=dict(color='red', width=2),
            marker=dict(size=8),
            yaxis='y2'
        ))
        
        # 80% ì„ 
        fig.add_hline(y=80, line_dash="dash", line_color="gray", 
                     annotation_text="80%", yref='y2')
        
        # ë ˆì´ì•„ì›ƒ
        fig.update_layout(
            title=title,
            xaxis=dict(title='ìš”ì¸'),
            yaxis=dict(title='íš¨ê³¼ í¬ê¸°', side='left'),
            yaxis2=dict(title='ëˆ„ì  ë¹„ìœ¨ (%)', side='right', overlaying='y', range=[0, 100]),
            hovermode='x unified',
            template='plotly_white'
        )
        
        return fig
    
    def create_residual_plots(self, actual: np.ndarray, predicted: np.ndarray,
                            feature_names: List[str] = None) -> go.Figure:
        """ì”ì°¨ ì§„ë‹¨ í”Œë¡¯"""
        residuals = actual - predicted
        
        # 4ê°œ ì„œë¸Œí”Œë¡¯
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('ì”ì°¨ vs ì í•©ê°’', 'ì •ê·œ Q-Q í”Œë¡¯', 
                          'ì²™ë„-ìœ„ì¹˜ í”Œë¡¯', 'ì”ì°¨ vs ë ˆë²„ë¦¬ì§€'),
            vertical_spacing=0.15,
            horizontal_spacing=0.1
        )
        
        # 1. ì”ì°¨ vs ì í•©ê°’
        fig.add_trace(
            go.Scatter(
                x=predicted,
                y=residuals,
                mode='markers',
                marker=dict(color='blue', size=6),
                showlegend=False
            ),
            row=1, col=1
        )
        fig.add_hline(y=0, line_dash="dash", line_color="red", row=1, col=1)
        
        # 2. Q-Q í”Œë¡¯
        qq_data = stats.probplot(residuals, dist="norm")
        fig.add_trace(
            go.Scatter(
                x=qq_data[0][0],
                y=qq_data[0][1],
                mode='markers',
                marker=dict(color='blue', size=6),
                showlegend=False
            ),
            row=1, col=2
        )
        
        # Q-Q ë¼ì¸
        fig.add_trace(
            go.Scatter(
                x=qq_data[0][0],
                y=qq_data[1][1] + qq_data[1][0] * qq_data[0][0],
                mode='lines',
                line=dict(color='red', dash='dash'),
                showlegend=False
            ),
            row=1, col=2
        )
        
        # 3. ì²™ë„-ìœ„ì¹˜ í”Œë¡¯
        standardized_residuals = residuals / np.std(residuals)
        sqrt_abs_residuals = np.sqrt(np.abs(standardized_residuals))
        
        fig.add_trace(
            go.Scatter(
                x=predicted,
                y=sqrt_abs_residuals,
                mode='markers',
                marker=dict(color='blue', size=6),
                showlegend=False
            ),
            row=2, col=1
        )
        
        # í‰í™œì„  ì¶”ê°€ (LOWESS)
        from statsmodels.nonparametric.smoothers_lowess import lowess
        smoothed = lowess(sqrt_abs_residuals, predicted, frac=0.6)
        
        fig.add_trace(
            go.Scatter(
                x=smoothed[:, 0],
                y=smoothed[:, 1],
                mode='lines',
                line=dict(color='red', width=2),
                showlegend=False
            ),
            row=2, col=1
        )
        
        # 4. ì”ì°¨ vs ë ˆë²„ë¦¬ì§€
        # ê°„ë‹¨í•œ ë ˆë²„ë¦¬ì§€ ê³„ì‚° (ì‹¤ì œë¡œëŠ” hat matrix í•„ìš”)
        n = len(residuals)
        leverage = np.ones(n) / n  # ë‹¨ìˆœí™”
        
        fig.add_trace(
            go.Scatter(
                x=leverage,
                y=standardized_residuals,
                mode='markers',
                marker=dict(color='blue', size=6),
                showlegend=False
            ),
            row=2, col=2
        )
        
        # Cook's distance ì„ê³„ì„ 
        fig.add_hline(y=2, line_dash="dash", line_color="red", row=2, col=2)
        fig.add_hline(y=-2, line_dash="dash", line_color="red", row=2, col=2)
        
        # ì¶• ë ˆì´ë¸”
        fig.update_xaxes(title_text="ì í•©ê°’", row=1, col=1)
        fig.update_yaxes(title_text="ì”ì°¨", row=1, col=1)
        
        fig.update_xaxes(title_text="ì´ë¡ ì  ë¶„ìœ„ìˆ˜", row=1, col=2)
        fig.update_yaxes(title_text="í‘œë³¸ ë¶„ìœ„ìˆ˜", row=1, col=2)
        
        fig.update_xaxes(title_text="ì í•©ê°’", row=2, col=1)
        fig.update_yaxes(title_text="âˆš|í‘œì¤€í™” ì”ì°¨|", row=2, col=1)
        
        fig.update_xaxes(title_text="ë ˆë²„ë¦¬ì§€", row=2, col=2)
        fig.update_yaxes(title_text="í‘œì¤€í™” ì”ì°¨", row=2, col=2)
        
        fig.update_layout(
            title="ì”ì°¨ ì§„ë‹¨ í”Œë¡¯",
            height=800,
            showlegend=False,
            template='plotly_white'
        )
        
        return fig
    
    def create_optimization_history_plot(self, optimization_results: Dict[str, Any]) -> go.Figure:
        """ìµœì í™” ì´ë ¥ í”Œë¡¯"""
        if 'optimization_history' not in optimization_results:
            return go.Figure()
        
        history = optimization_results['optimization_history']
        
        fig = go.Figure()
        
        # ëª©ì  í•¨ìˆ˜ ê°’ ì¶”ì´
        fig.add_trace(go.Scatter(
            x=list(range(len(history))),
            y=[h['value'] for h in history],
            mode='lines+markers',
            name='ëª©ì  í•¨ìˆ˜ ê°’',
            line=dict(color='blue', width=2)
        ))
        
        # ìµœì ê°’ ì¶”ì´
        best_values = []
        current_best = float('inf')
        
        for h in history:
            current_best = min(current_best, h['value'])
            best_values.append(current_best)
        
        fig.add_trace(go.Scatter(
            x=list(range(len(history))),
            y=best_values,
            mode='lines',
            name='ìµœì ê°’',
            line=dict(color='red', width=2, dash='dash')
        ))
        
        fig.update_layout(
            title="ìµœì í™” ìˆ˜ë ´ ì´ë ¥",
            xaxis_title="ë°˜ë³µ íšŸìˆ˜",
            yaxis_title="ëª©ì  í•¨ìˆ˜ ê°’",
            hovermode='x unified',
            template='plotly_white'
        )
        
        return fig
    
    def create_3d_molecule_view(self, smiles: str = None, sdf_file: str = None) -> str:
        """3D ë¶„ì ì‹œê°í™”"""
        if not PY3DMOL_AVAILABLE:
            return "<p>3D ë¶„ì ì‹œê°í™”ë¥¼ ìœ„í•´ py3Dmol ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.</p>"
        
        if not RDKIT_AVAILABLE:
            return "<p>ë¶„ì ì²˜ë¦¬ë¥¼ ìœ„í•´ RDKit ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.</p>"
        
        # ë¶„ì ìƒì„±
        if smiles:
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                return "<p>ìœ íš¨í•˜ì§€ ì•Šì€ SMILES ë¬¸ìì—´ì…ë‹ˆë‹¤.</p>"
            
            # 3D ì¢Œí‘œ ìƒì„±
            mol = Chem.AddHs(mol)
            AllChem.EmbedMolecule(mol, randomSeed=42)
            AllChem.MMFFOptimizeMolecule(mol)
            
            # SDF í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            mol_block = Chem.MolToMolBlock(mol)
        
        elif sdf_file:
            with open(sdf_file, 'r') as f:
                mol_block = f.read()
        
        else:
            return "<p>SMILES ë˜ëŠ” SDF íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.</p>"
        
        # 3D ë·°ì–´ ìƒì„±
        viewer = py3Dmol.view(width=800, height=600)
        viewer.addModel(mol_block, 'sdf')
        viewer.setStyle({'stick': {}})
        viewer.setBackgroundColor('white')
        viewer.zoomTo()
        
        return viewer.render()

# Polymer-doe-platform - Part 6
# ==================== AI ì—”ì§„ í†µí•© ì‹œìŠ¤í…œ (ì´ ì •ë¦¬) ====================
# ==================== AI ì—”ì§„ ì‚¬ìš©ëŸ‰ ì¶”ì ê¸° ====================
class UsageTracker:
    """API ì‚¬ìš©ëŸ‰ ì¶”ì  í´ë˜ìŠ¤"""
    
    def __init__(self, name: str):
        self.name = name
        self.usage_count = 0
        self.usage_history = []
        self.daily_limit = 1000
        self.last_reset = datetime.now()
    
    def track_usage(self, tokens: int = 1):
        """ì‚¬ìš©ëŸ‰ ì¶”ì """
        self.usage_count += tokens
        self.usage_history.append({
            'timestamp': datetime.now(),
            'tokens': tokens
        })
    
    def reset_daily(self):
        """ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹"""
        now = datetime.now()
        if (now - self.last_reset).days >= 1:
            self.usage_count = 0
            self.last_reset = now
            self.usage_history = []
    
    def is_within_limit(self) -> bool:
        """ì‚¬ìš©ëŸ‰ í•œë„ í™•ì¸"""
        self.reset_daily()
        return self.usage_count < self.daily_limit

# ==================== AI ì‘ë‹µ ìºì‹œ ====================
class AIResponseCache:
    """AI ì‘ë‹µ ìºì‹± í´ë˜ìŠ¤"""
    
    def __init__(self, name: str):
        self.name = name
        self.cache = {}
        self.max_size = 100
    
    def get(self, key: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°"""
        if key in self.cache:
            return self.cache[key]['response']
        return None
    
    def set(self, key: str, response: str):
        """ìºì‹œì— ì‘ë‹µ ì €ì¥"""
        if len(self.cache) >= self.max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]
        
        self.cache[key] = {
            'response': response,
            'timestamp': datetime.now()
        }

# ==================== ì†ë„ ì œí•œê¸° ====================
class RateLimiter:
    """API í˜¸ì¶œ ì†ë„ ì œí•œ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, calls_per_minute: int = 10):
        self.name = name
        self.calls_per_minute = calls_per_minute
        self.call_times = deque()
    
    async def acquire(self):
        """í˜¸ì¶œ í—ˆê°€ ëŒ€ê¸°"""
        now = datetime.now()
        
        # 1ë¶„ ì´ìƒ ì§€ë‚œ í˜¸ì¶œ ê¸°ë¡ ì œê±°
        while self.call_times and (now - self.call_times[0]).seconds >= 60:
            self.call_times.popleft()
        
        # ì œí•œì— ë„ë‹¬í•œ ê²½ìš° ëŒ€ê¸°
        if len(self.call_times) >= self.calls_per_minute:
            wait_time = 60 - (now - self.call_times[0]).seconds
            if wait_time > 0:
                await asyncio.sleep(wait_time)
                return await self.acquire()
        
        # í˜¸ì¶œ ê¸°ë¡
        self.call_times.append(now)

# ==================== AI ì—”ì§„ í†µí•© ì‹œìŠ¤í…œ ====================
class BaseAIEngine:
    """ëª¨ë“  AI ì—”ì§„ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, api_key_id: str):
        self.name = name
        self.api_key_id = api_key_id
        self.api_key = None
        self.client = None
        self.available = False
        self.rate_limiter = None
        self.usage_tracker = UsageTracker(name)
        self.cache = AIResponseCache(name)
        
    def initialize(self):
        """API í‚¤ í™•ì¸ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.api_key = api_key_manager.get_key(self.api_key_id)
        if not self.api_key:
            logger.warning(f"{self.name} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.available = False
            return False
            
        try:
            self._initialize_client()
            self.available = True
            self.rate_limiter = RateLimiter(self.name)
            logger.info(f"{self.name} ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            return True
        except Exception as e:
            logger.error(f"{self.name} ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.available = False
            return False
    
    def _initialize_client(self):
        """ê° AI ì—”ì§„ë³„ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    async def generate_response(self, 
                               prompt: str, 
                               context: Dict[str, Any] = None,
                               temperature: float = 0.7,
                               max_tokens: int = 1000,
                               user_level: UserLevel = UserLevel.BEGINNER) -> Dict[str, Any]:
        """AI ì‘ë‹µ ìƒì„±"""
        if not self.available:
            return {
                'status': 'error',
                'message': f'{self.name} ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'response': None
            }
        
        # ìºì‹œ í™•ì¸
        cache_key = self.cache.generate_key(prompt, context)
        cached_response = self.cache.get(cache_key)
        if cached_response:
            return cached_response
        
        # Rate limiting
        if not await self.rate_limiter.check_rate():
            return {
                'status': 'rate_limited',
                'message': f'{self.name} API í˜¸ì¶œ í•œë„ ì´ˆê³¼',
                'response': None
            }
        
        try:
            # ì‚¬ìš©ì ë ˆë²¨ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
            adjusted_prompt = self._adjust_prompt_for_level(prompt, user_level)
            
            # API í˜¸ì¶œ
            response = await self._call_api(
                adjusted_prompt, 
                context, 
                temperature, 
                max_tokens
            )
            
            # ì‚¬ìš©ëŸ‰ ì¶”ì 
            self.usage_tracker.track_usage(
                prompt_tokens=response.get('prompt_tokens', 0),
                completion_tokens=response.get('completion_tokens', 0)
            )
            
            # ê²°ê³¼ ìºì‹±
            result = {
                'status': 'success',
                'response': response['content'],
                'metadata': {
                    'engine': self.name,
                    'tokens_used': response.get('total_tokens', 0),
                    'temperature': temperature,
                    'user_level': user_level.name
                }
            }
            
            self.cache.set(cache_key, result)
            return result
            
        except Exception as e:
            logger.error(f"{self.name} API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {
                'status': 'error',
                'message': str(e),
                'response': None
            }
    
    def _adjust_prompt_for_level(self, prompt: str, user_level: UserLevel) -> str:
        """ì‚¬ìš©ì ë ˆë²¨ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •"""
        level_adjustments = {
            UserLevel.BEGINNER: {
                'prefix': "ì´ˆë³´ìë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ì „ë¬¸ ìš©ì–´ëŠ” í’€ì–´ì„œ ì„¤ëª…í•˜ê³ , ë‹¨ê³„ë³„ë¡œ ìì„¸íˆ ì•ˆë‚´í•´ì£¼ì„¸ìš”.\n\n",
                'suffix': "\n\nì„¤ëª…í•  ë•Œ ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”:\n1. ì™œ ì´ë ‡ê²Œ í•˜ëŠ”ì§€ ì´ìœ \n2. ê° ë‹¨ê³„ì˜ ì¤‘ìš”ì„±\n3. ì£¼ì˜í•  ì \n4. ì˜ˆìƒë˜ëŠ” ê²°ê³¼"
            },
            UserLevel.INTERMEDIATE: {
                'prefix': "ì¤‘ê¸‰ ì‚¬ìš©ìë¥¼ ìœ„í•´ í•µì‹¬ ê°œë…ê³¼ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n",
                'suffix': "\n\nì—¬ëŸ¬ ì˜µì…˜ì´ ìˆë‹¤ë©´ ì¥ë‹¨ì ê³¼ í•¨ê»˜ ì œì‹œí•´ì£¼ì„¸ìš”."
            },
            UserLevel.ADVANCED: {
                'prefix': "ê³ ê¸‰ ì‚¬ìš©ìë¥¼ ìœ„í•œ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n\n",
                'suffix': ""
            },
            UserLevel.EXPERT: {
                'prefix': "",
                'suffix': ""
            }
        }
        
        adjustment = level_adjustments.get(user_level, level_adjustments[UserLevel.BEGINNER])
        return adjustment['prefix'] + prompt + adjustment['suffix']
    
    async def _call_api(self, prompt: str, context: Dict, temperature: float, max_tokens: int) -> Dict:
        """ì‹¤ì œ API í˜¸ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    def get_capabilities(self) -> Dict[str, Any]:
        """AI ì—”ì§„ì˜ ê¸°ëŠ¥ ì •ë³´ ë°˜í™˜"""
        return {
            'name': self.name,
            'available': self.available,
            'capabilities': self._get_specific_capabilities(),
            'usage': self.usage_tracker.get_usage_stats()
        }
    
    def _get_specific_capabilities(self) -> List[str]:
        """ê° ì—”ì§„ë³„ íŠ¹í™” ê¸°ëŠ¥ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        return []

class GeminiEngine(BaseAIEngine):
    """Google Gemini AI ì—”ì§„"""
    
    def __init__(self):
        super().__init__("Gemini", "GEMINI_API_KEY")
        self.model_name = "gemini-2.0-flash-exp"
        
    def _initialize_client(self):
        """Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        import google.generativeai as genai
        genai.configure(api_key=self.api_key)
        self.client = genai.GenerativeModel(self.model_name)
        
    async def _call_api(self, prompt: str, context: Dict, temperature: float, max_tokens: int) -> Dict:
        """Gemini API í˜¸ì¶œ"""
        # ì»¨í…ìŠ¤íŠ¸ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        full_prompt = self._build_full_prompt(prompt, context)
        
        generation_config = {
            'temperature': temperature,
            'max_output_tokens': max_tokens,
            'top_p': 0.95,
            'top_k': 40
        }
        
        response = await asyncio.to_thread(
            self.client.generate_content,
            full_prompt,
            generation_config=generation_config
        )
        
        return {
            'content': response.text,
            'prompt_tokens': len(prompt.split()),  # ê·¼ì‚¬ì¹˜
            'completion_tokens': len(response.text.split()),
            'total_tokens': len(prompt.split()) + len(response.text.split())
        }
    
    def _build_full_prompt(self, prompt: str, context: Dict) -> str:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì „ì²´ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if not context:
            return prompt
            
        context_str = "### ì»¨í…ìŠ¤íŠ¸ ì •ë³´:\n"
        if 'polymer_type' in context:
            context_str += f"- ê³ ë¶„ì ì¢…ë¥˜: {context['polymer_type']}\n"
        if 'experiment_type' in context:
            context_str += f"- ì‹¤í—˜ ìœ í˜•: {context['experiment_type']}\n"
        if 'previous_results' in context:
            context_str += f"- ì´ì „ ì‹¤í—˜ ê²°ê³¼: {context['previous_results']}\n"
            
        return context_str + "\n### ì§ˆë¬¸:\n" + prompt
    
    def _get_specific_capabilities(self) -> List[str]:
        return [
            "ê³ ë¶„ì ê³¼í•™ ì „ë¬¸ ì§€ì‹",
            "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬",
            "ì‹¤í—˜ ì„¤ê³„ ì œì•ˆ",
            "ë°ì´í„° ë¶„ì„ ë° í•´ì„",
            "ì´ë¯¸ì§€ ë¶„ì„ (ë¶„ì êµ¬ì¡°, ê·¸ë˜í”„)",
            "ì½”ë“œ ìƒì„± ë° ë””ë²„ê¹…"
        ]

class GrokEngine(BaseAIEngine):
    """xAI Grok ì—”ì§„"""
    
    def __init__(self):
        super().__init__("Grok", "GROK_API_KEY")
        self.model_name = "grok-3-mini"
        
    def _initialize_client(self):
        """Grok í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        from openai import OpenAI
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.x.ai/v1"
        )
        
    async def _call_api(self, prompt: str, context: Dict, temperature: float, max_tokens: int) -> Dict:
        """Grok API í˜¸ì¶œ"""
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ê³ ë¶„ì ê³¼í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ ì‹¤í—˜ ê¸°ë²•ì— ëŒ€í•œ ê¹Šì€ ì´í•´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤."},
            {"role": "user", "content": self._build_full_prompt(prompt, context)}
        ]
        
        response = await asyncio.to_thread(
            self.client.chat.completions.create,
            model=self.model_name,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        return {
            'content': response.choices[0].message.content,
            'prompt_tokens': response.usage.prompt_tokens,
            'completion_tokens': response.usage.completion_tokens,
            'total_tokens': response.usage.total_tokens
        }
    
    def _get_specific_capabilities(self) -> List[str]:
        return [
            "ìµœì‹  ì—°êµ¬ ë™í–¥ íŒŒì•…",
            "ì°½ì˜ì  ì‹¤í—˜ ì•„ì´ë””ì–´",
            "ì‹¤ì‹œê°„ ì •ë³´ ì ‘ê·¼",
            "ë¹ ë¥¸ ì‘ë‹µ ì†ë„"
        ]

class SambaNovaEngine(BaseAIEngine):
    """SambaNova AI ì—”ì§„"""
    
    def __init__(self):
        super().__init__("SambaNova", "SAMBANOVA_API_KEY")
        self.model_name = "Meta-Llama-3.1-405B-Instruct"
        
    def _initialize_client(self):
        """SambaNova í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        from openai import OpenAI
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.sambanova.ai/v1"
        )
        
    async def _call_api(self, prompt: str, context: Dict, temperature: float, max_tokens: int) -> Dict:
        """SambaNova API í˜¸ì¶œ"""
        messages = [
            {"role": "system", "content": "You are a polymer science expert with deep knowledge in experimental design and data analysis."},
            {"role": "user", "content": self._build_full_prompt(prompt, context)}
        ]
        
        response = await asyncio.to_thread(
            self.client.chat.completions.create,
            model=self.model_name,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        return {
            'content': response.choices[0].message.content,
            'prompt_tokens': response.usage.prompt_tokens,
            'completion_tokens': response.usage.completion_tokens,
            'total_tokens': response.usage.total_tokens
        }
    
    def _get_specific_capabilities(self) -> List[str]:
        return [
            "ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬",
            "ë³µì¡í•œ íŒ¨í„´ ì¸ì‹",
            "ì •í™•í•œ í†µê³„ ë¶„ì„",
            "ì•ˆì •ì ì¸ ì‘ë‹µ"
        ]

class DeepSeekEngine(BaseAIEngine):
    """DeepSeek AI ì—”ì§„"""
    
    def __init__(self):
        super().__init__("DeepSeek", "DEEPSEEK_API_KEY")
        self.model_name = "deepseek-chat"
        
    def _initialize_client(self):
        """DeepSeek í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        from openai import OpenAI
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.deepseek.com/v1"
        )
        
    async def _call_api(self, prompt: str, context: Dict, temperature: float, max_tokens: int) -> Dict:
        """DeepSeek API í˜¸ì¶œ"""
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ê³ ë¶„ì í™”í•™ ë° ì¬ë£Œê³¼í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìˆ˜ì‹ ê³„ì‚°, í™”í•™ êµ¬ì¡° í•´ì„, ì½”ë“œ ìƒì„±ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."},
            {"role": "user", "content": self._build_full_prompt(prompt, context)}
        ]
        
        response = await asyncio.to_thread(
            self.client.chat.completions.create,
            model=self.model_name,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        return {
            'content': response.choices[0].message.content,
            'prompt_tokens': response.usage.prompt_tokens,
            'completion_tokens': response.usage.completion_tokens,
            'total_tokens': response.usage.total_tokens
        }
    
    def _get_specific_capabilities(self) -> List[str]:
        return [
            "ìˆ˜ì‹ ë° ê³„ì‚° ì „ë¬¸",
            "í™”í•™ êµ¬ì¡° í•´ì„",
            "ì½”ë“œ ìƒì„± ë° ìµœì í™”",
            "ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„"
        ]

class GroqEngine(BaseAIEngine):
    """Groq ì´ˆê³ ì† AI ì—”ì§„"""
    
    def __init__(self):
        super().__init__("Groq", "GROQ_API_KEY")
        self.model_name = "llama3-70b-8192"
        
    def _initialize_client(self):
        """Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        from groq import Groq
        self.client = Groq(api_key=self.api_key)
        
    async def _call_api(self, prompt: str, context: Dict, temperature: float, max_tokens: int) -> Dict:
        """Groq API í˜¸ì¶œ"""
        messages = [
            {"role": "system", "content": "You are a polymer science assistant focused on providing quick, accurate responses."},
            {"role": "user", "content": self._build_full_prompt(prompt, context)}
        ]
        
        response = await asyncio.to_thread(
            self.client.chat.completions.create,
            model=self.model_name,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        return {
            'content': response.choices[0].message.content,
            'prompt_tokens': response.usage.prompt_tokens,
            'completion_tokens': response.usage.completion_tokens,
            'total_tokens': response.usage.total_tokens
        }
    
    def _get_specific_capabilities(self) -> List[str]:
        return [
            "ì´ˆê³ ì† ì‘ë‹µ (ms ë‹¨ìœ„)",
            "ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©",
            "ëŒ€ëŸ‰ ìš”ì²­ ì²˜ë¦¬",
            "ë‚®ì€ ì§€ì—°ì‹œê°„"
        ]

class HuggingFaceEngine(BaseAIEngine):
    """HuggingFace íŠ¹í™” ëª¨ë¸ ì—”ì§„"""
    
    def __init__(self):
        super().__init__("HuggingFace", "HUGGINGFACE_API_KEY")
        self.specialized_models = {
            'chemistry': 'laituan245/molt5-large-smiles2caption',
            'materials': 'oliverguhr/materials-property-predictor',
            'polymer': 'bert-base-uncased'  # íŠ¹í™” ëª¨ë¸ë¡œ êµì²´ ê°€ëŠ¥
        }
        
    def _initialize_client(self):
        """HuggingFace í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        from huggingface_hub import InferenceClient
        self.client = InferenceClient(token=self.api_key)
        
    async def _call_api(self, prompt: str, context: Dict, temperature: float, max_tokens: int) -> Dict:
        """HuggingFace API í˜¸ì¶œ"""
        # ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
        task_type = context.get('task_type', 'general')
        model_id = self.specialized_models.get(task_type, 'bert-base-uncased')
        
        try:
            if task_type == 'chemistry':
                # í™”í•™ êµ¬ì¡° ì„¤ëª…
                response = await asyncio.to_thread(
                    self.client.text_generation,
                    prompt,
                    model=model_id,
                    max_new_tokens=max_tokens,
                    temperature=temperature
                )
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„±
                response = await asyncio.to_thread(
                    self.client.text_generation,
                    prompt,
                    max_new_tokens=max_tokens,
                    temperature=temperature
                )
                
            return {
                'content': response,
                'prompt_tokens': len(prompt.split()),
                'completion_tokens': len(response.split()),
                'total_tokens': len(prompt.split()) + len(response.split())
            }
        except Exception as e:
            logger.error(f"HuggingFace API ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _get_specific_capabilities(self) -> List[str]:
        return [
            "í™”í•™ êµ¬ì¡° ì „ë¬¸ ëª¨ë¸",
            "ì¬ë£Œ íŠ¹ì„± ì˜ˆì¸¡",
            "íŠ¹í™” ëª¨ë¸ í™œìš©",
            "ë¬´ë£Œ í‹°ì–´ ì œê³µ"
        ]

# ==================== AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ====================
class AIOrchestrator:
    """ë‹¤ì¤‘ AI ì—”ì§„ ì¡°ì • ë° í•©ì˜ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.engines = {
            'gemini': GeminiEngine(),
            'grok': GrokEngine(),
            'sambanova': SambaNovaEngine(),
            'deepseek': DeepSeekEngine(),
            'groq': GroqEngine(),
            'huggingface': HuggingFaceEngine()
        }
        self.initialized = False
        self.consensus_threshold = 0.7  # í•©ì˜ ì„ê³„ê°’
        self.learning_system = AILearningSystem()
        
    async def initialize(self):
        """ëª¨ë“  AI ì—”ì§„ ì´ˆê¸°í™”"""
        logger.info("AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™” ì‹œì‘...")
        
        # ë³‘ë ¬ë¡œ ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™”
        init_tasks = []
        for engine_name, engine in self.engines.items():
            init_tasks.append(self._init_engine(engine_name, engine))
            
        results = await asyncio.gather(*init_tasks)
        
        # ìµœì†Œ 3ê°œ ì´ìƒì˜ ì—”ì§„ì´ í™œì„±í™”ë˜ì–´ì•¼ í•¨
        active_count = sum(1 for r in results if r)
        if active_count >= 3:
            self.initialized = True
            logger.info(f"AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™” ì™„ë£Œ ({active_count}/{len(self.engines)} ì—”ì§„ í™œì„±)")
        else:
            logger.error(f"AI ì—”ì§„ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ ({active_count}/{len(self.engines)})")
            
    async def _init_engine(self, name: str, engine: BaseAIEngine) -> bool:
        """ê°œë³„ ì—”ì§„ ì´ˆê¸°í™”"""
        try:
            return await asyncio.to_thread(engine.initialize)
        except Exception as e:
            logger.error(f"{name} ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def query_single(self, 
                          engine_name: str, 
                          prompt: str,
                          context: Dict[str, Any] = None,
                          **kwargs) -> Dict[str, Any]:
        """ë‹¨ì¼ AI ì—”ì§„ ì§ˆì˜"""
        if not self.initialized:
            return {'status': 'error', 'message': 'AI ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
            
        engine = self.engines.get(engine_name)
        if not engine or not engine.available:
            return {'status': 'error', 'message': f'{engine_name} ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
            
        return await engine.generate_response(prompt, context, **kwargs)
    
    async def query_multiple(self,
                            prompt: str,
                            context: Dict[str, Any] = None,
                            engines: List[str] = None,
                            strategy: str = 'consensus',
                            **kwargs) -> Dict[str, Any]:
        """ë‹¤ì¤‘ AI ì—”ì§„ ì§ˆì˜ ë° ê²°ê³¼ í†µí•©"""
        if not self.initialized:
            return {'status': 'error', 'message': 'AI ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
            
        # ì‚¬ìš©í•  ì—”ì§„ ì„ íƒ
        if engines:
            active_engines = {k: v for k, v in self.engines.items() 
                            if k in engines and v.available}
        else:
            active_engines = {k: v for k, v in self.engines.items() 
                            if v.available}
            
        if len(active_engines) < 2:
            return {'status': 'error', 'message': 'ìµœì†Œ 2ê°œ ì´ìƒì˜ AI ì—”ì§„ì´ í•„ìš”í•©ë‹ˆë‹¤.'}
            
        # ë³‘ë ¬ë¡œ ëª¨ë“  ì—”ì§„ì— ì§ˆì˜
        query_tasks = []
        for engine_name, engine in active_engines.items():
            query_tasks.append(self._query_with_metadata(
                engine_name, engine, prompt, context, **kwargs
            ))
            
        responses = await asyncio.gather(*query_tasks, return_exceptions=True)
        
        # ì˜¤ë¥˜ í•„í„°ë§
        valid_responses = []
        for resp in responses:
            if isinstance(resp, dict) and resp.get('status') == 'success':
                valid_responses.append(resp)
                
        if not valid_responses:
            return {'status': 'error', 'message': 'ëª¨ë“  AI ì—”ì§„ì—ì„œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'}
            
        # ì „ëµì— ë”°ë¥¸ ê²°ê³¼ í†µí•©
        if strategy == 'consensus':
            result = await self._build_consensus(valid_responses, prompt)
        elif strategy == 'best':
            result = await self._select_best(valid_responses, context)
        elif strategy == 'ensemble':
            result = await self._ensemble_responses(valid_responses)
        else:
            result = valid_responses[0]  # ê¸°ë³¸ê°’: ì²« ë²ˆì§¸ ì‘ë‹µ
            
        # í•™ìŠµ ì‹œìŠ¤í…œì— í”¼ë“œë°±
        await self.learning_system.record_interaction(
            prompt=prompt,
            context=context,
            responses=valid_responses,
            final_result=result
        )
            
        return result
    
    async def _query_with_metadata(self, 
                                  engine_name: str, 
                                  engine: BaseAIEngine,
                                  prompt: str, 
                                  context: Dict,
                                  **kwargs) -> Dict:
        """ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì—”ì§„ ì§ˆì˜"""
        try:
            response = await engine.generate_response(prompt, context, **kwargs)
            response['engine_name'] = engine_name
            response['timestamp'] = datetime.now()
            return response
        except Exception as e:
            logger.error(f"{engine_name} ì§ˆì˜ ì˜¤ë¥˜: {str(e)}")
            return {
                'status': 'error',
                'engine_name': engine_name,
                'message': str(e)
            }

# Polymer-doe-platform - Part 7
# ==================== AI í•©ì˜ ì‹œìŠ¤í…œ (ê³„ì†) ====================
    async def _build_consensus(self, responses: List[Dict], prompt: str) -> Dict[str, Any]:
        """AI ì‘ë‹µë“¤ë¡œë¶€í„° í•©ì˜ ë„ì¶œ"""
        logger.info("AI í•©ì˜ ë¹Œë“œ ì‹œì‘...")
        
        # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
        contents = [r['response'] for r in responses]
        engines = [r['engine_name'] for r in responses]
        
        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        similarity_matrix = await self._calculate_similarity_matrix(contents)
        
        # í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì£¼ìš” ì˜ê²¬ ê·¸ë£¹ ì‹ë³„
        clusters = await self._cluster_responses(similarity_matrix, contents)
        
        # ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ì‘ë‹µ ì„ íƒ
        largest_cluster = max(clusters, key=lambda c: len(c['members']))
        
        # í•©ì˜ ì‘ë‹µ ìƒì„±
        consensus_prompt = f"""
        ë‹¤ìŒì€ ì—¬ëŸ¬ AIì˜ ì‘ë‹µì…ë‹ˆë‹¤. ì´ë“¤ì„ ì¢…í•©í•˜ì—¬ ê°€ì¥ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        
        ì›ë˜ ì§ˆë¬¸: {prompt}
        
        AI ì‘ë‹µë“¤:
        {self._format_responses_for_consensus(responses)}
        
        ìœ„ ì‘ë‹µë“¤ì„ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1. ì •í™•ì„±: ê³¼í•™ì ìœ¼ë¡œ ì •í™•í•œ ì •ë³´
        2. ì™„ì„±ë„: ë¹ ì§„ ë¶€ë¶„ ì—†ì´ ì™„ì „í•œ ë‹µë³€
        3. ì‹¤ìš©ì„±: ì‹¤ì œ ì ìš© ê°€ëŠ¥í•œ ì¡°ì–¸
        4. ëª…í™•ì„±: ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
        """
        
        # Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… í•©ì˜ ìƒì„± (ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì—”ì§„)
        if self.engines['gemini'].available:
            final_response = await self.engines['gemini'].generate_response(
                consensus_prompt,
                temperature=0.3  # ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µ
            )
        else:
            # Gemini ì‚¬ìš© ë¶ˆê°€ì‹œ ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ì‘ë‹µ ì‚¬ìš©
            final_response = {
                'status': 'success',
                'response': contents[largest_cluster['representative_idx']]
            }
        
        return {
            'status': 'success',
            'response': final_response.get('response', ''),
            'metadata': {
                'strategy': 'consensus',
                'participating_engines': engines,
                'consensus_confidence': largest_cluster['size'] / len(responses),
                'cluster_count': len(clusters),
                'timestamp': datetime.now()
            }
        }
    
    async def _calculate_similarity_matrix(self, contents: List[str]) -> np.ndarray:
        """ì‘ë‹µ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        
        # TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(max_features=500)
        tfidf_matrix = vectorizer.fit_transform(contents)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_matrix = cosine_similarity(tfidf_matrix)
        
        return similarity_matrix
    
    async def _cluster_responses(self, similarity_matrix: np.ndarray, contents: List[str]) -> List[Dict]:
        """ì‘ë‹µ í´ëŸ¬ìŠ¤í„°ë§"""
        from sklearn.cluster import DBSCAN
        
        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
        clustering = DBSCAN(eps=0.3, min_samples=2, metric='precomputed')
        distance_matrix = 1 - similarity_matrix
        cluster_labels = clustering.fit_predict(distance_matrix)
        
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ êµ¬ì„±
        clusters = []
        unique_labels = set(cluster_labels)
        
        for label in unique_labels:
            if label == -1:  # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ëŠ” ê°œë³„ í´ëŸ¬ìŠ¤í„°ë¡œ ì²˜ë¦¬
                indices = np.where(cluster_labels == label)[0]
                for idx in indices:
                    clusters.append({
                        'label': f'single_{idx}',
                        'members': [idx],
                        'size': 1,
                        'representative_idx': idx
                    })
            else:
                indices = np.where(cluster_labels == label)[0].tolist()
                # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ ì‘ë‹µì„ ëŒ€í‘œë¡œ ì„ íƒ
                cluster_similarities = similarity_matrix[np.ix_(indices, indices)]
                avg_similarities = cluster_similarities.mean(axis=1)
                representative_idx = indices[np.argmax(avg_similarities)]
                
                clusters.append({
                    'label': label,
                    'members': indices,
                    'size': len(indices),
                    'representative_idx': representative_idx
                })
        
        return clusters
    
    def _format_responses_for_consensus(self, responses: List[Dict]) -> str:
        """í•©ì˜ë¥¼ ìœ„í•œ ì‘ë‹µ í¬ë§·íŒ…"""
        formatted = []
        for i, resp in enumerate(responses):
            formatted.append(f"""
--- {resp['engine_name']} ì‘ë‹µ ---
{resp['response']}
---
""")
        return "\n".join(formatted)
    
    async def _select_best(self, responses: List[Dict], context: Dict) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì  ì‘ë‹µ ì„ íƒ"""
        # í‰ê°€ ê¸°ì¤€
        scores = []
        
        for resp in responses:
            score = 0
            content = resp['response']
            
            # ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ìƒì„¸í•¨)
            length = len(content)
            if 500 <= length <= 2000:
                score += 20
            elif 200 <= length < 500 or 2000 < length <= 3000:
                score += 10
            
            # êµ¬ì¡°í™” ì ìˆ˜ (ë‹¨ë½, ë¦¬ìŠ¤íŠ¸ ë“±)
            if '\n\n' in content:  # ë‹¨ë½ êµ¬ë¶„
                score += 10
            if any(marker in content for marker in ['1.', 'â€¢', '-', '*']):  # ë¦¬ìŠ¤íŠ¸
                score += 10
            
            # ì „ë¬¸ì„± ì ìˆ˜ (ì „ë¬¸ ìš©ì–´ í¬í•¨)
            technical_terms = ['ê³ ë¶„ì', 'ì¤‘í•©', 'ê°€êµ', 'ë¶„ìëŸ‰', 'ìœ ë¦¬ì „ì´ì˜¨ë„', 'Tg', 'Tm']
            term_count = sum(1 for term in technical_terms if term in content)
            score += min(term_count * 5, 30)
            
            # ì‹¤ìš©ì„± ì ìˆ˜ (ì‹¤í—˜ ì¡°ê±´, ìˆ˜ì¹˜ í¬í•¨)
            import re
            numbers = re.findall(r'\d+\.?\d*', content)
            score += min(len(numbers) * 2, 20)
            
            # ì—”ì§„ë³„ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜
            engine_weights = {
                'gemini': 1.2,
                'deepseek': 1.1,
                'sambanova': 1.0,
                'grok': 0.9,
                'groq': 0.8,
                'huggingface': 0.8
            }
            score *= engine_weights.get(resp['engine_name'], 1.0)
            
            scores.append(score)
        
        # ìµœê³  ì ìˆ˜ ì‘ë‹µ ì„ íƒ
        best_idx = np.argmax(scores)
        best_response = responses[best_idx]
        
        return {
            'status': 'success',
            'response': best_response['response'],
            'metadata': {
                'strategy': 'best',
                'selected_engine': best_response['engine_name'],
                'score': scores[best_idx],
                'all_scores': {r['engine_name']: s for r, s in zip(responses, scores)},
                'timestamp': datetime.now()
            }
        }
    
    async def _ensemble_responses(self, responses: List[Dict]) -> Dict[str, Any]:
        """ì‘ë‹µ ì•™ìƒë¸” (ê°€ì¤‘ í‰ê· )"""
        # ê° ì‘ë‹µì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        key_points = []
        for resp in responses:
            points = await self._extract_key_points(resp['response'])
            key_points.extend(points)
        
        # ì¤‘ë³µ ì œê±° ë° ì¤‘ìš”ë„ ê³„ì‚°
        unique_points = []
        point_counts = {}
        
        for point in key_points:
            if point not in point_counts:
                point_counts[point] = 0
            point_counts[point] += 1
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        sorted_points = sorted(point_counts.items(), key=lambda x: x[1], reverse=True)
        
        # ì•™ìƒë¸” ì‘ë‹µ ìƒì„±
        ensemble_prompt = f"""
        ë‹¤ìŒ í•µì‹¬ í¬ì¸íŠ¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        
        {self._format_key_points(sorted_points[:10])}  # ìƒìœ„ 10ê°œ í¬ì¸íŠ¸
        
        ìœ„ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì—¬ ì™„ì„±ë„ ìˆëŠ” ë‹µë³€ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        """
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ì—”ì§„ìœ¼ë¡œ ìƒì„±
        for engine in self.engines.values():
            if engine.available:
                final_response = await engine.generate_response(
                    ensemble_prompt,
                    temperature=0.5
                )
                break
        else:
            final_response = {'status': 'error', 'response': 'ì•™ìƒë¸” ìƒì„± ì‹¤íŒ¨'}
        
        return {
            'status': 'success',
            'response': final_response.get('response', ''),
            'metadata': {
                'strategy': 'ensemble',
                'point_count': len(sorted_points),
                'participating_engines': [r['engine_name'] for r in responses],
                'timestamp': datetime.now()
            }
        }
    
    async def _extract_key_points(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ êµ¬í˜„: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        import re
        sentences = re.split(r'[.!?]+', text)
        
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        key_points = []
        for sent in sentences:
            sent = sent.strip()
            if len(sent) > 20:  # ìµœì†Œ ê¸¸ì´
                key_points.append(sent)
        
        return key_points[:5]  # ì‘ë‹µë‹¹ ìµœëŒ€ 5ê°œ í¬ì¸íŠ¸
    
    def _format_key_points(self, points: List[Tuple[str, int]]) -> str:
        """í•µì‹¬ í¬ì¸íŠ¸ í¬ë§·íŒ…"""
        formatted = []
        for i, (point, count) in enumerate(points):
            formatted.append(f"{i+1}. {point} (ì–¸ê¸‰ íšŸìˆ˜: {count})")
        return "\n".join(formatted)
    
    def get_engine_status(self) -> Dict[str, Dict]:
        """ëª¨ë“  ì—”ì§„ ìƒíƒœ ë°˜í™˜"""
        status = {}
        for name, engine in self.engines.items():
            status[name] = {
                'available': engine.available,
                'capabilities': engine.get_capabilities(),
                'usage': engine.usage_tracker.get_usage_stats() if hasattr(engine, 'usage_tracker') else {}
            }
        return status

# ==================== AI í•™ìŠµ ì‹œìŠ¤í…œ ====================
class AILearningSystem:
    """AI ìƒí˜¸ì‘ìš© í•™ìŠµ ë° ê°œì„  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.interaction_db = InteractionDatabase()
        self.performance_metrics = defaultdict(lambda: {
            'success_rate': 0,
            'user_satisfaction': 0,
            'response_quality': 0,
            'usage_count': 0
        })
        self.feedback_queue = queue.Queue()
        self.learning_thread = None
        self.running = False
        
    async def record_interaction(self, 
                               prompt: str, 
                               context: Dict,
                               responses: List[Dict],
                               final_result: Dict,
                               user_feedback: Optional[Dict] = None):
        """ìƒí˜¸ì‘ìš© ê¸°ë¡"""
        interaction = {
            'id': str(uuid.uuid4()),
            'timestamp': datetime.now(),
            'prompt': prompt,
            'context': context,
            'responses': responses,
            'final_result': final_result,
            'user_feedback': user_feedback
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        await self.interaction_db.save_interaction(interaction)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self._update_performance_metrics(interaction)
        
        # í•™ìŠµ íì— ì¶”ê°€
        if user_feedback:
            self.feedback_queue.put(interaction)
    
    def _update_performance_metrics(self, interaction: Dict):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        for response in interaction['responses']:
            engine_name = response.get('engine_name')
            if engine_name:
                metrics = self.performance_metrics[engine_name]
                metrics['usage_count'] += 1
                
                # ì„±ê³µë¥  ê³„ì‚°
                if response.get('status') == 'success':
                    metrics['success_rate'] = (
                        metrics['success_rate'] * (metrics['usage_count'] - 1) + 1
                    ) / metrics['usage_count']
                
                # ì‚¬ìš©ì ë§Œì¡±ë„ (í”¼ë“œë°±ì´ ìˆëŠ” ê²½ìš°)
                if interaction.get('user_feedback'):
                    rating = interaction['user_feedback'].get('rating', 0)
                    if rating > 0:
                        metrics['user_satisfaction'] = (
                            metrics['user_satisfaction'] * (metrics['usage_count'] - 1) + rating
                        ) / metrics['usage_count']
    
    async def start_learning(self):
        """í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹œì‘"""
        self.running = True
        self.learning_thread = threading.Thread(target=self._learning_loop)
        self.learning_thread.start()
        logger.info("AI í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘ë¨")
    
    def _learning_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ë£¨í”„"""
        while self.running:
            try:
                # í”¼ë“œë°± íì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                interaction = self.feedback_queue.get(timeout=5)
                self._process_feedback(interaction)
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"í•™ìŠµ ë£¨í”„ ì˜¤ë¥˜: {str(e)}")
    
    def _process_feedback(self, interaction: Dict):
        """í”¼ë“œë°± ì²˜ë¦¬ ë° í•™ìŠµ"""
        feedback = interaction.get('user_feedback', {})
        rating = feedback.get('rating', 0)
        comments = feedback.get('comments', '')
        
        # ê¸ì •ì /ë¶€ì •ì  í”¼ë“œë°± ë¶„ì„
        if rating >= 4:
            # ê¸ì •ì  í”¼ë“œë°±: ì„±ê³µ íŒ¨í„´ ê°•í™”
            self._reinforce_positive_patterns(interaction)
        elif rating <= 2:
            # ë¶€ì •ì  í”¼ë“œë°±: ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
            self._analyze_failure_patterns(interaction)
        
        # ì½”ë©˜íŠ¸ ë¶„ì„ (ìˆëŠ” ê²½ìš°)
        if comments:
            self._analyze_user_comments(comments, interaction)
    
    def _reinforce_positive_patterns(self, interaction: Dict):
        """ê¸ì •ì  íŒ¨í„´ ê°•í™”"""
        # ì„±ê³µì ì¸ ì‘ë‹µì˜ íŠ¹ì„± ì¶”ì¶œ
        successful_features = {
            'prompt_length': len(interaction['prompt']),
            'response_length': len(interaction['final_result']['response']),
            'strategy': interaction['final_result']['metadata'].get('strategy'),
            'engines_used': interaction['final_result']['metadata'].get('participating_engines', [])
        }
        
        # íŒ¨í„´ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        asyncio.create_task(
            self.interaction_db.save_pattern('positive', successful_features)
        )
    
    def _analyze_failure_patterns(self, interaction: Dict):
        """ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„"""
        # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
        failure_analysis = {
            'prompt_clarity': self._assess_prompt_clarity(interaction['prompt']),
            'context_completeness': bool(interaction.get('context')),
            'response_issues': self._identify_response_issues(interaction['final_result']['response'])
        }
        
        # ê°œì„  ì œì•ˆ ìƒì„±
        improvements = self._generate_improvements(failure_analysis)
        
        # íŒ¨í„´ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        asyncio.create_task(
            self.interaction_db.save_pattern('negative', failure_analysis, improvements)
        )
    
    def _assess_prompt_clarity(self, prompt: str) -> float:
        """í”„ë¡¬í”„íŠ¸ ëª…í™•ì„± í‰ê°€"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í‰ê°€
        score = 1.0
        
        # ê¸¸ì´ ì²´í¬
        if len(prompt) < 10:
            score -= 0.3
        elif len(prompt) > 500:
            score -= 0.2
        
        # ì§ˆë¬¸ ë§ˆí¬ ì¡´ì¬
        if '?' not in prompt:
            score -= 0.1
        
        # êµ¬ì²´ì  í‚¤ì›Œë“œ ì¡´ì¬
        specific_keywords = ['ì–´ë–»ê²Œ', 'ì™œ', 'ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””', 'ì–¼ë§ˆë‚˜']
        if not any(kw in prompt for kw in specific_keywords):
            score -= 0.2
        
        return max(0, score)
    
    def _identify_response_issues(self, response: str) -> List[str]:
        """ì‘ë‹µ ë¬¸ì œì  ì‹ë³„"""
        issues = []
        
        if len(response) < 50:
            issues.append("ë„ˆë¬´ ì§§ì€ ì‘ë‹µ")
        
        if not any(c in response for c in '.!?'):
            issues.append("êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì‘ë‹µ")
        
        if response.count('\n') < 2:
            issues.append("ë‹¨ë½ êµ¬ë¶„ ë¶€ì¡±")
        
        return issues
    
    def _generate_improvements(self, analysis: Dict) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        improvements = []
        
        if analysis['prompt_clarity'] < 0.7:
            improvements.append("í”„ë¡¬í”„íŠ¸ë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±")
        
        if not analysis['context_completeness']:
            improvements.append("ì‹¤í—˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€")
        
        if 'response_issues' in analysis:
            for issue in analysis['response_issues']:
                if issue == "ë„ˆë¬´ ì§§ì€ ì‘ë‹µ":
                    improvements.append("ë” ìì„¸í•œ ì„¤ëª… ìš”ì²­")
                elif issue == "êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì‘ë‹µ":
                    improvements.append("ë‹¨ê³„ë³„ ë˜ëŠ” í¬ì¸íŠ¸ë³„ ì„¤ëª… ìš”ì²­")
        
        return improvements
    
    def _analyze_user_comments(self, comments: str, interaction: Dict):
        """ì‚¬ìš©ì ì½”ë©˜íŠ¸ ë¶„ì„"""
        # ê°ì„± ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
        # ì‹¤ì œë¡œëŠ” NLP ëª¨ë¸ì„ ì‚¬ìš©í•˜ê² ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ êµ¬í˜„
        keywords = {
            'positive': ['ì¢‹ì•„ìš”', 'í›Œë¥­', 'ì •í™•', 'ë„ì›€', 'ê°ì‚¬'],
            'negative': ['ë¶€ì¡±', 'í‹€ë¦¼', 'ì´í•´', 'ì–´ë ¤ì›€', 'ë³µì¡'],
            'suggestions': ['í•˜ë©´', 'ìœ¼ë©´', 'ë”', 'ê°œì„ ', 'ì¶”ê°€']
        }
        
        comment_analysis = {
            'sentiment': 'neutral',
            'key_issues': [],
            'suggestions': []
        }
        
        # ê°ì„± íŒë‹¨
        positive_count = sum(1 for word in keywords['positive'] if word in comments)
        negative_count = sum(1 for word in keywords['negative'] if word in comments)
        
        if positive_count > negative_count:
            comment_analysis['sentiment'] = 'positive'
        elif negative_count > positive_count:
            comment_analysis['sentiment'] = 'negative'
        
        # ì œì•ˆ ì‚¬í•­ ì¶”ì¶œ
        for word in keywords['suggestions']:
            if word in comments:
                comment_analysis['suggestions'].append(comments)
                break
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        asyncio.create_task(
            self.interaction_db.save_comment_analysis(
                interaction['id'], 
                comment_analysis
            )
        )
    
    def get_learning_insights(self) -> Dict[str, Any]:
        """í•™ìŠµ ì¸ì‚¬ì´íŠ¸ ë°˜í™˜"""
        return {
            'performance_metrics': dict(self.performance_metrics),
            'total_interactions': self.interaction_db.get_interaction_count(),
            'feedback_processed': self.feedback_queue.qsize(),
            'top_performing_engines': self._get_top_engines(),
            'common_issues': self._get_common_issues(),
            'improvement_trends': self._get_improvement_trends()
        }
    
    def _get_top_engines(self) -> List[Tuple[str, float]]:
        """ìƒìœ„ ì„±ëŠ¥ ì—”ì§„ ë°˜í™˜"""
        engine_scores = []
        
        for engine, metrics in self.performance_metrics.items():
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            score = (
                metrics['success_rate'] * 0.3 +
                metrics['user_satisfaction'] * 0.5 +
                metrics['response_quality'] * 0.2
            )
            engine_scores.append((engine, score))
        
        return sorted(engine_scores, key=lambda x: x[1], reverse=True)[:3]
    
    def _get_common_issues(self) -> List[Dict]:
        """ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ ë°˜í™˜"""
        return self.interaction_db.get_common_issues(limit=5)
    
    def _get_improvement_trends(self) -> Dict:
        """ê°œì„  ì¶”ì„¸ ë°˜í™˜"""
        return self.interaction_db.get_improvement_trends(days=30)

# ==================== ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ë§¤ë‹ˆì € ====================
class DatabaseIntegrationManager:
    """ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ê´€ë¦¬"""
    
    def __init__(self):
        self.databases = {
            'materials_project': MaterialsProjectClient(),
            'pubchem': PubChemClient(),
            'polyinfo': PolyInfoClient(),
            'protocols_io': ProtocolsIOClient(),
            'github': GitHubClient(),
            'zenodo': ZenodoClient(),
            'figshare': FigshareClient(),
            'openalex': OpenAlexClient(),
            'crossref': CrossrefClient()
        }
        self.cache = DatabaseCache()
        self.search_orchestrator = SearchOrchestrator()
        
    async def initialize(self):
        """ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        init_tasks = []
        for db_name, client in self.databases.items():
            init_tasks.append(self._init_database(db_name, client))
            
        results = await asyncio.gather(*init_tasks)
        
        active_count = sum(1 for r in results if r)
        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ ({active_count}/{len(self.databases)} í™œì„±)")
    
    async def _init_database(self, name: str, client) -> bool:
        """ê°œë³„ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            await client.initialize()
            logger.info(f"{name} ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            return True
        except Exception as e:
            logger.error(f"{name} ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def search_all(self, 
                        query: str, 
                        search_type: str = 'general',
                        filters: Dict[str, Any] = None) -> Dict[str, List[Dict]]:
        """ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ê²€ìƒ‰"""
        # ìºì‹œ í™•ì¸
        cache_key = self.cache.generate_search_key(query, search_type, filters)
        cached_results = self.cache.get(cache_key)
        if cached_results:
            return cached_results
        
        # ê²€ìƒ‰ ìœ í˜•ë³„ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ
        target_databases = self._select_databases(search_type)
        
        # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
        search_tasks = []
        for db_name in target_databases:
            if db_name in self.databases and self.databases[db_name].is_available:
                search_tasks.append(
                    self._search_database(db_name, query, filters)
                )
        
        results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•©
        integrated_results = self._integrate_search_results(
            results, 
            target_databases, 
            search_type
        )
        
        # ìºì‹œ ì €ì¥
        self.cache.set(cache_key, integrated_results, ttl=3600)  # 1ì‹œê°„ ìºì‹œ
        
        return integrated_results
    
    def _select_databases(self, search_type: str) -> List[str]:
        """ê²€ìƒ‰ ìœ í˜•ë³„ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ"""
        database_groups = {
            'literature': ['openalex', 'crossref', 'pubchem'],
            'protocols': ['protocols_io', 'github', 'zenodo', 'figshare'],
            'materials': ['materials_project', 'polyinfo', 'pubchem'],
            'general': list(self.databases.keys())
        }
        
        return database_groups.get(search_type, database_groups['general'])
    
    async def _search_database(self, 
                              db_name: str, 
                              query: str, 
                              filters: Dict) -> Dict[str, Any]:
        """ê°œë³„ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰"""
        try:
            client = self.databases[db_name]
            results = await client.search(query, filters)
            
            return {
                'database': db_name,
                'status': 'success',
                'results': results,
                'count': len(results)
            }
        except Exception as e:
            logger.error(f"{db_name} ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return {
                'database': db_name,
                'status': 'error',
                'error': str(e),
                'results': []
            }

# Polymer-doe-platform - Part 8
# ==================== ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„ ====================
class BaseDBClient:
    """ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, base_url: str, requires_auth: bool = False):
        self.name = name
        self.base_url = base_url
        self.requires_auth = requires_auth
        self.is_available = False
        self.session = None
        self.rate_limiter = RateLimiter(name)
        self.auth_credentials = None
        
    async def initialize(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        import aiohttp
        self.session = aiohttp.ClientSession()
        
        if self.requires_auth:
            self.auth_credentials = await self._get_auth_credentials()
            if not self.auth_credentials:
                logger.warning(f"{self.name}: ì¸ì¦ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
                self.is_available = False
                return
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            await self._test_connection()
            self.is_available = True
            logger.info(f"{self.name}: ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"{self.name}: ì—°ê²° ì‹¤íŒ¨ - {str(e)}")
            self.is_available = False
    
    async def _get_auth_credentials(self) -> Optional[Dict]:
        """ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        # Streamlit secretsì—ì„œ ê°€ì ¸ì˜¤ê¸°
        try:
            if self.name == 'materials_project':
                return {'api_key': st.secrets.get('MATERIALS_PROJECT_API_KEY')}
            elif self.name == 'protocols_io':
                return {'token': st.secrets.get('PROTOCOLS_IO_TOKEN')}
            elif self.name == 'github':
                return {'token': st.secrets.get('GITHUB_TOKEN')}
            # ì¶”ê°€ ë°ì´í„°ë² ì´ìŠ¤ ì¸ì¦ ì •ë³´...
        except Exception:
            return None
    
    async def _test_connection(self):
        """ì—°ê²° í…ŒìŠ¤íŠ¸ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    async def search(self, query: str, filters: Dict = None) -> List[Dict]:
        """ê²€ìƒ‰ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session:
            await self.session.close()

class MaterialsProjectClient(BaseDBClient):
    """Materials Project API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        super().__init__(
            name="materials_project",
            base_url="https://api.materialsproject.org",
            requires_auth=True
        )
        
    async def _test_connection(self):
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        headers = {'X-API-KEY': self.auth_credentials['api_key']}
        async with self.session.get(
            f"{self.base_url}/heartbeat",
            headers=headers
        ) as response:
            if response.status != 200:
                raise ConnectionError(f"API ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
    
    async def search(self, query: str, filters: Dict = None) -> List[Dict]:
        """ì¬ë£Œ ê²€ìƒ‰"""
        if not self.is_available:
            return []
        
        # Rate limiting ì²´í¬
        if not await self.rate_limiter.check_rate():
            logger.warning(f"{self.name}: Rate limit ì´ˆê³¼")
            return []
        
        headers = {'X-API-KEY': self.auth_credentials['api_key']}
        
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° êµ¬ì„±
        params = {
            'keywords': query,
            'limit': filters.get('limit', 20) if filters else 20
        }
        
        # í•„í„° ì ìš©
        if filters:
            if 'elements' in filters:
                params['elements'] = ','.join(filters['elements'])
            if 'properties' in filters:
                params['properties'] = ','.join(filters['properties'])
        
        try:
            async with self.session.get(
                f"{self.base_url}/materials/search",
                headers=headers,
                params=params
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._format_results(data.get('data', []))
                else:
                    logger.error(f"{self.name}: ê²€ìƒ‰ ì˜¤ë¥˜ - {response.status}")
                    return []
        except Exception as e:
            logger.error(f"{self.name}: ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ - {str(e)}")
            return []
    
    def _format_results(self, raw_results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        for item in raw_results:
            formatted.append({
                'source': 'Materials Project',
                'material_id': item.get('material_id'),
                'formula': item.get('formula_pretty'),
                'structure': item.get('structure'),
                'properties': {
                    'band_gap': item.get('band_gap'),
                    'density': item.get('density'),
                    'formation_energy': item.get('formation_energy_per_atom'),
                    'elasticity': item.get('elasticity')
                },
                'url': f"https://materialsproject.org/materials/{item.get('material_id')}"
            })
        return formatted

class PubChemClient(BaseDBClient):
    """PubChem API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        super().__init__(
            name="pubchem",
            base_url="https://pubchem.ncbi.nlm.nih.gov/rest/pug",
            requires_auth=False
        )
        
    async def _test_connection(self):
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        async with self.session.get(f"{self.base_url}/compound/name/water/property/MolecularFormula/JSON") as response:
            if response.status != 200:
                raise ConnectionError(f"API ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
    
    async def search(self, query: str, filters: Dict = None) -> List[Dict]:
        """í™”í•©ë¬¼ ê²€ìƒ‰"""
        if not self.is_available:
            return []
        
        search_type = filters.get('search_type', 'name') if filters else 'name'
        properties = filters.get('properties', ['MolecularFormula', 'MolecularWeight', 'CanonicalSMILES']) if filters else ['MolecularFormula', 'MolecularWeight', 'CanonicalSMILES']
        
        property_string = ','.join(properties)
        
        try:
            # ê²€ìƒ‰ ìˆ˜í–‰
            if search_type == 'name':
                url = f"{self.base_url}/compound/name/{query}/property/{property_string}/JSON"
            elif search_type == 'smiles':
                url = f"{self.base_url}/compound/smiles/{query}/property/{property_string}/JSON"
            elif search_type == 'formula':
                url = f"{self.base_url}/compound/fastformula/{query}/property/{property_string}/JSON"
            else:
                url = f"{self.base_url}/compound/name/{query}/property/{property_string}/JSON"
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._format_results(data.get('PropertyTable', {}).get('Properties', []))
                else:
                    logger.error(f"{self.name}: ê²€ìƒ‰ ì˜¤ë¥˜ - {response.status}")
                    return []
        except Exception as e:
            logger.error(f"{self.name}: ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ - {str(e)}")
            return []
    
    def _format_results(self, raw_results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        for item in raw_results:
            formatted.append({
                'source': 'PubChem',
                'cid': item.get('CID'),
                'molecular_formula': item.get('MolecularFormula'),
                'molecular_weight': item.get('MolecularWeight'),
                'smiles': item.get('CanonicalSMILES'),
                'iupac_name': item.get('IUPACName'),
                'url': f"https://pubchem.ncbi.nlm.nih.gov/compound/{item.get('CID')}"
            })
        return formatted

class PolyInfoClient(BaseDBClient):
    """PoLyInfo ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        super().__init__(
            name="polyinfo",
            base_url="https://polymer.nims.go.jp",
            requires_auth=True
        )
        
    async def search(self, query: str, filters: Dict = None) -> List[Dict]:
        """ê³ ë¶„ì ì •ë³´ ê²€ìƒ‰"""
        if not self.is_available:
            return []
        
        # PoLyInfoëŠ” ì›¹ ìŠ¤í¬ë˜í•‘ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ë°ì´í„° ë°˜í™˜
        polymer_database = {
            'PET': {
                'name': 'Polyethylene Terephthalate',
                'formula': '(C10H8O4)n',
                'properties': {
                    'Tg': 75,  # Â°C
                    'Tm': 260,  # Â°C
                    'density': 1.38,  # g/cmÂ³
                    'tensile_strength': 55,  # MPa
                    'elongation': 70  # %
                }
            },
            'PP': {
                'name': 'Polypropylene',
                'formula': '(C3H6)n',
                'properties': {
                    'Tg': -20,
                    'Tm': 165,
                    'density': 0.90,
                    'tensile_strength': 35,
                    'elongation': 400
                }
            },
            'PS': {
                'name': 'Polystyrene',
                'formula': '(C8H8)n',
                'properties': {
                    'Tg': 100,
                    'Tm': 240,
                    'density': 1.05,
                    'tensile_strength': 45,
                    'elongation': 3
                }
            }
        }
        
        results = []
        query_upper = query.upper()
        
        for code, data in polymer_database.items():
            if query_upper in code or query_upper in data['name'].upper():
                results.append({
                    'source': 'PoLyInfo',
                    'polymer_code': code,
                    'name': data['name'],
                    'formula': data['formula'],
                    'properties': data['properties'],
                    'url': f"https://polymer.nims.go.jp/PoLyInfo/{code}"
                })
        
        return results

class ProtocolsIOClient(BaseDBClient):
    """Protocols.io API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        super().__init__(
            name="protocols_io",
            base_url="https://www.protocols.io/api/v3",
            requires_auth=True
        )
        
    async def search(self, query: str, filters: Dict = None) -> List[Dict]:
        """í”„ë¡œí† ì½œ ê²€ìƒ‰"""
        if not self.is_available:
            return []
        
        headers = {
            'Authorization': f"Bearer {self.auth_credentials['token']}",
            'Content-Type': 'application/json'
        }
        
        params = {
            'q': query,
            'page_size': filters.get('limit', 20) if filters else 20,
            'order_field': 'relevance'
        }
        
        # í•„í„° ì ìš©
        if filters:
            if 'tags' in filters:
                params['tags'] = ','.join(filters['tags'])
            if 'created_after' in filters:
                params['created_after'] = filters['created_after']
        
        try:
            async with self.session.get(
                f"{self.base_url}/protocols",
                headers=headers,
                params=params
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._format_results(data.get('items', []))
                else:
                    logger.error(f"{self.name}: ê²€ìƒ‰ ì˜¤ë¥˜ - {response.status}")
                    return []
        except Exception as e:
            logger.error(f"{self.name}: ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ - {str(e)}")
            return []
    
    def _format_results(self, raw_results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        for item in raw_results:
            formatted.append({
                'source': 'Protocols.io',
                'protocol_id': item.get('id'),
                'title': item.get('title'),
                'description': item.get('description'),
                'authors': [author.get('name') for author in item.get('authors', [])],
                'tags': item.get('tags', []),
                'created_date': item.get('created_on'),
                'url': item.get('uri')
            })
        return formatted

class GitHubClient(BaseDBClient):
    """GitHub API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        super().__init__(
            name="github",
            base_url="https://api.github.com",
            requires_auth=True
        )
        
    async def search(self, query: str, filters: Dict = None) -> List[Dict]:
        """ì½”ë“œ ë° ì €ì¥ì†Œ ê²€ìƒ‰"""
        if not self.is_available:
            return []
        
        headers = {
            'Authorization': f"token {self.auth_credentials['token']}",
            'Accept': 'application/vnd.github.v3+json'
        }
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = f"{query} polymer"
        if filters:
            if 'language' in filters:
                search_query += f" language:{filters['language']}"
            if 'stars' in filters:
                search_query += f" stars:>{filters['stars']}"
        
        params = {
            'q': search_query,
            'sort': 'stars',
            'order': 'desc',
            'per_page': filters.get('limit', 20) if filters else 20
        }
        
        try:
            async with self.session.get(
                f"{self.base_url}/search/repositories",
                headers=headers,
                params=params
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._format_results(data.get('items', []))
                else:
                    logger.error(f"{self.name}: ê²€ìƒ‰ ì˜¤ë¥˜ - {response.status}")
                    return []
        except Exception as e:
            logger.error(f"{self.name}: ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ - {str(e)}")
            return []
    
    def _format_results(self, raw_results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        for item in raw_results:
            formatted.append({
                'source': 'GitHub',
                'repo_name': item.get('full_name'),
                'description': item.get('description'),
                'stars': item.get('stargazers_count'),
                'language': item.get('language'),
                'topics': item.get('topics', []),
                'last_updated': item.get('updated_at'),
                'url': item.get('html_url')
            })
        return formatted

class OpenAlexClient(BaseDBClient):
    """OpenAlex API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        super().__init__(
            name="openalex",
            base_url="https://api.openalex.org",
            requires_auth=False
        )
        
    async def _test_connection(self):
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        async with self.session.get(f"{self.base_url}/works?filter=title.search:polymer&per_page=1") as response:
            if response.status != 200:
                raise ConnectionError(f"API ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
    
    async def search(self, query: str, filters: Dict = None) -> List[Dict]:
        """í•™ìˆ  ë¬¸í—Œ ê²€ìƒ‰"""
        if not self.is_available:
            return []
        
        # ê²€ìƒ‰ í•„í„° êµ¬ì„±
        filter_parts = [f"title.search:{query} OR abstract.search:{query}"]
        
        if filters:
            if 'year' in filters:
                filter_parts.append(f"publication_year:{filters['year']}")
            if 'type' in filters:
                filter_parts.append(f"type:{filters['type']}")
            if 'open_access' in filters and filters['open_access']:
                filter_parts.append("is_oa:true")
        
        params = {
            'filter': ','.join(filter_parts),
            'per_page': filters.get('limit', 25) if filters else 25,
            'sort': 'relevance_score:desc'
        }
        
        try:
            async with self.session.get(
                f"{self.base_url}/works",
                params=params
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._format_results(data.get('results', []))
                else:
                    logger.error(f"{self.name}: ê²€ìƒ‰ ì˜¤ë¥˜ - {response.status}")
                    return []
        except Exception as e:
            logger.error(f"{self.name}: ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ - {str(e)}")
            return []
    
    def _format_results(self, raw_results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        for item in raw_results:
            formatted.append({
                'source': 'OpenAlex',
                'work_id': item.get('id'),
                'title': item.get('title'),
                'authors': [
                    author.get('author', {}).get('display_name') 
                    for author in item.get('authorships', [])
                ],
                'publication_year': item.get('publication_year'),
                'doi': item.get('doi'),
                'abstract': item.get('abstract'),
                'cited_by_count': item.get('cited_by_count'),
                'is_open_access': item.get('is_oa'),
                'url': item.get('doi', '').replace('https://doi.org/', 'https://doi.org/') if item.get('doi') else None
            })
        return formatted

# ==================== ê³ ê¸‰ ì‹¤í—˜ ì„¤ê³„ ì—”ì§„ ====================
class AdvancedExperimentDesignEngine:
    """AI ê¸°ë°˜ ê³ ê¸‰ ì‹¤í—˜ ì„¤ê³„ ì—”ì§„"""
    
    def __init__(self, ai_orchestrator: AIOrchestrator, db_manager: DatabaseIntegrationManager):
        self.ai_orchestrator = ai_orchestrator
        self.db_manager = db_manager
        self.design_strategies = {
            'screening': ScreeningDesignStrategy(),
            'optimization': OptimizationDesignStrategy(),
            'mixture': MixtureDesignStrategy(),
            'robust': RobustDesignStrategy(),
            'adaptive': AdaptiveDesignStrategy(),
            'sequential': SequentialDesignStrategy()
        }
        self.design_validator = DesignValidator()
        self.cost_estimator = CostEstimator()
        
    async def create_experiment_design(self,
                                     project_info: Dict[str, Any],
                                     user_level: UserLevel = UserLevel.BEGINNER) -> Dict[str, Any]:
        """ì‹¤í—˜ ì„¤ê³„ ìƒì„±"""
        logger.info("ê³ ê¸‰ ì‹¤í—˜ ì„¤ê³„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        
        # 1. í”„ë¡œì íŠ¸ ë¶„ì„ ë° ì„¤ê³„ ì „ëµ ì„ íƒ
        strategy = await self._select_design_strategy(project_info, user_level)
        
        # 2. ê´€ë ¨ ë¬¸í—Œ ë° í”„ë¡œí† ì½œ ê²€ìƒ‰
        reference_data = await self._search_references(project_info)
        
        # 3. AI ê¸°ë°˜ ì„¤ê³„ ìƒì„±
        ai_design = await self._generate_ai_design(
            project_info, 
            strategy, 
            reference_data,
            user_level
        )
        
        # 4. ì„¤ê³„ ê²€ì¦ ë° ìµœì í™”
        validated_design = await self._validate_and_optimize(ai_design, project_info)
        
        # 5. ë¹„ìš© ë° ì‹œê°„ ì¶”ì •
        estimates = await self._estimate_cost_and_time(validated_design, project_info)
        
        # 6. ì‚¬ìš©ì ë ˆë²¨ë³„ ì„¤ëª… ì¶”ê°€
        final_design = await self._add_level_appropriate_explanations(
            validated_design, 
            estimates, 
            user_level
        )
        
        return final_design
    
    async def _select_design_strategy(self, 
                                    project_info: Dict, 
                                    user_level: UserLevel) -> str:
        """í”„ë¡œì íŠ¸ì— ì í•©í•œ ì„¤ê³„ ì „ëµ ì„ íƒ"""
        # AIì—ê²Œ í”„ë¡œì íŠ¸ ë¶„ì„ ìš”ì²­
        analysis_prompt = f"""
        ë‹¤ìŒ ê³ ë¶„ì ì‹¤í—˜ í”„ë¡œì íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ê°€ì¥ ì í•©í•œ ì‹¤í—˜ ì„¤ê³„ ì „ëµì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.
        
        í”„ë¡œì íŠ¸ ì •ë³´:
        - ëª©ì : {project_info.get('objective')}
        - ê³ ë¶„ì ì¢…ë¥˜: {project_info.get('polymer_type')}
        - ì£¼ìš” íŠ¹ì„±: {project_info.get('target_properties')}
        - ìš”ì¸ ìˆ˜: {len(project_info.get('factors', []))}
        - ì˜ˆì‚°: {project_info.get('budget')} ë§Œì›
        - ê¸°ê°„: {project_info.get('timeline')} ì£¼
        
        ê°€ëŠ¥í•œ ì „ëµ:
        1. screening: ìŠ¤í¬ë¦¬ë‹ (ë§ì€ ìš”ì¸ ì¤‘ ì¤‘ìš” ìš”ì¸ ì„ ë³„)
        2. optimization: ìµœì í™” (ë°˜ì‘í‘œë©´ë¶„ì„)
        3. mixture: í˜¼í•©ë¬¼ ì„¤ê³„ (ì¡°ì„± ìµœì í™”)
        4. robust: ê°•ê±´ ì„¤ê³„ (ì¡ìŒ ì¸ì ê³ ë ¤)
        5. adaptive: ì ì‘í˜• ì„¤ê³„ (ì‹¤ì‹œê°„ ì¡°ì •)
        6. sequential: ìˆœì°¨ì  ì„¤ê³„ (ë‹¨ê³„ë³„ ì •ë°€í™”)
        
        ì¶”ì²œ ì „ëµê³¼ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        """
        
        response = await self.ai_orchestrator.query_single(
            'gemini',
            analysis_prompt,
            temperature=0.3
        )
        
        # ì‘ë‹µì—ì„œ ì „ëµ ì¶”ì¶œ
        strategy = self._extract_strategy_from_response(response.get('response', ''))
        
        # ì‚¬ìš©ì ë ˆë²¨ì— ë”°ë¥¸ ì „ëµ ì¡°ì •
        if user_level == UserLevel.BEGINNER and strategy in ['adaptive', 'sequential']:
            strategy = 'screening'  # ì´ˆë³´ìëŠ” ë‹¨ìˆœí•œ ì „ëµìœ¼ë¡œ
        
        return strategy
    
    def _extract_strategy_from_response(self, response: str) -> str:
        """AI ì‘ë‹µì—ì„œ ì „ëµ ì¶”ì¶œ"""
        strategies = ['screening', 'optimization', 'mixture', 'robust', 'adaptive', 'sequential']
        
        for strategy in strategies:
            if strategy in response.lower():
                return strategy
        
        return 'screening'  # ê¸°ë³¸ê°’
    
    async def _search_references(self, project_info: Dict) -> Dict[str, List]:
        """ê´€ë ¨ ë¬¸í—Œ ë° í”„ë¡œí† ì½œ ê²€ìƒ‰"""
        logger.info("ì°¸ê³  ìë£Œ ê²€ìƒ‰ ì¤‘...")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_queries = []
        
        # ê³ ë¶„ì ì¢…ë¥˜ ê¸°ë°˜ ê²€ìƒ‰
        if 'polymer_type' in project_info:
            search_queries.append(f"{project_info['polymer_type']} characterization")
            search_queries.append(f"{project_info['polymer_type']} synthesis")
        
        # ëª©í‘œ íŠ¹ì„± ê¸°ë°˜ ê²€ìƒ‰
        if 'target_properties' in project_info:
            for prop in project_info['target_properties']:
                search_queries.append(f"polymer {prop} measurement")
        
        # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
        search_tasks = []
        for query in search_queries[:3]:  # ìƒìœ„ 3ê°œ ì¿¼ë¦¬ë§Œ
            search_tasks.append(self.db_manager.search_all(
                query,
                search_type='literature',
                filters={'limit': 10}
            ))
        
        results = await asyncio.gather(*search_tasks)
        
        # ê²°ê³¼ í†µí•©
        reference_data = {
            'papers': [],
            'protocols': [],
            'materials_data': []
        }
        
        for result_set in results:
            for db_name, items in result_set.items():
                if 'openalex' in db_name or 'crossref' in db_name:
                    reference_data['papers'].extend(items[:5])
                elif 'protocols' in db_name:
                    reference_data['protocols'].extend(items[:3])
                elif 'materials' in db_name or 'polyinfo' in db_name:
                    reference_data['materials_data'].extend(items[:3])
        
        return reference_data
    
    async def _generate_ai_design(self,
                                project_info: Dict,
                                strategy: str,
                                reference_data: Dict,
                                user_level: UserLevel) -> Dict:
        """AI ê¸°ë°˜ ì‹¤í—˜ ì„¤ê³„ ìƒì„±"""
        # ì°¸ê³  ìë£Œ ìš”ì•½
        ref_summary = self._summarize_references(reference_data)
        
        # ì„¤ê³„ ìƒì„± í”„ë¡¬í”„íŠ¸
        design_prompt = f"""
        ê³ ë¶„ì ì‹¤í—˜ ì„¤ê³„ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        
        í”„ë¡œì íŠ¸ ì •ë³´:
        {json.dumps(project_info, ensure_ascii=False, indent=2)}
        
        ì„ íƒëœ ì „ëµ: {strategy}
        
        ì°¸ê³  ìë£Œ ìš”ì•½:
        {ref_summary}
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‹¤í—˜ ì„¤ê³„ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
        1. ì‹¤í—˜ ì œëª©
        2. ì„¤ê³„ ê·¼ê±°
        3. ìš”ì¸ ë° ìˆ˜ì¤€
        4. ë°˜ì‘ë³€ìˆ˜
        5. ì‹¤í—˜ ë§¤íŠ¸ë¦­ìŠ¤
        6. ì˜ˆìƒë˜ëŠ” ìƒí˜¸ì‘ìš©
        7. ì£¼ì˜ì‚¬í•­
        8. ì¶”ì²œ ë¶„ì„ ë°©ë²•
        """
        
        # ì—¬ëŸ¬ AIì˜ ì˜ê²¬ ìˆ˜ì§‘
        response = await self.ai_orchestrator.query_multiple(
            design_prompt,
            strategy='consensus',
            engines=['gemini', 'deepseek', 'sambanova'],
            temperature=0.5,
            user_level=user_level
        )
        
        # ì‘ë‹µ íŒŒì‹±
        design = self._parse_design_response(response.get('response', ''))
        
        # ì „ëµë³„ êµ¬ì²´ì  ì„¤ê³„ ìƒì„±
        strategy_impl = self.design_strategies.get(strategy)
        if strategy_impl:
            design['matrix'] = await strategy_impl.generate_design_matrix(
                design['factors'],
                design['responses'],
                project_info
            )
        
        return design
    
    def _summarize_references(self, reference_data: Dict) -> str:
        """ì°¸ê³  ìë£Œ ìš”ì•½"""
        summary = []
        
        if reference_data['papers']:
            summary.append("### ê´€ë ¨ ë…¼ë¬¸:")
            for paper in reference_data['papers'][:3]:
                summary.append(f"- {paper.get('title', 'N/A')} ({paper.get('publication_year', 'N/A')})")
        
        if reference_data['protocols']:
            summary.append("\n### ì‹¤í—˜ í”„ë¡œí† ì½œ:")
            for protocol in reference_data['protocols'][:2]:
                summary.append(f"- {protocol.get('title', 'N/A')}")
        
        if reference_data['materials_data']:
            summary.append("\n### ì¬ë£Œ ë°ì´í„°:")
            for material in reference_data['materials_data'][:2]:
                summary.append(f"- {material.get('name', material.get('formula', 'N/A'))}")
        
        return "\n".join(summary) if summary else "ì°¸ê³  ìë£Œ ì—†ìŒ"
    
    def _parse_design_response(self, response: str) -> Dict:
        """AI ì‘ë‹µì—ì„œ ì„¤ê³„ ì •ë³´ ì¶”ì¶œ"""
        design = {
            'experiment_title': '',
            'reasoning': '',
            'factors': [],
            'responses': [],
            'interactions': [],
            'precautions': [],
            'analysis_methods': []
        }
        
        # ê°„ë‹¨í•œ íŒŒì‹± ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
        sections = response.split('\n\n')
        
        for section in sections:
            if 'ì‹¤í—˜ ì œëª©' in section or 'Experiment Title' in section:
                design['experiment_title'] = section.split(':', 1)[-1].strip()
            elif 'ì„¤ê³„ ê·¼ê±°' in section or 'Design Rationale' in section:
                design['reasoning'] = section.split(':', 1)[-1].strip()
            # ... ì¶”ê°€ íŒŒì‹± ë¡œì§
        
        return design

# Polymer-doe-platform - Part 9
# ==================== ì‹¤í—˜ ì„¤ê³„ ê²€ì¦ ë° ìµœì í™” ====================
    async def _validate_and_optimize(self, 
                                   ai_design: Dict, 
                                   project_info: Dict) -> Dict:
        """ì„¤ê³„ ê²€ì¦ ë° ìµœì í™”"""
        logger.info("ì‹¤í—˜ ì„¤ê³„ ê²€ì¦ ë° ìµœì í™” ì‹œì‘...")
        
        # 1. í†µê³„ì  ê²€ì¦
        statistical_validation = await self.design_validator.validate_statistical_properties(
            ai_design,
            project_info
        )
        
        # 2. ì‹¤ìš©ì„± ê²€ì¦
        practical_validation = await self.design_validator.validate_practical_constraints(
            ai_design,
            project_info
        )
        
        # 3. ì•ˆì „ì„± ê²€ì¦
        safety_validation = await self.design_validator.validate_safety(
            ai_design,
            project_info
        )
        
        # 4. ìµœì í™” ì œì•ˆ
        if not all([statistical_validation['is_valid'], 
                   practical_validation['is_valid'], 
                   safety_validation['is_valid']]):
            optimized_design = await self._optimize_design(
                ai_design,
                {
                    'statistical': statistical_validation,
                    'practical': practical_validation,
                    'safety': safety_validation
                }
            )
        else:
            optimized_design = ai_design
        
        # 5. ìµœì¢… ê²€ì¦ ë³´ê³ ì„œ ì¶”ê°€
        optimized_design['validation_report'] = {
            'statistical': statistical_validation,
            'practical': practical_validation,
            'safety': safety_validation,
            'overall_score': self._calculate_design_score(
                statistical_validation,
                practical_validation,
                safety_validation
            )
        }
        
        return optimized_design
    
    async def _optimize_design(self, design: Dict, validation_results: Dict) -> Dict:
        """ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ê³„ ìµœì í™”"""
        optimization_prompt = f"""
        ì‹¤í—˜ ì„¤ê³„ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê³  ìµœì í™”í•´ì£¼ì„¸ìš”.
        
        í˜„ì¬ ì„¤ê³„:
        {json.dumps(design, ensure_ascii=False, indent=2)}
        
        ë°œê²¬ëœ ë¬¸ì œì :
        - í†µê³„ì  ë¬¸ì œ: {validation_results['statistical'].get('issues', [])}
        - ì‹¤ìš©ì„± ë¬¸ì œ: {validation_results['practical'].get('issues', [])}
        - ì•ˆì „ì„± ë¬¸ì œ: {validation_results['safety'].get('issues', [])}
        
        ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ ê°œì„ í•´ì£¼ì„¸ìš”:
        1. í†µê³„ì  ê²€ì •ë ¥ í™•ë³´
        2. ì‹¤í—˜ ê°€ëŠ¥ì„±
        3. ì•ˆì „ì„± ë³´ì¥
        4. ë¹„ìš© íš¨ìœ¨ì„±
        """
        
        response = await self.ai_orchestrator.query_single(
            'deepseek',  # ìˆ˜í•™/ìµœì í™”ì— ê°•í•œ ì—”ì§„
            optimization_prompt,
            temperature=0.3
        )
        
        # ì‘ë‹µì—ì„œ ê°œì„ ëœ ì„¤ê³„ ì¶”ì¶œ
        optimized_design = self._extract_optimized_design(
            design, 
            response.get('response', '')
        )
        
        return optimized_design
    
    def _calculate_design_score(self, *validations) -> float:
        """ì„¤ê³„ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        scores = []
        weights = [0.4, 0.4, 0.2]  # í†µê³„, ì‹¤ìš©ì„±, ì•ˆì „ì„± ê°€ì¤‘ì¹˜
        
        for validation, weight in zip(validations, weights):
            score = validation.get('score', 0.5)
            scores.append(score * weight)
        
        return sum(scores)
    
    async def _estimate_cost_and_time(self, 
                                    design: Dict, 
                                    project_info: Dict) -> Dict:
        """ë¹„ìš© ë° ì‹œê°„ ì¶”ì •"""
        estimates = await self.cost_estimator.estimate(design, project_info)
        
        # AIë¥¼ í†µí•œ ì¶”ê°€ ì¶”ì •
        estimation_prompt = f"""
        ë‹¤ìŒ ê³ ë¶„ì ì‹¤í—˜ì˜ ë¹„ìš©ê³¼ ì‹œê°„ì„ ì¶”ì •í•´ì£¼ì„¸ìš”.
        
        ì‹¤í—˜ ì„¤ê³„:
        - ì‹¤í—˜ ìˆ˜: {len(design.get('matrix', []))}
        - ìš”ì¸: {[f['name'] for f in design.get('factors', [])]}
        - ë°˜ì‘ë³€ìˆ˜: {[r['name'] for r in design.get('responses', [])]}
        
        í”„ë¡œì íŠ¸ ì •ë³´:
        - ê³ ë¶„ì: {project_info.get('polymer_type')}
        - ì¥ë¹„: {project_info.get('equipment', [])}
        
        ë‹¤ìŒì„ ì¶”ì •í•´ì£¼ì„¸ìš”:
        1. ì¬ë£Œë¹„ (ë§Œì›)
        2. ì¸ê±´ë¹„ (ë§Œì›)
        3. ë¶„ì„ë¹„ (ë§Œì›)
        4. ì´ ì†Œìš” ì‹œê°„ (ì¼)
        5. ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ì†Œìš” ì‹œê°„ (ì¼)
        """
        
        ai_estimate = await self.ai_orchestrator.query_single(
            'gemini',
            estimation_prompt,
            temperature=0.5
        )
        
        # ì¶”ì •ì¹˜ í†µí•©
        final_estimates = self._merge_estimates(estimates, ai_estimate)
        
        return final_estimates
    
    def _merge_estimates(self, 
                        calculated_estimates: Dict, 
                        ai_estimates: Dict) -> Dict:
        """ê³„ì‚°ëœ ì¶”ì •ì¹˜ì™€ AI ì¶”ì •ì¹˜ í†µí•©"""
        # AI ì‘ë‹µì—ì„œ ìˆ«ì ì¶”ì¶œ
        ai_values = self._extract_numbers_from_text(ai_estimates.get('response', ''))
        
        return {
            'material_cost': (calculated_estimates.get('material_cost', 0) + 
                            ai_values.get('material_cost', 0)) / 2,
            'labor_cost': (calculated_estimates.get('labor_cost', 0) + 
                         ai_values.get('labor_cost', 0)) / 2,
            'analysis_cost': (calculated_estimates.get('analysis_cost', 0) + 
                            ai_values.get('analysis_cost', 0)) / 2,
            'total_time_sequential': max(
                calculated_estimates.get('total_time', 0),
                ai_values.get('total_time', 0)
            ),
            'total_time_parallel': min(
                calculated_estimates.get('parallel_time', 0),
                ai_values.get('parallel_time', 0)
            ),
            'confidence': 0.8  # ì¶”ì • ì‹ ë¢°ë„
        }
    
    async def _add_level_appropriate_explanations(self,
                                                design: Dict,
                                                estimates: Dict,
                                                user_level: UserLevel) -> Dict:
        """ì‚¬ìš©ì ë ˆë²¨ì— ë§ëŠ” ì„¤ëª… ì¶”ê°€"""
        if user_level == UserLevel.EXPERT:
            # ì „ë¬¸ê°€ëŠ” ì¶”ê°€ ì„¤ëª… ë¶ˆí•„ìš”
            return {**design, 'estimates': estimates}
        
        # ì„¤ëª… ìƒì„±
        explanation_prompt = f"""
        {'ì´ˆë³´ì' if user_level == UserLevel.BEGINNER else 'ì¤‘ê¸‰ì'}ë¥¼ ìœ„í•´ 
        ë‹¤ìŒ ì‹¤í—˜ ì„¤ê³„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        
        ì„¤ê³„ ì •ë³´:
        - ì „ëµ: {design.get('strategy')}
        - ìš”ì¸ ìˆ˜: {len(design.get('factors', []))}
        - ì‹¤í—˜ ìˆ˜: {len(design.get('matrix', []))}
        
        {'ì´ˆë³´ì' if user_level == UserLevel.BEGINNER else 'ì¤‘ê¸‰ì'}ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡:
        1. ì™œ ì´ ì„¤ê³„ë¥¼ ì„ íƒí–ˆëŠ”ì§€
        2. ê° ìš”ì¸ì´ ì™œ ì¤‘ìš”í•œì§€
        3. ì–´ë–¤ ìˆœì„œë¡œ ì‹¤í—˜í•˜ë©´ ì¢‹ì€ì§€
        4. ì£¼ì˜í•´ì•¼ í•  ì ì€ ë¬´ì—‡ì¸ì§€
        
        ì‰½ê³  ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        """
        
        explanation = await self.ai_orchestrator.query_single(
            'gemini',
            explanation_prompt,
            temperature=0.7,
            user_level=user_level
        )
        
        # ë‹¨ê³„ë³„ ê°€ì´ë“œ ì¶”ê°€
        if user_level == UserLevel.BEGINNER:
            step_by_step_guide = await self._generate_step_by_step_guide(design)
            design['beginner_guide'] = step_by_step_guide
        
        design['explanation'] = explanation.get('response', '')
        design['estimates'] = estimates
        design['user_level'] = user_level.name
        
        return design
    
    async def _generate_step_by_step_guide(self, design: Dict) -> List[Dict]:
        """ì´ˆë³´ìë¥¼ ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œ ìƒì„±"""
        guide = []
        
        # 1. ì¤€ë¹„ ë‹¨ê³„
        guide.append({
            'step': 1,
            'title': 'ì‹¤í—˜ ì¤€ë¹„',
            'tasks': [
                'í•„ìš”í•œ ì¬ë£Œ í™•ì¸ ë° ì£¼ë¬¸',
                'ì¥ë¹„ ì ê²€ ë° ìº˜ë¦¬ë¸Œë ˆì´ì…˜',
                'ì•ˆì „ ì¥ë¹„ ì¤€ë¹„',
                'ì‹¤í—˜ ë…¸íŠ¸ ì¤€ë¹„'
            ],
            'tips': 'ëª¨ë“  ì¬ë£ŒëŠ” ì‹¤í—˜ ì‹œì‘ ì „ì— ì¤€ë¹„í•˜ì„¸ìš”. íŠ¹íˆ ì˜¨ë„ì— ë¯¼ê°í•œ ì¬ë£ŒëŠ” ë³´ê´€ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”.',
            'estimated_time': '1-2ì¼'
        })
        
        # 2. ì²« ì‹¤í—˜
        guide.append({
            'step': 2,
            'title': 'ì²« ë²ˆì§¸ ì‹¤í—˜ (ì¤‘ì‹¬ì )',
            'tasks': [
                'ì¤‘ì‹¬ ì¡°ê±´ ì„¤ì •',
                'ì¥ë¹„ ì„¸íŒ…',
                'ì‹¤í—˜ ìˆ˜í–‰',
                'ë°ì´í„° ê¸°ë¡'
            ],
            'tips': 'ì¤‘ì‹¬ì  ì‹¤í—˜ì€ ì¬í˜„ì„± í™•ì¸ì„ ìœ„í•´ 3íšŒ ë°˜ë³µí•˜ì„¸ìš”.',
            'why': 'ì¤‘ì‹¬ì ì€ ì‹¤í—˜ì˜ ê¸°ì¤€ì´ ë˜ë©°, ì¬í˜„ì„±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        })
        
        # ì¶”ê°€ ë‹¨ê³„ë“¤...
        
        return guide

# ==================== ì‹¤í—˜ ì„¤ê³„ ì „ëµ êµ¬í˜„ ====================
class ScreeningDesignStrategy:
    """ìŠ¤í¬ë¦¬ë‹ ì„¤ê³„ ì „ëµ"""
    
    async def generate_design_matrix(self, 
                                   factors: List[Dict], 
                                   responses: List[Dict],
                                   project_info: Dict) -> pd.DataFrame:
        """ìŠ¤í¬ë¦¬ë‹ ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        n_factors = len(factors)
        
        if n_factors <= 7:
            # Fractional Factorial Design (2^(k-p))
            from pyDOE2 import fracfact
            
            # Resolution IV ì´ìƒ ì„¤ê³„ ì„ íƒ
            if n_factors <= 4:
                design = fracfact(f'2^{n_factors}')  # Full factorial
            elif n_factors <= 7:
                design = fracfact(f'2^({n_factors}-1)')  # Half fraction
            
        else:
            # Plackett-Burman Design
            from pyDOE2 import pbdesign
            design = pbdesign(n_factors)
        
        # ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜
        matrix_data = []
        for run in design:
            row = {}
            for i, factor in enumerate(factors):
                if run[i] == -1:
                    value = factor['min_value']
                else:
                    value = factor['max_value']
                row[factor['name']] = value
            matrix_data.append(row)
        
        # ì¤‘ì‹¬ì  ì¶”ê°€
        center_point = {}
        for factor in factors:
            center_point[factor['name']] = (factor['min_value'] + factor['max_value']) / 2
        
        # ì¤‘ì‹¬ì  3íšŒ ë°˜ë³µ
        for _ in range(3):
            matrix_data.append(center_point.copy())
        
        return pd.DataFrame(matrix_data)

class OptimizationDesignStrategy:
    """ìµœì í™” ì„¤ê³„ ì „ëµ (RSM)"""
    
    async def generate_design_matrix(self,
                                   factors: List[Dict],
                                   responses: List[Dict],
                                   project_info: Dict) -> pd.DataFrame:
        """ë°˜ì‘í‘œë©´ ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        n_factors = len(factors)
        
        if n_factors <= 3:
            # Central Composite Design
            from pyDOE2 import ccdesign
            design = ccdesign(n_factors, alpha='orthogonal', face='circumscribed')
        else:
            # Box-Behnken Design
            from pyDOE2 import bbdesign
            design = bbdesign(n_factors)
        
        # ì½”ë“œí™”ëœ ê°’ì„ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜
        matrix_data = []
        for run in design:
            row = {}
            for i, factor in enumerate(factors):
                # -alpha to +alphaë¥¼ ì‹¤ì œ ë²”ìœ„ë¡œ ë³€í™˜
                coded_value = run[i]
                min_val = factor['min_value']
                max_val = factor['max_value']
                center = (min_val + max_val) / 2
                half_range = (max_val - min_val) / 2
                
                # ì¶• ì ì˜ ê²½ìš° alpha ê°’ ê³ ë ¤
                if abs(coded_value) > 1:
                    alpha = abs(coded_value)
                    actual_value = center + coded_value * half_range / alpha
                else:
                    actual_value = center + coded_value * half_range
                
                row[factor['name']] = round(actual_value, 3)
            matrix_data.append(row)
        
        return pd.DataFrame(matrix_data)

class MixtureDesignStrategy:
    """í˜¼í•©ë¬¼ ì„¤ê³„ ì „ëµ"""
    
    async def generate_design_matrix(self,
                                   factors: List[Dict],
                                   responses: List[Dict],
                                   project_info: Dict) -> pd.DataFrame:
        """í˜¼í•©ë¬¼ ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        # Simplex Lattice Design
        n_components = len(factors)
        
        if n_components <= 4:
            # {3,2} or {4,2} design
            design_points = self._generate_simplex_lattice(n_components, degree=2)
        else:
            # Simplex Centroid Design
            design_points = self._generate_simplex_centroid(n_components)
        
        # ì œì•½ ì¡°ê±´ í™•ì¸
        constraints = project_info.get('mixture_constraints', {})
        filtered_points = self._apply_constraints(design_points, constraints)
        
        # DataFrame ìƒì„±
        matrix_data = []
        for point in filtered_points:
            row = {}
            for i, factor in enumerate(factors):
                row[factor['name']] = round(point[i], 4)
            matrix_data.append(row)
        
        return pd.DataFrame(matrix_data)
    
    def _generate_simplex_lattice(self, n: int, degree: int) -> List[List[float]]:
        """Simplex Lattice ì  ìƒì„±"""
        import itertools
        
        points = []
        
        # ê¼­ì§€ì 
        for i in range(n):
            point = [0] * n
            point[i] = 1
            points.append(point)
        
        if degree >= 2:
            # ëª¨ì„œë¦¬ ì¤‘ì 
            for i, j in itertools.combinations(range(n), 2):
                point = [0] * n
                point[i] = 0.5
                point[j] = 0.5
                points.append(point)
        
        # ì¤‘ì‹¬ì 
        center = [1/n] * n
        points.append(center)
        
        return points
    
    def _generate_simplex_centroid(self, n: int) -> List[List[float]]:
        """Simplex Centroid ì  ìƒì„±"""
        import itertools
        
        points = []
        
        # ëª¨ë“  ë¶€ë¶„ì§‘í•©ì— ëŒ€í•œ ì¤‘ì‹¬ì 
        for r in range(1, n+1):
            for subset in itertools.combinations(range(n), r):
                point = [0] * n
                for idx in subset:
                    point[idx] = 1 / r
                points.append(point)
        
        return points
    
    def _apply_constraints(self, 
                          points: List[List[float]], 
                          constraints: Dict) -> List[List[float]]:
        """í˜¼í•©ë¬¼ ì œì•½ ì¡°ê±´ ì ìš©"""
        filtered = []
        
        for point in points:
            valid = True
            
            # ê°œë³„ ì„±ë¶„ ì œì•½
            for i, value in enumerate(point):
                min_val = constraints.get(f'component_{i}_min', 0)
                max_val = constraints.get(f'component_{i}_max', 1)
                
                if value < min_val or value > max_val:
                    valid = False
                    break
            
            # ë¹„ìœ¨ ì œì•½
            if valid and 'ratio_constraints' in constraints:
                for constraint in constraints['ratio_constraints']:
                    i, j = constraint['components']
                    min_ratio = constraint.get('min_ratio', 0)
                    max_ratio = constraint.get('max_ratio', float('inf'))
                    
                    if point[j] > 0:
                        ratio = point[i] / point[j]
                        if ratio < min_ratio or ratio > max_ratio:
                            valid = False
                            break
            
            if valid:
                filtered.append(point)
        
        return filtered

class RobustDesignStrategy:
    """ê°•ê±´ ì„¤ê³„ ì „ëµ (Taguchi)"""
    
    async def generate_design_matrix(self,
                                   factors: List[Dict],
                                   responses: List[Dict],
                                   project_info: Dict) -> pd.DataFrame:
        """ê°•ê±´ ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        # ì œì–´ ì¸ìì™€ ì¡ìŒ ì¸ì ë¶„ë¦¬
        control_factors = [f for f in factors if not f.get('noise_factor', False)]
        noise_factors = [f for f in factors if f.get('noise_factor', False)]
        
        # Inner array (ì œì–´ ì¸ì)
        if len(control_factors) <= 4:
            inner_array = self._generate_orthogonal_array(len(control_factors), 'L8')
        else:
            inner_array = self._generate_orthogonal_array(len(control_factors), 'L16')
        
        # Outer array (ì¡ìŒ ì¸ì)
        if noise_factors:
            outer_array = self._generate_orthogonal_array(len(noise_factors), 'L4')
        else:
            outer_array = [[0]]  # ì¡ìŒ ì¸ìê°€ ì—†ëŠ” ê²½ìš°
        
        # Cross array ìƒì„±
        matrix_data = []
        for inner_run in inner_array:
            for outer_run in outer_array:
                row = {}
                
                # ì œì–´ ì¸ì ì„¤ì •
                for i, factor in enumerate(control_factors):
                    level = inner_run[i]
                    row[factor['name']] = self._get_factor_level(factor, level)
                
                # ì¡ìŒ ì¸ì ì„¤ì •
                for i, factor in enumerate(noise_factors):
                    level = outer_run[i]
                    row[factor['name']] = self._get_factor_level(factor, level)
                
                matrix_data.append(row)
        
        return pd.DataFrame(matrix_data)
    
    def _generate_orthogonal_array(self, n_factors: int, array_type: str) -> List[List[int]]:
        """ì§êµ ë°°ì—´ ìƒì„±"""
        # ê°„ë‹¨í•œ ì§êµ ë°°ì—´ êµ¬í˜„
        arrays = {
            'L4': [
                [0, 0, 0],
                [0, 1, 1],
                [1, 0, 1],
                [1, 1, 0]
            ],
            'L8': [
                [0, 0, 0, 0, 0, 0, 0],
                [0, 0, 0, 1, 1, 1, 1],
                [0, 1, 1, 0, 0, 1, 1],
                [0, 1, 1, 1, 1, 0, 0],
                [1, 0, 1, 0, 1, 0, 1],
                [1, 0, 1, 1, 0, 1, 0],
                [1, 1, 0, 0, 1, 1, 0],
                [1, 1, 0, 1, 0, 0, 1]
            ],
            'L16': [
                # L16 ë°°ì—´ (ìƒëµ)
            ]
        }
        
        array = arrays.get(array_type, arrays['L4'])
        # n_factorsì— ë§ê²Œ ì—´ ì„ íƒ
        return [row[:n_factors] for row in array]
    
    def _get_factor_level(self, factor: Dict, level: int) -> float:
        """ë ˆë²¨ ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜"""
        if factor.get('categorical', False):
            return factor['categories'][level]
        else:
            levels = factor.get('levels', [factor['min_value'], factor['max_value']])
            return levels[level]

class AdaptiveDesignStrategy:
    """ì ì‘í˜• ì„¤ê³„ ì „ëµ"""
    
    def __init__(self):
        self.surrogate_model = None
        self.acquisition_function = 'EI'  # Expected Improvement
        self.exploration_rate = 0.1
        
    async def generate_design_matrix(self,
                                   factors: List[Dict],
                                   responses: List[Dict],
                                   project_info: Dict) -> pd.DataFrame:
        """ì ì‘í˜• ì„¤ê³„ - ì´ˆê¸° ì„¤ê³„ë§Œ ìƒì„±"""
        # ì´ˆê¸° ì„¤ê³„ëŠ” Latin Hypercube Sampling
        n_initial = max(10, 2 * len(factors))
        
        from pyDOE2 import lhs
        lhs_design = lhs(len(factors), samples=n_initial, criterion='maximin')
        
        # ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜
        matrix_data = []
        for run in lhs_design:
            row = {}
            for i, factor in enumerate(factors):
                min_val = factor['min_value']
                max_val = factor['max_value']
                actual_value = min_val + run[i] * (max_val - min_val)
                row[factor['name']] = round(actual_value, 3)
            matrix_data.append(row)
        
        df = pd.DataFrame(matrix_data)
        df['experiment_type'] = 'initial'
        df['suggested_order'] = range(1, len(df) + 1)
        
        return df
    
    async def suggest_next_experiment(self,
                                    current_data: pd.DataFrame,
                                    factors: List[Dict],
                                    response_name: str) -> Dict:
        """ë‹¤ìŒ ì‹¤í—˜ì  ì œì•ˆ"""
        # ì„œë¡œê²Œì´íŠ¸ ëª¨ë¸ í•™ìŠµ
        X = current_data[[f['name'] for f in factors]]
        y = current_data[response_name]
        
        if self.surrogate_model is None:
            from sklearn.gaussian_process import GaussianProcessRegressor
            from sklearn.gaussian_process.kernels import Matern
            
            kernel = Matern(length_scale=1.0, nu=2.5)
            self.surrogate_model = GaussianProcessRegressor(
                kernel=kernel,
                alpha=1e-6,
                normalize_y=True
            )
        
        self.surrogate_model.fit(X, y)
        
        # íšë“ í•¨ìˆ˜ ìµœì í™”
        next_point = await self._optimize_acquisition(X, y, factors)
        
        return {
            'next_experiment': next_point,
            'expected_improvement': self._calculate_ei(next_point, X, y),
            'uncertainty': self._calculate_uncertainty(next_point)
        }
    
    async def _optimize_acquisition(self, X, y, factors):
        """íšë“ í•¨ìˆ˜ ìµœì í™”"""
        from scipy.optimize import differential_evolution
        
        def acquisition(x):
            x_array = np.array(x).reshape(1, -1)
            
            if self.acquisition_function == 'EI':
                return -self._expected_improvement(x_array, X, y)
            elif self.acquisition_function == 'UCB':
                return -self._upper_confidence_bound(x_array, X, y)
            else:
                return -self._probability_of_improvement(x_array, X, y)
        
        bounds = [(f['min_value'], f['max_value']) for f in factors]
        
        result = differential_evolution(
            acquisition,
            bounds,
            seed=42,
            maxiter=100
        )
        
        next_point = {}
        for i, factor in enumerate(factors):
            next_point[factor['name']] = round(result.x[i], 3)
        
        return next_point
    
    def _expected_improvement(self, x, X_obs, y_obs):
        """Expected Improvement ê³„ì‚°"""
        mu, sigma = self.surrogate_model.predict(x, return_std=True)
        
        # í˜„ì¬ ìµœì ê°’
        f_best = y_obs.max()
        
        # EI ê³„ì‚°
        with np.errstate(divide='warn'):
            imp = mu - f_best
            Z = imp / sigma
            ei = imp * stats.norm.cdf(Z) + sigma * stats.norm.pdf(Z)
            ei[sigma == 0.0] = 0.0
        
        return ei
    
    def _calculate_uncertainty(self, point):
        """ë¶ˆí™•ì‹¤ì„± ê³„ì‚°"""
        x_array = np.array(list(point.values())).reshape(1, -1)
        _, std = self.surrogate_model.predict(x_array, return_std=True)
        return float(std[0])

# ==================== ì„¤ê³„ ê²€ì¦ê¸° ====================
class DesignValidator:
    """ì‹¤í—˜ ì„¤ê³„ ê²€ì¦ê¸°"""
    
    async def validate_statistical_properties(self, 
                                           design: Dict, 
                                           project_info: Dict) -> Dict:
        """í†µê³„ì  ì†ì„± ê²€ì¦"""
        validation_result = {
            'is_valid': True,
            'score': 1.0,
            'issues': [],
            'suggestions': []
        }
        
        matrix = design.get('matrix', pd.DataFrame())
        if matrix.empty:
            validation_result['is_valid'] = False
            validation_result['issues'].append("ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return validation_result
        
        # 1. ì‹¤í—˜ ìˆ˜ í™•ì¸
        n_experiments = len(matrix)
        n_factors = len(design.get('factors', []))
        min_experiments = self._calculate_min_experiments(n_factors, design.get('strategy'))
        
        if n_experiments < min_experiments:
            validation_result['score'] *= 0.7
            validation_result['issues'].append(
                f"ì‹¤í—˜ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ {min_experiments}ê°œ í•„ìš” (í˜„ì¬: {n_experiments}ê°œ)"
            )
            validation_result['suggestions'].append(
                f"í†µê³„ì  ìœ ì˜ì„±ì„ ìœ„í•´ {min_experiments - n_experiments}ê°œì˜ ì‹¤í—˜ì„ ì¶”ê°€í•˜ì„¸ìš”."
            )
        
        # 2. ê· í˜•ì„± í™•ì¸
        balance_score = await self._check_design_balance(matrix, design.get('factors', []))
        if balance_score < 0.8:
            validation_result['score'] *= balance_score
            validation_result['issues'].append("ì„¤ê³„ê°€ ë¶ˆê· í˜•í•©ë‹ˆë‹¤.")
            validation_result['suggestions'].append("ê° ìš”ì¸ì˜ ìˆ˜ì¤€ì´ ê· ë“±í•˜ê²Œ ë¶„í¬í•˜ë„ë¡ ì¡°ì •í•˜ì„¸ìš”.")
        
        # 3. ì§êµì„± í™•ì¸
        orthogonality_score = await self._check_orthogonality(matrix, design.get('factors', []))
        if orthogonality_score < 0.8:
            validation_result['score'] *= orthogonality_score
            validation_result['issues'].append("ìš”ì¸ ê°„ ìƒê´€ê´€ê³„ê°€ ë†’ìŠµë‹ˆë‹¤.")
            validation_result['suggestions'].append("ì§êµ ì„¤ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì¸ íš¨ê³¼ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì¶”ì •í•˜ì„¸ìš”.")
        
        # 4. ê²€ì •ë ¥ ë¶„ì„
        power_analysis = await self._power_analysis(design, project_info)
        if power_analysis['power'] < 0.8:
            validation_result['score'] *= power_analysis['power']
            validation_result['issues'].append(f"í†µê³„ì  ê²€ì •ë ¥ì´ ë‚®ìŠµë‹ˆë‹¤ ({power_analysis['power']:.2f})")
            validation_result['suggestions'].append(
                f"ê²€ì •ë ¥ 0.8 ë‹¬ì„±ì„ ìœ„í•´ {power_analysis['required_n']}ê°œì˜ ì‹¤í—˜ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )
        
        validation_result['details'] = {
            'balance_score': balance_score,
            'orthogonality_score': orthogonality_score,
            'power': power_analysis['power']
        }
        
        return validation_result

    def _calculate_min_experiments(self, n_factors: int, strategy: str) -> int:
        """ìµœì†Œ ì‹¤í—˜ ìˆ˜ ê³„ì‚°"""
        min_experiments = {
            'screening': max(n_factors + 1, 8),
            'optimization': max(n_factors * 2 + 1, 15),
            'mixture': max(n_factors * (n_factors + 1) // 2, 10),
            'robust': max(n_factors * 4, 16),
            'adaptive': max(n_factors * 2, 10),
            'sequential': max(n_factors + 5, 12)
        }
        
        return min_experiments.get(strategy, n_factors * 3)
    
    async def _check_design_balance(self, matrix: pd.DataFrame, factors: List[Dict]) -> float:
        """ì„¤ê³„ ê· í˜•ì„± í™•ì¸"""
        balance_scores = []
        
        for factor in factors:
            factor_name = factor['name']
            if factor_name not in matrix.columns:
                continue
            
            if factor.get('categorical', False):
                # ë²”ì£¼í˜• ìš”ì¸ì˜ ê· í˜•
                value_counts = matrix[factor_name].value_counts()
                expected_count = len(matrix) / len(factor['categories'])
                
                chi2, p_value = stats.chisquare(value_counts.values)
                balance_score = 1 - (chi2 / len(matrix))
            else:
                # ì—°ì†í˜• ìš”ì¸ì˜ ë¶„í¬
                values = matrix[factor_name].values
                min_val, max_val = factor['min_value'], factor['max_value']
                
                # ë²”ìœ„ ë‚´ ê· ë“± ë¶„í¬ í™•ì¸
                hist, _ = np.histogram(values, bins=5)
                expected_freq = len(values) / 5
                chi2, p_value = stats.chisquare(hist, f_exp=[expected_freq] * 5)
                balance_score = p_value  # pê°’ì´ ë†’ì„ìˆ˜ë¡ ê· ë“±
            
            balance_scores.append(max(0, min(1, balance_score)))
        
        return np.mean(balance_scores) if balance_scores else 0.5
    
    async def _check_orthogonality(self, matrix: pd.DataFrame, factors: List[Dict]) -> float:
        """ì§êµì„± í™•ì¸"""
        factor_names = [f['name'] for f in factors if f['name'] in matrix.columns]
        
        if len(factor_names) < 2:
            return 1.0
        
        # ìƒê´€ í–‰ë ¬ ê³„ì‚°
        numeric_matrix = matrix[factor_names].apply(pd.to_numeric, errors='coerce')
        corr_matrix = numeric_matrix.corr().abs()
        
        # ëŒ€ê°ì„  ì œì™¸í•œ ìƒê´€ê³„ìˆ˜ë“¤
        upper_triangle = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        correlations = upper_triangle.stack().values
        
        # í‰ê·  ìƒê´€ê³„ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì§êµì„±ì´ ë†’ìŒ
        avg_correlation = np.mean(correlations)
        orthogonality_score = 1 - avg_correlation
        
        return orthogonality_score
    
    async def _power_analysis(self, design: Dict, project_info: Dict) -> Dict:
        """í†µê³„ì  ê²€ì •ë ¥ ë¶„ì„"""
        import statsmodels.stats.power as smp
        
        n_experiments = len(design.get('matrix', []))
        n_factors = len(design.get('factors', []))
        
        # íš¨ê³¼ í¬ê¸° ì¶”ì •
        effect_size = project_info.get('expected_effect_size', 0.5)
        alpha = project_info.get('significance_level', 0.05)
        
        # ANOVA ê²€ì •ë ¥ ê³„ì‚°
        try:
            power = smp.FTestAnovaPower().solve_power(
                effect_size=effect_size,
                nobs=n_experiments,
                alpha=alpha,
                k_groups=n_factors + 1
            )
            
            # í•„ìš”í•œ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
            required_n = smp.FTestAnovaPower().solve_power(
                effect_size=effect_size,
                power=0.8,
                alpha=alpha,
                k_groups=n_factors + 1
            )
            
            required_n = int(np.ceil(required_n))
        except:
            # ê·¼ì‚¬ ê³„ì‚°
            power = min(0.9, n_experiments / (n_factors * 5))
            required_n = max(n_experiments, n_factors * 5)
        
        return {
            'power': power,
            'required_n': required_n,
            'current_n': n_experiments,
            'effect_size': effect_size,
            'alpha': alpha
        }
    
    async def validate_practical_constraints(self, 
                                          design: Dict, 
                                          project_info: Dict) -> Dict:
        """ì‹¤ìš©ì„± ì œì•½ ê²€ì¦"""
        validation_result = {
            'is_valid': True,
            'score': 1.0,
            'issues': [],
            'suggestions': []
        }
        
        matrix = design.get('matrix', pd.DataFrame())
        
        # 1. ì‹œê°„ ì œì•½ í™•ì¸
        total_time = await self._estimate_total_time(matrix, design)
        available_time = project_info.get('timeline', 4) * 7 * 24  # ì£¼ -> ì‹œê°„
        
        if total_time > available_time:
            validation_result['score'] *= 0.7
            validation_result['issues'].append(
                f"ì˜ˆìƒ ì‹¤í—˜ ì‹œê°„({total_time:.1f}ì‹œê°„)ì´ ê°€ìš© ì‹œê°„({available_time}ì‹œê°„)ì„ ì´ˆê³¼í•©ë‹ˆë‹¤."
            )
            validation_result['suggestions'].append(
                "ë³‘ë ¬ ì‹¤í—˜ ìˆ˜í–‰ ë˜ëŠ” ì‹¤í—˜ ìˆ˜ ê°ì†Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # 2. ì¥ë¹„ ì œì•½ í™•ì¸
        equipment_issues = await self._check_equipment_constraints(design, project_info)
        if equipment_issues:
            validation_result['score'] *= 0.8
            validation_result['issues'].extend(equipment_issues)
            validation_result['suggestions'].append(
                "ì¥ë¹„ ì‚¬ìš© ìŠ¤ì¼€ì¤„ì„ ìµœì í™”í•˜ê±°ë‚˜ ëŒ€ì²´ ì¥ë¹„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # 3. ì¬ë£Œ ê°€ìš©ì„± í™•ì¸
        material_issues = await self._check_material_availability(design, project_info)
        if material_issues:
            validation_result['score'] *= 0.9
            validation_result['issues'].extend(material_issues)
        
        # 4. ì‘ì—… ìˆœì„œ ìµœì í™”
        optimized_sequence = await self._optimize_experiment_sequence(matrix, design)
        validation_result['optimized_sequence'] = optimized_sequence
        
        return validation_result
    
    async def _estimate_total_time(self, matrix: pd.DataFrame, design: Dict) -> float:
        """ì´ ì‹¤í—˜ ì‹œê°„ ì¶”ì •"""
        base_time_per_experiment = 4  # ê¸°ë³¸ 4ì‹œê°„
        
        # ìš”ì¸ë³„ ì¶”ê°€ ì‹œê°„
        time_factors = {
            'temperature': 2,  # ì˜¨ë„ ì•ˆì •í™”
            'reaction_time': 1,  # ë°˜ì‘ ì‹œê°„
            'analysis': 1.5  # ë¶„ì„ ì‹œê°„
        }
        
        total_time = len(matrix) * base_time_per_experiment
        
        # ì¶”ê°€ ì‹œê°„ ê³„ì‚°
        for factor in design.get('factors', []):
            if any(keyword in factor['name'].lower() for keyword in time_factors.keys()):
                total_time += len(matrix) * time_factors.get(
                    next(k for k in time_factors if k in factor['name'].lower()), 
                    0
                )
        
        return total_time
    
    async def _check_equipment_constraints(self, 
                                         design: Dict, 
                                         project_info: Dict) -> List[str]:
        """ì¥ë¹„ ì œì•½ í™•ì¸"""
        issues = []
        available_equipment = project_info.get('equipment', [])
        
        # í•„ìš” ì¥ë¹„ ì¶”ë¡ 
        required_equipment = set()
        
        for factor in design.get('factors', []):
            factor_name_lower = factor['name'].lower()
            
            if 'temperature' in factor_name_lower:
                required_equipment.add('ì˜¨ë„ ì¡°ì ˆ ì¥ì¹˜')
            if 'pressure' in factor_name_lower:
                required_equipment.add('ì••ë ¥ ì¡°ì ˆ ì¥ì¹˜')
            if 'mixing' in factor_name_lower or 'rpm' in factor_name_lower:
                required_equipment.add('êµë°˜ê¸°')
            
        for response in design.get('responses', []):
            response_name_lower = response['name'].lower()
            
            if 'tensile' in response_name_lower:
                required_equipment.add('ë§ŒëŠ¥ì‹œí—˜ê¸°')
            if 'thermal' in response_name_lower or 'dsc' in response_name_lower:
                required_equipment.add('DSC')
            if 'molecular' in response_name_lower:
                required_equipment.add('GPC')
        
        # ëˆ„ë½ëœ ì¥ë¹„ í™•ì¸
        missing_equipment = required_equipment - set(available_equipment)
        
        if missing_equipment:
            issues.append(f"í•„ìš”í•œ ì¥ë¹„ê°€ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_equipment)}")
        
        return issues
    
    async def validate_safety(self, design: Dict, project_info: Dict) -> Dict:
        """ì•ˆì „ì„± ê²€ì¦"""
        validation_result = {
            'is_valid': True,
            'score': 1.0,
            'issues': [],
            'safety_requirements': [],
            'risk_level': 'low'
        }
        
        # 1. í™”í•™ë¬¼ì§ˆ ìœ„í—˜ì„± í‰ê°€
        chemical_risks = await self._assess_chemical_risks(design, project_info)
        if chemical_risks['level'] != 'low':
            validation_result['score'] *= 0.8
            validation_result['issues'].extend(chemical_risks['issues'])
            validation_result['safety_requirements'].extend(chemical_risks['requirements'])
            validation_result['risk_level'] = chemical_risks['level']
        
        # 2. ê³µì • ì¡°ê±´ ìœ„í—˜ì„± í‰ê°€
        process_risks = await self._assess_process_risks(design)
        if process_risks['level'] != 'low':
            validation_result['score'] *= 0.85
            validation_result['issues'].extend(process_risks['issues'])
            validation_result['safety_requirements'].extend(process_risks['requirements'])
        
        # 3. ì•ˆì „ í”„ë¡œí† ì½œ ìƒì„±
        safety_protocol = await self._generate_safety_protocol(
            chemical_risks, 
            process_risks,
            design
        )
        validation_result['safety_protocol'] = safety_protocol
        
        # ì „ì²´ ìœ„í—˜ë„ ê²°ì •
        if validation_result['score'] < 0.7:
            validation_result['is_valid'] = False
            validation_result['risk_level'] = 'high'
        elif validation_result['score'] < 0.85:
            validation_result['risk_level'] = 'medium'
        
        return validation_result
    
    async def _assess_chemical_risks(self, design: Dict, project_info: Dict) -> Dict:
        """í™”í•™ë¬¼ì§ˆ ìœ„í—˜ì„± í‰ê°€"""
        risks = {
            'level': 'low',
            'issues': [],
            'requirements': []
        }
        
        polymer_type = project_info.get('polymer_type', '').lower()
        
        # ê³ ë¶„ìë³„ ìœ„í—˜ì„± ë°ì´í„°ë² ì´ìŠ¤
        hazard_data = {
            'epoxy': {
                'hazards': ['í”¼ë¶€ ìê·¹ì„±', 'ì•Œë ˆë¥´ê¸° ìœ ë°œ'],
                'requirements': ['ì¥ê°‘ ì°©ìš©', 'í™˜ê¸° í•„ìˆ˜']
            },
            'polyurethane': {
                'hazards': ['ì´ì†Œì‹œì•„ë„¤ì´íŠ¸ ë…¸ì¶œ ìœ„í—˜'],
                'requirements': ['í„í›„ë“œ ì‘ì—…', 'í˜¸í¡ë³´í˜¸êµ¬ ì°©ìš©']
            },
            'vinyl': {
                'hazards': ['ë°œì•” ê°€ëŠ¥ì„±'],
                'requirements': ['ë°€í ì‹œìŠ¤í…œ', 'ê°œì¸ë³´í˜¸êµ¬ ì°©ìš©']
            }
        }
        
        # ìœ„í—˜ì„± í™•ì¸
        for key, data in hazard_data.items():
            if key in polymer_type:
                risks['level'] = 'medium'
                risks['issues'].extend(data['hazards'])
                risks['requirements'].extend(data['requirements'])
        
        # ì˜¨ë„ ì¡°ê±´ í™•ì¸
        for factor in design.get('factors', []):
            if 'temperature' in factor['name'].lower():
                max_temp = factor.get('max_value', 0)
                if max_temp > 200:
                    risks['level'] = 'high' if risks['level'] == 'medium' else 'medium'
                    risks['issues'].append(f"ê³ ì˜¨ ì‘ì—… ({max_temp}Â°C)")
                    risks['requirements'].append("ë‚´ì—´ ì¥ê°‘ ë° ë³´í˜¸êµ¬ ì°©ìš©")
        
        return risks
    
    async def _generate_safety_protocol(self, 
                                      chemical_risks: Dict,
                                      process_risks: Dict,
                                      design: Dict) -> Dict:
        """ì•ˆì „ í”„ë¡œí† ì½œ ìƒì„±"""
        protocol = {
            'ppe_requirements': set(),
            'engineering_controls': set(),
            'emergency_procedures': [],
            'waste_disposal': []
        }
        
        # PPE ìš”êµ¬ì‚¬í•­
        if chemical_risks['level'] != 'low':
            protocol['ppe_requirements'].update(['ì•ˆì „ê³ ê¸€', 'ì‹¤í—˜ë³µ', 'ì•ˆì „í™”'])
            protocol['ppe_requirements'].update(
                req for req in chemical_risks['requirements'] if 'ì°©ìš©' in req
            )
        
        # ê³µí•™ì  ì œì–´
        if process_risks['level'] != 'low':
            protocol['engineering_controls'].update(
                req for req in process_risks['requirements'] if 'ì‹œìŠ¤í…œ' in req or 'í™˜ê¸°' in req
            )
        
        # ë¹„ìƒ ì ˆì°¨
        if chemical_risks['level'] == 'high' or process_risks['level'] == 'high':
            protocol['emergency_procedures'].extend([
                "ë¹„ìƒ ìƒ¤ì›Œ ë° ëˆˆ ì„¸ì²™ ì‹œì„¤ ìœ„ì¹˜ í™•ì¸",
                "í™”ì¬ ì§„ì••ê¸° ìœ„ì¹˜ ë° ì‚¬ìš©ë²• ìˆ™ì§€",
                "ë¹„ìƒ ì—°ë½ì²˜ ê²Œì‹œ"
            ])
        
        # íê¸°ë¬¼ ì²˜ë¦¬
        protocol['waste_disposal'].extend([
            "ê³ ë¶„ì íê¸°ë¬¼: ì§€ì • ìš©ê¸°ì— ë¶„ë¦¬ ìˆ˜ê±°",
            "ìš©ë§¤ íê¸°ë¬¼: í• ë¡œê²/ë¹„í• ë¡œê² ë¶„ë¦¬",
            "ê³ í˜• íê¸°ë¬¼: ì˜¤ì—¼/ë¹„ì˜¤ì—¼ ë¶„ë¦¬"
        ])
        
        return protocol

# Polymer-doe-platform - Part 10
# ==================== ë¹„ìš© ì¶”ì •ê¸° ====================
class CostEstimator:
    """ì‹¤í—˜ ë¹„ìš© ì¶”ì •ê¸°"""
    
    def __init__(self):
        self.material_prices = {
            # ì¼ë°˜ ê³ ë¶„ì (ì›/kg)
            'PE': 2000, 'PP': 2500, 'PS': 3000, 'PVC': 2800,
            'PET': 3500, 'PA': 8000, 'PC': 12000, 'PMMA': 7000,
            # íŠ¹ìˆ˜ ê³ ë¶„ì
            'PEEK': 150000, 'PPS': 25000, 'PSU': 35000,
            # ì²¨ê°€ì œ
            'carbon_black': 3000, 'glass_fiber': 5000, 'talc': 1000
        }
        
        self.analysis_costs = {
            # ë¶„ì„ ë¹„ìš© (ì›/ìƒ˜í”Œ)
            'DSC': 50000, 'TGA': 50000, 'DMA': 80000,
            'GPC': 100000, 'FTIR': 30000, 'XRD': 60000,
            'SEM': 100000, 'TEM': 200000, 'AFM': 150000,
            'UTM': 40000, 'Impact': 30000, 'HDT': 40000
        }
    
    async def estimate(self, design: Dict, project_info: Dict) -> Dict:
        """ì´ ë¹„ìš© ì¶”ì •"""
        material_cost = await self._estimate_material_cost(design, project_info)
        analysis_cost = await self._estimate_analysis_cost(design, project_info)
        labor_cost = await self._estimate_labor_cost(design, project_info)
        overhead_cost = (material_cost + analysis_cost + labor_cost) * 0.3
        
        total_cost = material_cost + analysis_cost + labor_cost + overhead_cost
        
        return {
            'material_cost': material_cost / 10000,  # ë§Œì› ë‹¨ìœ„
            'analysis_cost': analysis_cost / 10000,
            'labor_cost': labor_cost / 10000,
            'overhead_cost': overhead_cost / 10000,
            'total_cost': total_cost / 10000,
            'cost_breakdown': {
                'materials': self._get_material_breakdown(design, project_info),
                'analyses': self._get_analysis_breakdown(design)
            },
            'cost_per_experiment': total_cost / len(design.get('matrix', [1])) / 10000
        }
    
    async def _estimate_material_cost(self, design: Dict, project_info: Dict) -> float:
        """ì¬ë£Œë¹„ ì¶”ì •"""
        polymer_type = project_info.get('polymer_type', 'PE')
        base_price = self.material_prices.get(polymer_type.upper(), 5000)
        
        # ì‹¤í—˜ë‹¹ í•„ìš”ëŸ‰ (kg)
        sample_weight = project_info.get('sample_weight', 0.1)  # ê¸°ë³¸ 100g
        n_experiments = len(design.get('matrix', []))
        
        # ì—¬ìœ ë¶„ í¬í•¨ (20%)
        total_weight = sample_weight * n_experiments * 1.2
        
        # ê¸°ë³¸ ì¬ë£Œë¹„
        material_cost = total_weight * base_price
        
        # ì²¨ê°€ì œ ë¹„ìš©
        for factor in design.get('factors', []):
            if 'filler' in factor['name'].lower() or 'additive' in factor['name'].lower():
                additive_cost = total_weight * 0.1 * 5000  # ì²¨ê°€ì œ 10% ê°€ì •
                material_cost += additive_cost
        
        return material_cost

# ==================== ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ì‹œìŠ¤í…œ (ì´ ì •ë¦¬) ====================
# ==================== ê³ ë¶„ì ë°ì´í„°ë² ì´ìŠ¤ ====================
class PolymerDatabase:
    """ê³ ë¶„ì ë°ì´í„°ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.polymers_data = {
            'PET': {
                'name': 'í´ë¦¬ì—í‹¸ë Œ í…Œë ˆí”„íƒˆë ˆì´íŠ¸',
                'type': 'ì—´ê°€ì†Œì„±',
                'properties': {
                    'Tg': 75,  # Â°C
                    'Tm': 260,  # Â°C
                    'density': 1.38,  # g/cmÂ³
                    'tensile_strength': 55  # MPa
                },
                'applications': ['ë³‘', 'ì„¬ìœ ', 'í•„ë¦„'],
                'processing': ['ì‚¬ì¶œì„±í˜•', 'ë¸”ë¡œìš°ì„±í˜•', 'ì••ì¶œ']
            },
            'PP': {
                'name': 'í´ë¦¬í”„ë¡œí•„ë Œ',
                'type': 'ì—´ê°€ì†Œì„±',
                'properties': {
                    'Tg': -10,
                    'Tm': 165,
                    'density': 0.90,
                    'tensile_strength': 35
                },
                'applications': ['í¬ì¥ì¬', 'ìë™ì°¨ë¶€í’ˆ', 'ì„¬ìœ '],
                'processing': ['ì‚¬ì¶œì„±í˜•', 'ì••ì¶œ', 'ë¸”ë¡œìš°ì„±í˜•']
            },
            'PMMA': {
                'name': 'í´ë¦¬ë©”í‹¸ë©”íƒ€í¬ë¦´ë ˆì´íŠ¸',
                'type': 'ì—´ê°€ì†Œì„±',
                'properties': {
                    'Tg': 105,
                    'Tm': None,  # ë¹„ê²°ì •ì„±
                    'density': 1.18,
                    'tensile_strength': 70
                },
                'applications': ['ê´‘í•™ì¬ë£Œ', 'ê°„íŒ', 'ì¡°ëª…'],
                'processing': ['ì‚¬ì¶œì„±í˜•', 'ì••ì¶œ', 'ìºìŠ¤íŒ…']
            }
        }
        
        self.search_index = self._build_search_index()
    
    def _build_search_index(self):
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        index = {}
        for polymer_id, data in self.polymers_data.items():
            # ì´ë¦„ìœ¼ë¡œ ì¸ë±ì‹±
            index[data['name'].lower()] = polymer_id
            # ì•½ì–´ë¡œ ì¸ë±ì‹±
            index[polymer_id.lower()] = polymer_id
            # ì‘ìš©ë¶„ì•¼ë¡œ ì¸ë±ì‹±
            for app in data['applications']:
                if app not in index:
                    index[app] = []
                if polymer_id not in index[app]:
                    index[app].append(polymer_id)
        return index
    
    def search(self, query: str) -> List[Dict]:
        """ê³ ë¶„ì ê²€ìƒ‰"""
        query_lower = query.lower()
        results = []
        
        # ì§ì ‘ ë§¤ì¹­
        if query_lower in self.search_index:
            match = self.search_index[query_lower]
            if isinstance(match, str):
                results.append(self.get_polymer(match))
            elif isinstance(match, list):
                for polymer_id in match:
                    results.append(self.get_polymer(polymer_id))
        
        # ë¶€ë¶„ ë§¤ì¹­
        for key, value in self.search_index.items():
            if query_lower in key and key != query_lower:
                if isinstance(value, str):
                    polymer = self.get_polymer(value)
                    if polymer and polymer not in results:
                        results.append(polymer)
        
        return results
    
    def get_polymer(self, polymer_id: str) -> Optional[Dict]:
        """ê³ ë¶„ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if polymer_id in self.polymers_data:
            return {
                'id': polymer_id,
                **self.polymers_data[polymer_id]
            }
        return None
    
    def get_all_polymers(self) -> List[Dict]:
        """ëª¨ë“  ê³ ë¶„ì ëª©ë¡"""
        return [
            {'id': pid, **data} 
            for pid, data in self.polymers_data.items()
        ]

# ==================== í”„ë¡œì íŠ¸ í…œí”Œë¦¿ ====================
class ProjectTemplates:
    """í”„ë¡œì íŠ¸ í…œí”Œë¦¿ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.templates = {
            'packaging': {
                'name': 'í¬ì¥ì¬ ê°œë°œ',
                'factors': ['ë‘ê»˜', 'ì²¨ê°€ì œ í•¨ëŸ‰', 'ê°€ê³µì˜¨ë„'],
                'responses': ['ì¸ì¥ê°•ë„', 'íˆ¬ëª…ë„', 'ì‚°ì†Œíˆ¬ê³¼ë„'],
                'typical_budget': 500,
                'typical_timeline': 8
            },
            'automotive': {
                'name': 'ìë™ì°¨ ë¶€í’ˆ',
                'factors': ['ìœ ë¦¬ì„¬ìœ  í•¨ëŸ‰', 'ì„±í˜•ì˜¨ë„', 'ëƒ‰ê°ì‹œê°„'],
                'responses': ['ì¶©ê²©ê°•ë„', 'ì¹˜ìˆ˜ì•ˆì •ì„±', 'ë‚´ì—´ì„±'],
                'typical_budget': 1000,
                'typical_timeline': 12
            },
            'biomedical': {
                'name': 'ì˜ë£Œìš© ì†Œì¬',
                'factors': ['ê°€êµë„', 'pH', 'ë©¸ê· ë°©ë²•'],
                'responses': ['ìƒì²´ì í•©ì„±', 'ë¶„í•´ì†ë„', 'ê¸°ê³„ì  íŠ¹ì„±'],
                'typical_budget': 2000,
                'typical_timeline': 16
            }
        }
    
    def get_template(self, template_id: str) -> Optional[Dict]:
        """í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°"""
        return self.templates.get(template_id)
    
    def get_all_templates(self) -> Dict:
        """ëª¨ë“  í…œí”Œë¦¿"""
        return self.templates

# ==================== ìš”ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ====================
class FactorLibrary:
    """ì‹¤í—˜ ìš”ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬"""
    
    def __init__(self):
        self.factors = {
            'temperature': {
                'name': 'ì˜¨ë„',
                'unit': 'Â°C',
                'typical_range': [20, 300],
                'description': 'ê°€ê³µ ë˜ëŠ” ë°˜ì‘ ì˜¨ë„'
            },
            'time': {
                'name': 'ì‹œê°„',
                'unit': 'min',
                'typical_range': [1, 240],
                'description': 'ë°˜ì‘ ë˜ëŠ” ê°€ê³µ ì‹œê°„'
            },
            'pressure': {
                'name': 'ì••ë ¥',
                'unit': 'MPa',
                'typical_range': [0.1, 50],
                'description': 'ê°€ê³µ ì••ë ¥'
            },
            'concentration': {
                'name': 'ë†ë„',
                'unit': 'wt%',
                'typical_range': [0, 100],
                'description': 'ì²¨ê°€ì œ ë˜ëŠ” ìš©ì§ˆ ë†ë„'
            }
        }
    
    def get_factor(self, factor_id: str) -> Optional[Dict]:
        """ìš”ì¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        return self.factors.get(factor_id)
    
    def get_all_factors(self) -> Dict:
        """ëª¨ë“  ìš”ì¸"""
        return self.factors

# ==================== ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ì‹œìŠ¤í…œ ====================
class UserInterfaceSystem:
    """Streamlit ê¸°ë°˜ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.pages = {
            'home': HomePage(),
            'project_setup': ProjectSetupPage(),
            'experiment_design': ExperimentDesignPage(),
            'data_analysis': DataAnalysisPage(),
            'results_visualization': ResultsVisualizationPage(),
            'learning_center': LearningCenterPage(),
            'collaboration': CollaborationPage()
        }
        self.current_user_level = UserLevel.BEGINNER
        self.help_system = HelpSystem()
        self.tutorial_system = TutorialSystem()
        
    def render(self):
        """ë©”ì¸ UI ë Œë”ë§"""
        # ì‚¬ì´ë“œë°” ì„¤ì •
        self._render_sidebar()
        
        # ë©”ì¸ í˜ì´ì§€ ë Œë”ë§
        page = st.session_state.get('current_page', 'home')
        if page in self.pages:
            self.pages[page].render(self.current_user_level)
        
        # ë„ì›€ë§ ì‹œìŠ¤í…œ
        if self.current_user_level in [UserLevel.BEGINNER, UserLevel.INTERMEDIATE]:
            self.help_system.render_contextual_help(page)
    
    def _render_sidebar(self):
        """ì‚¬ì´ë“œë°” ë Œë”ë§"""
        with st.sidebar:
            st.title("ğŸ§¬ ê³ ë¶„ì ì‹¤í—˜ ì„¤ê³„ í”Œë«í¼")
            
            # ì‚¬ìš©ì ë ˆë²¨ ì„ íƒ
            st.markdown("### ğŸ‘¤ ì‚¬ìš©ì ë ˆë²¨")
            level_names = {
                UserLevel.BEGINNER: "ğŸŒ± ì´ˆë³´ì",
                UserLevel.INTERMEDIATE: "ğŸŒ¿ ì¤‘ê¸‰ì",
                UserLevel.ADVANCED: "ğŸŒ³ ê³ ê¸‰ì",
                UserLevel.EXPERT: "ğŸ“ ì „ë¬¸ê°€"
            }
            
            selected_level = st.selectbox(
                "í˜„ì¬ ë ˆë²¨",
                options=list(level_names.keys()),
                format_func=lambda x: level_names[x],
                key='user_level'
            )
            self.current_user_level = selected_level
            
            # ë„¤ë¹„ê²Œì´ì…˜
            st.markdown("### ğŸ“ ë„¤ë¹„ê²Œì´ì…˜")
            page_names = {
                'home': "ğŸ  í™ˆ",
                'project_setup': "ğŸ“‹ í”„ë¡œì íŠ¸ ì„¤ì •",
                'experiment_design': "ğŸ”¬ ì‹¤í—˜ ì„¤ê³„",
                'data_analysis': "ğŸ“Š ë°ì´í„° ë¶„ì„",
                'results_visualization': "ğŸ“ˆ ê²°ê³¼ ì‹œê°í™”",
                'learning_center': "ğŸ“š í•™ìŠµ ì„¼í„°",
                'collaboration': "ğŸ‘¥ í˜‘ì—…"
            }
            
            for page_key, page_name in page_names.items():
                if st.button(page_name, key=f"nav_{page_key}"):
                    st.session_state.current_page = page_key
                    st.rerun()
            
            # AI ìƒíƒœ
            st.markdown("### ğŸ¤– AI ì‹œìŠ¤í…œ ìƒíƒœ")
            if hasattr(st.session_state, 'ai_orchestrator'):
                engine_status = st.session_state.ai_orchestrator.get_engine_status()
                for engine, status in engine_status.items():
                    if status['available']:
                        st.success(f"âœ… {engine}")
                    else:
                        st.error(f"âŒ {engine}")
            else:
                st.info("AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
            st.markdown("### ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ")
            if hasattr(st.session_state, 'db_manager'):
                for db_name, client in st.session_state.db_manager.databases.items():
                    if client.is_available:
                        st.success(f"âœ… {db_name}")
                    else:
                        st.warning(f"âš ï¸ {db_name}")

class HomePage:
    """í™ˆ í˜ì´ì§€"""
    
    def render(self, user_level: UserLevel):
        st.title("ğŸ§¬ ë²”ìš© ê³ ë¶„ì ì‹¤í—˜ ì„¤ê³„ í”Œë«í¼")
        st.markdown("### AI ê¸°ë°˜ ì§€ëŠ¥í˜• ì‹¤í—˜ ì„¤ê³„ ë° ë¶„ì„ ì‹œìŠ¤í…œ")
        
        # í™˜ì˜ ë©”ì‹œì§€
        if user_level == UserLevel.BEGINNER:
            st.info("""
            ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤! ì´ í”Œë«í¼ì€ ê³ ë¶„ì ì‹¤í—˜ì„ ì²˜ìŒ ì‹œì‘í•˜ëŠ” ë¶„ë“¤ë„ 
            ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. 
            
            ê° ë‹¨ê³„ë§ˆë‹¤ ìì„¸í•œ ì„¤ëª…ê³¼ ë„ì›€ë§ì´ ì œê³µë˜ë‹ˆ ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”!
            """)
        
        # ë¹ ë¥¸ ì‹œì‘
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("#### ğŸš€ ë¹ ë¥¸ ì‹œì‘")
            if st.button("ìƒˆ í”„ë¡œì íŠ¸ ì‹œì‘", key="quick_start_new"):
                st.session_state.current_page = 'project_setup'
                st.rerun()
            
            if st.button("ê¸°ì¡´ í”„ë¡œì íŠ¸ ì—´ê¸°", key="quick_start_open"):
                st.session_state.show_project_list = True
        
        with col2:
            st.markdown("#### ğŸ“Š ìµœê·¼ í™œë™")
            # ìµœê·¼ í”„ë¡œì íŠ¸ í‘œì‹œ
            if 'recent_projects' in st.session_state:
                for project in st.session_state.recent_projects[:3]:
                    st.write(f"â€¢ {project['name']} ({project['date']})")
            else:
                st.write("ìµœê·¼ í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with col3:
            st.markdown("#### ğŸ“ˆ í†µê³„")
            st.metric("ì™„ë£Œëœ ì‹¤í—˜", "23", "3")
            st.metric("ì ˆì•½ëœ ì‹œê°„", "156ì‹œê°„", "24ì‹œê°„")
            st.metric("ì •í™•ë„", "94.2%", "2.1%")
        
        # ì£¼ìš” ê¸°ëŠ¥ ì†Œê°œ
        st.markdown("---")
        st.markdown("### âœ¨ ì£¼ìš” ê¸°ëŠ¥")
        
        features_cols = st.columns(4)
        features = [
            ("ğŸ¤–", "AI ì‹¤í—˜ ì„¤ê³„", "6ê°œ AIê°€ í˜‘ë ¥í•˜ì—¬ ìµœì ì˜ ì‹¤í—˜ ì„¤ê³„ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤."),
            ("ğŸ”", "í†µí•© ê²€ìƒ‰", "9ê°œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ í•œë²ˆì— ê²€ìƒ‰í•©ë‹ˆë‹¤."),
            ("ğŸ“Š", "ì‹¤ì‹œê°„ ë¶„ì„", "ì‹¤í—˜ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ë©´ ì¦‰ì‹œ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
            ("ğŸ“š", "ë§ì¶¤í˜• í•™ìŠµ", "ì‚¬ìš©ì ë ˆë²¨ì— ë§ëŠ” ì„¤ëª…ê³¼ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
        ]
        
        for col, (icon, title, desc) in zip(features_cols, features):
            with col:
                st.markdown(f"#### {icon} {title}")
                st.write(desc)
        
        # ìµœì‹  ì†Œì‹
        st.markdown("---")
        st.markdown("### ğŸ“° ìµœì‹  ì†Œì‹")
        
        news_container = st.container()
        with news_container:
            st.info("ğŸ‰ v4.0 ì¶œì‹œ: DeepSeek, Groq ì—”ì§„ ì¶”ê°€!")
            st.success("ğŸ“š ìƒˆë¡œìš´ íŠœí† ë¦¬ì–¼: í˜¼í•©ë¬¼ ì„¤ê³„ ë§ˆìŠ¤í„°í•˜ê¸°")
            st.warning("ğŸ”§ ì˜ˆì •ëœ ìœ ì§€ë³´ìˆ˜: 12ì›” 25ì¼ ì˜¤ì „ 2-4ì‹œ")

# ==================== ì‹¤ì‹œê°„ í˜‘ì—… ì‹œìŠ¤í…œ ====================
class CollaborationSystem:
    """ì‹¤ì‹œê°„ í˜‘ì—… ê¸°ëŠ¥"""
    
    def __init__(self):
        self.active_sessions = {}
        self.message_queue = defaultdict(deque)
        self.shared_designs = {}
        self.collaboration_db = CollaborationDatabase()
        
    async def create_session(self, 
                           project_id: str, 
                           creator_id: str,
                           session_name: str) -> str:
        """í˜‘ì—… ì„¸ì…˜ ìƒì„±"""
        session_id = str(uuid.uuid4())
        
        session = {
            'id': session_id,
            'project_id': project_id,
            'name': session_name,
            'creator': creator_id,
            'participants': [creator_id],
            'created_at': datetime.now(),
            'status': 'active',
            'shared_data': {},
            'chat_history': []
        }
        
        self.active_sessions[session_id] = session
        await self.collaboration_db.save_session(session)
        
        return session_id
    
    async def join_session(self, session_id: str, user_id: str) -> bool:
        """ì„¸ì…˜ ì°¸ì—¬"""
        if session_id not in self.active_sessions:
            # DBì—ì„œ ì„¸ì…˜ ë¡œë“œ
            session = await self.collaboration_db.load_session(session_id)
            if not session:
                return False
            self.active_sessions[session_id] = session
        
        session = self.active_sessions[session_id]
        if user_id not in session['participants']:
            session['participants'].append(user_id)
            
            # ì°¸ì—¬ ì•Œë¦¼
            await self.broadcast_message(
                session_id,
                {
                    'type': 'user_joined',
                    'user_id': user_id,
                    'timestamp': datetime.now()
                }
            )
        
        return True
    
    async def share_design(self, 
                         session_id: str, 
                         user_id: str,
                         design_data: Dict) -> bool:
        """ì„¤ê³„ ê³µìœ """
        if session_id not in self.active_sessions:
            return False
        
        session = self.active_sessions[session_id]
        
        # ì„¤ê³„ ë°ì´í„° ì €ì¥
        design_id = str(uuid.uuid4())
        self.shared_designs[design_id] = {
            'session_id': session_id,
            'shared_by': user_id,
            'data': design_data,
            'timestamp': datetime.now(),
            'comments': [],
            'votes': {}
        }
        
        # ì„¸ì…˜ì— ì„¤ê³„ ID ì¶”ê°€
        if 'shared_designs' not in session['shared_data']:
            session['shared_data']['shared_designs'] = []
        session['shared_data']['shared_designs'].append(design_id)
        
        # ê³µìœ  ì•Œë¦¼
        await self.broadcast_message(
            session_id,
            {
                'type': 'design_shared',
                'design_id': design_id,
                'user_id': user_id,
                'title': design_data.get('experiment_title', 'ì œëª© ì—†ìŒ')
            }
        )
        
        return True
    
    async def add_comment(self,
                        design_id: str,
                        user_id: str,
                        comment: str,
                        parent_id: Optional[str] = None) -> bool:
        """ì„¤ê³„ì— ëŒ“ê¸€ ì¶”ê°€"""
        if design_id not in self.shared_designs:
            return False
        
        comment_data = {
            'id': str(uuid.uuid4()),
            'user_id': user_id,
            'text': comment,
            'timestamp': datetime.now(),
            'parent_id': parent_id,
            'reactions': {}
        }
        
        self.shared_designs[design_id]['comments'].append(comment_data)
        
        # ëŒ“ê¸€ ì•Œë¦¼
        session_id = self.shared_designs[design_id]['session_id']
        await self.broadcast_message(
            session_id,
            {
                'type': 'comment_added',
                'design_id': design_id,
                'comment': comment_data
            }
        )
        
        return True

    async def broadcast_message(self, session_id: str, message: Dict):
        """ì„¸ì…˜ ì°¸ê°€ìì—ê²Œ ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        if session_id not in self.active_sessions:
            return
        
        session = self.active_sessions[session_id]
        
        # ë©”ì‹œì§€ë¥¼ ê° ì°¸ê°€ìì˜ íì— ì¶”ê°€
        for participant_id in session['participants']:
            queue_key = f"{session_id}:{participant_id}"
            self.message_queue[queue_key].append(message)
            
            # í í¬ê¸° ì œí•œ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
            if len(self.message_queue[queue_key]) > 100:
                self.message_queue[queue_key].popleft()
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        if message['type'] == 'chat':
            session['chat_history'].append(message)
            
            # DBì— ì €ì¥
            await self.collaboration_db.save_chat_message(session_id, message)
    
    async def send_chat_message(self,
                              session_id: str,
                              user_id: str,
                              message: str) -> bool:
        """ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡"""
        if session_id not in self.active_sessions:
            return False
        
        chat_message = {
            'type': 'chat',
            'user_id': user_id,
            'message': message,
            'timestamp': datetime.now()
        }
        
        await self.broadcast_message(session_id, chat_message)
        return True
    
    def get_pending_messages(self, session_id: str, user_id: str) -> List[Dict]:
        """ëŒ€ê¸° ì¤‘ì¸ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°"""
        queue_key = f"{session_id}:{user_id}"
        messages = list(self.message_queue[queue_key])
        self.message_queue[queue_key].clear()
        return messages
    
    async def vote_on_design(self,
                           design_id: str,
                           user_id: str,
                           vote: int) -> bool:
        """ì„¤ê³„ íˆ¬í‘œ (1-5ì )"""
        if design_id not in self.shared_designs:
            return False
        
        self.shared_designs[design_id]['votes'][user_id] = vote
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        votes = self.shared_designs[design_id]['votes'].values()
        avg_score = sum(votes) / len(votes) if votes else 0
        
        # íˆ¬í‘œ ì•Œë¦¼
        session_id = self.shared_designs[design_id]['session_id']
        await self.broadcast_message(
            session_id,
            {
                'type': 'vote_updated',
                'design_id': design_id,
                'user_id': user_id,
                'vote': vote,
                'avg_score': avg_score
            }
        )
        
        return True

# Polymer-doe-platform - Part 11
# ==================== í”„ë¡œì íŠ¸ ì„¤ì • í˜ì´ì§€ ====================
class ProjectSetupPage:
    """í”„ë¡œì íŠ¸ ì„¤ì • í˜ì´ì§€"""
    
    def __init__(self):
        self.polymer_database = PolymerDatabase()
        self.project_templates = ProjectTemplates()
        
    def render(self, user_level: UserLevel):
        st.title("ğŸ“‹ í”„ë¡œì íŠ¸ ì„¤ì •")
        
        # í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´
        st.markdown("### 1. ê¸°ë³¸ ì •ë³´")
        
        col1, col2 = st.columns(2)
        
        with col1:
            project_name = st.text_input(
                "í”„ë¡œì íŠ¸ ì´ë¦„",
                placeholder="ì˜ˆ: PET í•„ë¦„ ê¸°ê³„ì  íŠ¹ì„± ìµœì í™”",
                help="í”„ë¡œì íŠ¸ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ëª…í™•í•œ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”."
            )
            
            objective = st.text_area(
                "ì—°êµ¬ ëª©ì ",
                placeholder="ì´ ì‹¤í—˜ì„ í†µí•´ ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” ëª©í‘œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.",
                height=100,
                help="êµ¬ì²´ì ì¸ ëª©í‘œê°€ ìˆì„ìˆ˜ë¡ AIê°€ ë” ì •í™•í•œ ì„¤ê³„ë¥¼ ì œì•ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        
        with col2:
            # í…œí”Œë¦¿ ì„ íƒ
            st.markdown("#### í…œí”Œë¦¿ í™œìš©")
            template_names = self.project_templates.get_template_names()
            
            selected_template = st.selectbox(
                "í”„ë¡œì íŠ¸ í…œí”Œë¦¿",
                ["ì§ì ‘ ì„¤ì •"] + template_names,
                help="ìœ ì‚¬í•œ í”„ë¡œì íŠ¸ í…œí”Œë¦¿ì„ ì„ íƒí•˜ë©´ ë¹ ë¥´ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            
            if selected_template != "ì§ì ‘ ì„¤ì •":
                if st.button("í…œí”Œë¦¿ ì ìš©"):
                    template_data = self.project_templates.get_template(selected_template)
                    st.session_state.update(template_data)
                    st.success("í…œí”Œë¦¿ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.rerun()
        
        # ê³ ë¶„ì ì„ íƒ
        st.markdown("### 2. ê³ ë¶„ì ì„ íƒ")
        
        # AI ì¶”ì²œ ì‹œìŠ¤í…œ
        if user_level == UserLevel.BEGINNER:
            st.info("""
            ğŸ’¡ **ì´ˆë³´ì ê°€ì´ë“œ**: ì—°êµ¬í•˜ê³ ì í•˜ëŠ” ê³ ë¶„ìë¥¼ ì„ íƒí•˜ì„¸ìš”. 
            ê° ê³ ë¶„ìì˜ íŠ¹ì„±ê³¼ ì¼ë°˜ì ì¸ ìš©ë„ê°€ í•¨ê»˜ í‘œì‹œë©ë‹ˆë‹¤.
            """)
        
        # ê³ ë¶„ì ì¹´í…Œê³ ë¦¬ ì„ íƒ
        polymer_categories = list(POLYMER_CATEGORIES['base_types'].keys())
        selected_category = st.selectbox(
            "ê³ ë¶„ì ì¹´í…Œê³ ë¦¬",
            polymer_categories,
            format_func=lambda x: POLYMER_CATEGORIES['base_types'][x]['name']
        )
        
        # êµ¬ì²´ì  ê³ ë¶„ì ì„ íƒ
        category_info = POLYMER_CATEGORIES['base_types'][selected_category]
        polymer_examples = category_info['examples']
        
        col1, col2, col3 = st.columns([2, 2, 1])
        
        with col1:
            selected_polymer = st.selectbox(
                "ê³ ë¶„ì ì¢…ë¥˜",
                polymer_examples,
                help=category_info['description']
            )
        
        with col2:
            # ê³ ë¶„ì ì •ë³´ í‘œì‹œ
            if selected_polymer:
                polymer_info = self.polymer_database.get_polymer_info(selected_polymer)
                if polymer_info:
                    st.markdown(f"**{polymer_info['name']}**")
                    st.markdown(f"í™”í•™ì‹: {polymer_info.get('formula', 'N/A')}")
                    
                    # ì£¼ìš” íŠ¹ì„± í‘œì‹œ
                    if 'properties' in polymer_info:
                        props = polymer_info['properties']
                        st.metric("Tg (Â°C)", props.get('Tg', 'N/A'))
                        st.metric("Tm (Â°C)", props.get('Tm', 'N/A'))
        
        with col3:
            # 3D êµ¬ì¡° í‘œì‹œ ë²„íŠ¼
            if st.button("3D êµ¬ì¡° ë³´ê¸°"):
                st.session_state.show_3d_structure = True
        
        # íƒ€ê²Ÿ íŠ¹ì„± ì„ íƒ
        st.markdown("### 3. ëª©í‘œ íŠ¹ì„±")
        
        typical_properties = category_info.get('typical_properties', [])
        
        selected_properties = st.multiselect(
            "ê°œì„ í•˜ê³ ì í•˜ëŠ” íŠ¹ì„±",
            typical_properties,
            default=typical_properties[:2] if len(typical_properties) >= 2 else typical_properties,
            help="ì‹¤í—˜ì„ í†µí•´ ìµœì í™”í•˜ê³ ì í•˜ëŠ” íŠ¹ì„±ë“¤ì„ ì„ íƒí•˜ì„¸ìš”."
        )
        
        # ì œì•½ ì¡°ê±´
        st.markdown("### 4. ì œì•½ ì¡°ê±´")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            budget = st.number_input(
                "ì˜ˆì‚° (ë§Œì›)",
                min_value=10,
                max_value=10000,
                value=500,
                step=50,
                help="ì‹¤í—˜ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ ì˜ˆì‚°"
            )
        
        with col2:
            timeline = st.number_input(
                "ê¸°ê°„ (ì£¼)",
                min_value=1,
                max_value=52,
                value=4,
                help="ì‹¤í—˜ ì™„ë£Œê¹Œì§€ì˜ ëª©í‘œ ê¸°ê°„"
            )
        
        with col3:
            max_experiments = st.number_input(
                "ìµœëŒ€ ì‹¤í—˜ ìˆ˜",
                min_value=5,
                max_value=1000,
                value=50,
                help="ìˆ˜í–‰ ê°€ëŠ¥í•œ ìµœëŒ€ ì‹¤í—˜ íšŸìˆ˜"
            )
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ë¹„
        st.markdown("### 5. ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ë¹„")
        
        equipment_categories = {
            "ê°€ê³µ ì¥ë¹„": ["ì‚¬ì¶œì„±í˜•ê¸°", "ì••ì¶œê¸°", "í•«í”„ë ˆìŠ¤", "ìŠ¤í•€ì½”í„°", "3D í”„ë¦°í„°"],
            "ì¸¡ì • ì¥ë¹„": ["ë§ŒëŠ¥ì‹œí—˜ê¸°", "ì¶©ê²©ì‹œí—˜ê¸°", "ê²½ë„ê³„", "ìœ ë³€ë¬¼ì„±ì¸¡ì •ê¸°"],
            "ì—´ë¶„ì„": ["DSC", "TGA", "DMA", "TMA", "ì—´ì „ë„ë„ì¸¡ì •ê¸°"],
            "êµ¬ì¡°ë¶„ì„": ["FTIR", "XRD", "SEM", "TEM", "AFM"],
            "ë¶„ìëŸ‰ë¶„ì„": ["GPC", "ì ë„ê³„", "ì§ˆëŸ‰ë¶„ì„ê¸°"]
        }
        
        selected_equipment = []
        
        for category, items in equipment_categories.items():
            with st.expander(f"{category} ({len(items)}ì¢…)"):
                for item in items:
                    if st.checkbox(item, key=f"equip_{item}"):
                        selected_equipment.append(item)
        
        # AI ì¶”ì²œ
        st.markdown("### 6. AI ì¶”ì²œì‚¬í•­")
        
        if st.button("AI ì¶”ì²œ ë°›ê¸°", type="primary"):
            with st.spinner("AIê°€ í”„ë¡œì íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                recommendations = asyncio.run(
                    self._get_ai_recommendations(
                        {
                            'polymer': selected_polymer,
                            'properties': selected_properties,
                            'budget': budget,
                            'timeline': timeline,
                            'equipment': selected_equipment
                        }
                    )
                )
                
                if recommendations:
                    st.success("AI ì¶”ì²œì‚¬í•­ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ì¶”ì²œ ë‚´ìš© í‘œì‹œ
                    with st.expander("ğŸ¤– AI ì¶”ì²œì‚¬í•­", expanded=True):
                        st.markdown(recommendations)
        
        # ì €ì¥ ë²„íŠ¼
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col2:
            if st.button("í”„ë¡œì íŠ¸ ì €ì¥ ë° ë‹¤ìŒ ë‹¨ê³„", type="primary", use_container_width=True):
                # í”„ë¡œì íŠ¸ ì •ë³´ ì €ì¥
                project_info = {
                    'name': project_name,
                    'objective': objective,
                    'polymer_type': selected_polymer,
                    'polymer_category': selected_category,
                    'target_properties': selected_properties,
                    'budget': budget,
                    'timeline': timeline,
                    'max_experiments': max_experiments,
                    'equipment': selected_equipment,
                    'created_at': datetime.now(),
                    'user_level': user_level.name
                }
                
                st.session_state.project_info = project_info
                st.session_state.current_page = 'experiment_design'
                st.success("í”„ë¡œì íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
    
    async def _get_ai_recommendations(self, project_data: Dict) -> str:
        """AI ì¶”ì²œì‚¬í•­ ìƒì„±"""
        if not hasattr(st.session_state, 'ai_orchestrator'):
            return "AI ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        prompt = f"""
        ë‹¤ìŒ ê³ ë¶„ì ì‹¤í—˜ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì¶”ì²œì‚¬í•­ì„ ì œê³µí•´ì£¼ì„¸ìš”:
        
        - ê³ ë¶„ì: {project_data['polymer']}
        - ëª©í‘œ íŠ¹ì„±: {', '.join(project_data['properties'])}
        - ì˜ˆì‚°: {project_data['budget']}ë§Œì›
        - ê¸°ê°„: {project_data['timeline']}ì£¼
        - ì‚¬ìš© ê°€ëŠ¥ ì¥ë¹„: {', '.join(project_data['equipment'][:5])}  # ìƒìœ„ 5ê°œë§Œ
        
        ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•´ì„œ ì¶”ì²œí•´ì£¼ì„¸ìš”:
        1. ê¶Œì¥ ì‹¤í—˜ ì„¤ê³„ ìœ í˜•
        2. ì£¼ìš” ê³ ë ¤ ìš”ì¸ (3-5ê°œ)
        3. ì˜ˆìƒë˜ëŠ” ë„ì „ ê³¼ì œ
        4. ì„±ê³µ í™•ë¥ ì„ ë†’ì´ëŠ” íŒ
        """
        
        response = await st.session_state.ai_orchestrator.query_single(
            'gemini',
            prompt,
            temperature=0.7
        )
        
        return response.get('response', 'ì¶”ì²œì‚¬í•­ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')

# ==================== ì‹¤í—˜ ì„¤ê³„ í˜ì´ì§€ ====================
class ExperimentDesignPage:
    """ì‹¤í—˜ ì„¤ê³„ í˜ì´ì§€"""
    
    def __init__(self):
        self.design_engine = None
        self.factor_library = FactorLibrary()
        
    def render(self, user_level: UserLevel):
        st.title("ğŸ”¬ ì‹¤í—˜ ì„¤ê³„")
        
        # í”„ë¡œì íŠ¸ ì •ë³´ í™•ì¸
        if 'project_info' not in st.session_state:
            st.warning("ë¨¼ì € í”„ë¡œì íŠ¸ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            if st.button("í”„ë¡œì íŠ¸ ì„¤ì •ìœ¼ë¡œ ì´ë™"):
                st.session_state.current_page = 'project_setup'
                st.rerun()
            return
        
        project_info = st.session_state.project_info
        
        # í”„ë¡œì íŠ¸ ìš”ì•½
        with st.expander("ğŸ“‹ í”„ë¡œì íŠ¸ ì •ë³´", expanded=False):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown(f"**í”„ë¡œì íŠ¸**: {project_info['name']}")
                st.markdown(f"**ê³ ë¶„ì**: {project_info['polymer_type']}")
            with col2:
                st.markdown(f"**ì˜ˆì‚°**: {project_info['budget']}ë§Œì›")
                st.markdown(f"**ê¸°ê°„**: {project_info['timeline']}ì£¼")
            with col3:
                st.markdown(f"**ëª©í‘œ íŠ¹ì„±**: {', '.join(project_info['target_properties'][:3])}")
        
        # íƒ­ êµ¬ì„±
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "1ï¸âƒ£ ìš”ì¸ ì„ íƒ",
            "2ï¸âƒ£ ë°˜ì‘ë³€ìˆ˜ ì •ì˜", 
            "3ï¸âƒ£ ì„¤ê³„ ìƒì„±",
            "4ï¸âƒ£ ê²€ì¦ ë° ìµœì í™”",
            "5ï¸âƒ£ ìµœì¢… í™•ì¸"
        ])
        
        with tab1:
            self._render_factor_selection(project_info, user_level)
        
        with tab2:
            self._render_response_definition(project_info, user_level)
        
        with tab3:
            self._render_design_generation(project_info, user_level)
        
        with tab4:
            self._render_validation(user_level)
        
        with tab5:
            self._render_final_confirmation(user_level)
    
    def _render_factor_selection(self, project_info: Dict, user_level: UserLevel):
        """ìš”ì¸ ì„ íƒ íƒ­"""
        st.markdown("### ì‹¤í—˜ ìš”ì¸ ì„ íƒ")
        
        if user_level == UserLevel.BEGINNER:
            st.info("""
            ğŸ’¡ **ìš”ì¸(Factor)ì´ë€?** ì‹¤í—˜ì—ì„œ ë³€í™”ì‹œí‚¬ ë³€ìˆ˜ë“¤ì…ë‹ˆë‹¤.
            ì˜ˆ: ì˜¨ë„, ì‹œê°„, ë†ë„, ì••ë ¥ ë“±
            
            ê° ìš”ì¸ë§ˆë‹¤ ìµœì†Œ 2ê°œ ì´ìƒì˜ ìˆ˜ì¤€(Level)ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
            """)
        
        # AI ì¶”ì²œ ìš”ì¸
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown("#### ì¶”ì²œ ìš”ì¸")
            
            # ê³ ë¶„ìë³„ ì¶”ì²œ ìš”ì¸ ê°€ì ¸ì˜¤ê¸°
            recommended_factors = self.factor_library.get_recommended_factors(
                project_info['polymer_type'],
                project_info['target_properties']
            )
            
            # ì¶”ì²œ ìš”ì¸ í‘œì‹œ
            for i, factor in enumerate(recommended_factors[:5]):
                col_a, col_b, col_c = st.columns([3, 2, 1])
                
                with col_a:
                    factor_selected = st.checkbox(
                        factor['name'],
                        value=i < 3,  # ìƒìœ„ 3ê°œëŠ” ê¸°ë³¸ ì„ íƒ
                        key=f"rec_factor_{i}",
                        help=factor.get('description', '')
                    )
                
                with col_b:
                    if factor_selected:
                        st.markdown(f"ì¼ë°˜ ë²”ìœ„: {factor['typical_range']}")
                
                with col_c:
                    if factor_selected:
                        importance = st.select_slider(
                            "ì¤‘ìš”ë„",
                            options=['ë‚®ìŒ', 'ë³´í†µ', 'ë†’ìŒ'],
                            value='ë³´í†µ',
                            key=f"imp_{i}"
                        )
        
        with col2:
            # AI ë„ì›€ë§
            if st.button("ğŸ¤– AI ì¡°ì–¸"):
                advice = self._get_factor_advice(project_info, recommended_factors)
                st.info(advice)
        
        # ì‚¬ìš©ì ì •ì˜ ìš”ì¸ ì¶”ê°€
        st.markdown("#### ì‚¬ìš©ì ì •ì˜ ìš”ì¸ ì¶”ê°€")
        
        with st.expander("â• ìƒˆ ìš”ì¸ ì¶”ê°€"):
            new_factor_name = st.text_input("ìš”ì¸ ì´ë¦„")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                factor_type = st.selectbox(
                    "ìš”ì¸ ìœ í˜•",
                    ["ì—°ì†í˜•", "ë²”ì£¼í˜•"],
                    help="ì—°ì†í˜•: ìˆ«ìë¡œ í‘œí˜„ (ì˜¨ë„, ì‹œê°„ ë“±)\në²”ì£¼í˜•: ì¢…ë¥˜ë¡œ í‘œí˜„ (ì´‰ë§¤ ì¢…ë¥˜ ë“±)"
                )
            
            with col2:
                if factor_type == "ì—°ì†í˜•":
                    min_val = st.number_input("ìµœì†Œê°’", value=0.0)
                    max_val = st.number_input("ìµœëŒ€ê°’", value=100.0)
                    unit = st.text_input("ë‹¨ìœ„", value="")
                else:
                    categories = st.text_area(
                        "ë²”ì£¼ ëª©ë¡ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
                        placeholder="Aí˜•\nBí˜•\nCí˜•"
                    )
            
            with col3:
                if st.button("ìš”ì¸ ì¶”ê°€", type="primary"):
                    # ìš”ì¸ ì¶”ê°€ ë¡œì§
                    st.success(f"'{new_factor_name}' ìš”ì¸ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì„ íƒëœ ìš”ì¸ ìš”ì•½
        st.markdown("#### ì„ íƒëœ ìš”ì¸ ìš”ì•½")
        
        if 'selected_factors' in st.session_state:
            factor_df = pd.DataFrame(st.session_state.selected_factors)
            st.dataframe(factor_df, use_container_width=True)
            
            # ìš”ì¸ ìˆ˜ì— ë”°ë¥¸ ì‹¤í—˜ ìˆ˜ ì˜ˆìƒ
            n_factors = len(st.session_state.selected_factors)
            if n_factors > 0:
                min_runs = 2 ** (n_factors - 1) if n_factors <= 5 else n_factors * 2
                st.info(f"ì„ íƒëœ ìš”ì¸ ìˆ˜: {n_factors}ê°œ â†’ ìµœì†Œ ì‹¤í—˜ ìˆ˜: {min_runs}íšŒ")
    
    def _render_response_definition(self, project_info: Dict, user_level: UserLevel):
        """ë°˜ì‘ë³€ìˆ˜ ì •ì˜ íƒ­"""
        st.markdown("### ë°˜ì‘ë³€ìˆ˜ ì •ì˜")
        
        if user_level == UserLevel.BEGINNER:
            st.info("""
            ğŸ’¡ **ë°˜ì‘ë³€ìˆ˜(Response)ë€?** ì‹¤í—˜ì—ì„œ ì¸¡ì •í•  ê²°ê³¼ê°’ë“¤ì…ë‹ˆë‹¤.
            ì˜ˆ: ì¸ì¥ê°•ë„, ì‹ ìœ¨, íˆ¬ëª…ë„, ë¶„ìëŸ‰ ë“±
            
            ëª©í‘œë¥¼ ëª…í™•íˆ ì„¤ì •í•˜ë©´ ìµœì í™”ê°€ ì‰¬ì›Œì§‘ë‹ˆë‹¤.
            """)
        
        # í”„ë¡œì íŠ¸ì˜ ëª©í‘œ íŠ¹ì„± ê¸°ë°˜ ë°˜ì‘ë³€ìˆ˜
        st.markdown("#### ì£¼ìš” ë°˜ì‘ë³€ìˆ˜")
        
        for i, prop in enumerate(project_info['target_properties']):
            with st.container():
                col1, col2, col3, col4 = st.columns([3, 2, 2, 1])
                
                with col1:
                    response_name = st.text_input(
                        "ë°˜ì‘ë³€ìˆ˜ ì´ë¦„",
                        value=prop,
                        key=f"resp_name_{i}"
                    )
                
                with col2:
                    response_unit = st.text_input(
                        "ë‹¨ìœ„",
                        value=self._get_default_unit(prop),
                        key=f"resp_unit_{i}"
                    )
                
                with col3:
                    optimization_goal = st.selectbox(
                        "ìµœì í™” ëª©í‘œ",
                        ["ìµœëŒ€í™”", "ìµœì†Œí™”", "ëª©í‘œê°’", "ë²”ìœ„ë‚´"],
                        key=f"resp_goal_{i}"
                    )
                    
                    if optimization_goal == "ëª©í‘œê°’":
                        target_value = st.number_input(
                            "ëª©í‘œê°’",
                            key=f"resp_target_{i}"
                        )
                    elif optimization_goal == "ë²”ìœ„ë‚´":
                        col_a, col_b = st.columns(2)
                        with col_a:
                            lower_limit = st.number_input("í•˜í•œ", key=f"resp_lower_{i}")
                        with col_b:
                            upper_limit = st.number_input("ìƒí•œ", key=f"resp_upper_{i}")
                
                with col4:
                    if i > 0:  # ì²« ë²ˆì§¸ ë°˜ì‘ë³€ìˆ˜ëŠ” ì‚­ì œ ë¶ˆê°€
                        if st.button("ì‚­ì œ", key=f"del_resp_{i}"):
                            st.session_state.target_properties.pop(i)
                            st.rerun()
        
        # ì¶”ê°€ ë°˜ì‘ë³€ìˆ˜
        if st.button("â• ë°˜ì‘ë³€ìˆ˜ ì¶”ê°€"):
            if 'additional_responses' not in st.session_state:
                st.session_state.additional_responses = []
            st.session_state.additional_responses.append({})
            st.rerun()
        
        # ì¸¡ì • ë°©ë²• ë° ì¡°ê±´
        st.markdown("#### ì¸¡ì • ë°©ë²• ì„¤ì •")
        
        with st.expander("ğŸ”¬ ì¸¡ì • í”„ë¡œí† ì½œ"):
            for response in st.session_state.get('responses', []):
                st.markdown(f"**{response['name']}**")
                
                # ì¸¡ì • ì¥ë¹„ ì„ íƒ
                available_equipment = project_info.get('equipment', [])
                measurement_equipment = st.selectbox(
                    "ì¸¡ì • ì¥ë¹„",
                    available_equipment,
                    key=f"measure_equip_{response['name']}"
                )
                
                # ì¸¡ì • ì¡°ê±´
                col1, col2 = st.columns(2)
                with col1:
                    sample_prep = st.text_area(
                        "ì‹œë£Œ ì¤€ë¹„",
                        placeholder="ì‹œë£Œ ì¤€ë¹„ ë°©ë²•ì„ ì…ë ¥í•˜ì„¸ìš”",
                        key=f"prep_{response['name']}"
                    )
                with col2:
                    measurement_conditions = st.text_area(
                        "ì¸¡ì • ì¡°ê±´",
                        placeholder="ì˜¨ë„, ìŠµë„, ì†ë„ ë“±",
                        key=f"cond_{response['name']}"
                    )
    
    def _render_design_generation(self, project_info: Dict, user_level: UserLevel):
        """ì„¤ê³„ ìƒì„± íƒ­"""
        st.markdown("### ì‹¤í—˜ ì„¤ê³„ ìƒì„±")
        
        # ì„¤ê³„ ì „ëµ ì„ íƒ
        col1, col2 = st.columns([2, 1])
        
        with col1:
            design_strategy = st.selectbox(
                "ì„¤ê³„ ì „ëµ",
                list(DESIGN_TYPES.keys()),
                format_func=lambda x: DESIGN_TYPES[x]['name'],
                help="ì‹¤í—˜ ëª©ì ì— ë§ëŠ” ì„¤ê³„ ì „ëµì„ ì„ íƒí•˜ì„¸ìš”."
            )
            
            # ì „ëµ ì„¤ëª…
            strategy_info = DESIGN_TYPES[design_strategy]
            st.info(f"""
            **{strategy_info['name']}**
            
            {strategy_info['description']}
            
            **ì¥ì **: {', '.join(strategy_info['pros'])}
            **ë‹¨ì **: {', '.join(strategy_info['cons'])}
            **ì í•©í•œ ê²½ìš°**: {strategy_info['suitable_for']}
            """)
        
        with col2:
            # AI ì¶”ì²œ
            if st.button("ğŸ¤– AI ì¶”ì²œ ì „ëµ"):
                with st.spinner("AIê°€ ìµœì  ì „ëµì„ ë¶„ì„ ì¤‘..."):
                    recommended_strategy = asyncio.run(
                        self._get_ai_strategy_recommendation(project_info)
                    )
                    st.success(f"ì¶”ì²œ ì „ëµ: {recommended_strategy}")
        
        # ì„¤ê³„ ì˜µì…˜
        st.markdown("#### ì„¤ê³„ ì˜µì…˜")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if design_strategy in ['screening', 'fractional']:
                resolution = st.selectbox(
                    "í•´ìƒë„",
                    ["III", "IV", "V"],
                    index=1,
                    help="ë†’ì€ í•´ìƒë„ì¼ìˆ˜ë¡ êµí˜¸ì‘ìš© ì¶”ì •ì´ ì •í™•í•´ì§‘ë‹ˆë‹¤."
                )
            elif design_strategy == 'ccd':
                alpha_type = st.selectbox(
                    "ì¶•ì  ìœ í˜•",
                    ["orthogonal", "rotatable", "face"],
                    help="ì¶•ì ì˜ ìœ„ì¹˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."
                )
        
        with col2:
            center_points = st.number_input(
                "ì¤‘ì‹¬ì  ë°˜ë³µìˆ˜",
                min_value=0,
                max_value=10,
                value=3,
                help="ì¬í˜„ì„± í™•ì¸ì„ ìœ„í•œ ì¤‘ì‹¬ì  ë°˜ë³µ"
            )
        
        with col3:
            if user_level != UserLevel.BEGINNER:
                randomize = st.checkbox(
                    "ì‹¤í—˜ ìˆœì„œ ë¬´ì‘ìœ„í™”",
                    value=True,
                    help="ì‹œê°„ íš¨ê³¼ë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ ê¶Œì¥ë©ë‹ˆë‹¤."
                )
            else:
                randomize = True
        
        # ì„¤ê³„ ìƒì„± ë²„íŠ¼
        if st.button("ğŸ¯ ì‹¤í—˜ ì„¤ê³„ ìƒì„±", type="primary", use_container_width=True):
            with st.spinner("AIê°€ ìµœì ì˜ ì‹¤í—˜ ì„¤ê³„ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                # ì„¤ê³„ ìƒì„±
                design_result = asyncio.run(
                    self._generate_experiment_design(
                        project_info,
                        design_strategy,
                        user_level
                    )
                )
                
                if design_result['status'] == 'success':
                    st.session_state.experiment_design = design_result['design']
                    st.success("ì‹¤í—˜ ì„¤ê³„ê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤ í‘œì‹œ
                    st.markdown("#### ğŸ“Š ì‹¤í—˜ ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤")
                    
                    design_df = design_result['design']['matrix']
                    
                    # ì‹¤í—˜ ìˆœì„œ ì¶”ê°€
                    design_df.insert(0, 'ì‹¤í—˜ë²ˆí˜¸', range(1, len(design_df) + 1))
                    
                    # ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼ë§
                    styled_df = design_df.style.background_gradient(
                        subset=[col for col in design_df.columns if col != 'ì‹¤í—˜ë²ˆí˜¸']
                    )
                    
                    st.dataframe(styled_df, use_container_width=True)
                    
                    # ì„¤ê³„ í†µê³„
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("ì´ ì‹¤í—˜ìˆ˜", len(design_df))
                    with col2:
                        st.metric("ìš”ì¸ ìˆ˜", len(st.session_state.selected_factors))
                    with col3:
                        st.metric("ì˜ˆìƒ ë¹„ìš©", f"{design_result['design']['estimates']['total_cost']:.0f}ë§Œì›")
                    with col4:
                        st.metric("ì˜ˆìƒ ê¸°ê°„", f"{design_result['design']['estimates']['total_time_sequential']:.0f}ì¼")
                    
                    # ë‹¤ìš´ë¡œë“œ ì˜µì…˜
                    st.markdown("#### ğŸ’¾ ì„¤ê³„ ë‹¤ìš´ë¡œë“œ")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        # Excel ë‹¤ìš´ë¡œë“œ
                        excel_buffer = io.BytesIO()
                        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                            design_df.to_excel(writer, sheet_name='ì‹¤í—˜ì„¤ê³„', index=False)
                            
                            # í”„ë¡œì íŠ¸ ì •ë³´ ì‹œíŠ¸
                            project_df = pd.DataFrame([project_info])
                            project_df.to_excel(writer, sheet_name='í”„ë¡œì íŠ¸ì •ë³´', index=False)
                        
                        excel_buffer.seek(0)
                        
                        st.download_button(
                            label="ğŸ“Š Excel ë‹¤ìš´ë¡œë“œ",
                            data=excel_buffer,
                            file_name=f"{project_info['name']}_ì‹¤í—˜ì„¤ê³„.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    
                    with col2:
                        # CSV ë‹¤ìš´ë¡œë“œ
                        csv = design_df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            label="ğŸ“„ CSV ë‹¤ìš´ë¡œë“œ",
                            data=csv,
                            file_name=f"{project_info['name']}_ì‹¤í—˜ì„¤ê³„.csv",
                            mime="text/csv"
                        )
                    
                    with col3:
                        # PDF ë¦¬í¬íŠ¸ (ì¶”í›„ êµ¬í˜„)
                        if st.button("ğŸ“‘ PDF ë¦¬í¬íŠ¸"):
                            st.info("PDF ë¦¬í¬íŠ¸ ìƒì„± ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.")
                else:
                    st.error(f"ì„¤ê³„ ìƒì„± ì‹¤íŒ¨: {design_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

    def _render_validation(self, user_level: UserLevel):
        """ê²€ì¦ ë° ìµœì í™” íƒ­"""
        st.markdown("### ì‹¤í—˜ ì„¤ê³„ ê²€ì¦")
        
        if 'experiment_design' not in st.session_state:
            st.warning("ë¨¼ì € ì‹¤í—˜ ì„¤ê³„ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return
        
        design = st.session_state.experiment_design
        validation_report = design.get('validation_report', {})
        
        # ì „ì²´ ê²€ì¦ ì ìˆ˜
        overall_score = validation_report.get('overall_score', 0)
        
        col1, col2, col3 = st.columns([1, 2, 1])
        
        with col2:
            # ê²Œì´ì§€ ì°¨íŠ¸ë¡œ ì ìˆ˜ í‘œì‹œ
            fig = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=overall_score * 100,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "ì„¤ê³„ í’ˆì§ˆ ì ìˆ˜"},
                delta={'reference': 80, 'relative': True},
                gauge={
                    'axis': {'range': [None, 100]},
                    'bar': {'color': "darkblue"},
                    'steps': [
                        {'range': [0, 50], 'color': "lightgray"},
                        {'range': [50, 80], 'color': "gray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': 90
                    }
                }
            ))
            
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
        
        # ìƒì„¸ ê²€ì¦ ê²°ê³¼
        st.markdown("#### ğŸ“‹ ìƒì„¸ ê²€ì¦ ê²°ê³¼")
        
        # íƒ­ìœ¼ë¡œ êµ¬ë¶„
        val_tab1, val_tab2, val_tab3 = st.tabs(["í†µê³„ì  ê²€ì¦", "ì‹¤ìš©ì„± ê²€ì¦", "ì•ˆì „ì„± ê²€ì¦"])
        
        with val_tab1:
            stat_validation = validation_report.get('statistical', {})
            
            # ê²€ì¦ í•­ëª©ë³„ ê²°ê³¼
            metrics = {
                "ê· í˜•ì„±": stat_validation.get('details', {}).get('balance_score', 0),
                "ì§êµì„±": stat_validation.get('details', {}).get('orthogonality_score', 0),
                "ê²€ì •ë ¥": stat_validation.get('details', {}).get('power', 0)
            }
            
            cols = st.columns(len(metrics))
            for (metric_name, score), col in zip(metrics.items(), cols):
                with col:
                    color = "green" if score >= 0.8 else "orange" if score >= 0.6 else "red"
                    st.metric(
                        metric_name,
                        f"{score:.2f}",
                        f"{(score - 0.8) * 100:.1f}%",
                        delta_color="normal" if score >= 0.8 else "inverse"
                    )
            
            # ë¬¸ì œì  ë° ì œì•ˆì‚¬í•­
            if stat_validation.get('issues'):
                st.warning("**ë°œê²¬ëœ ë¬¸ì œì :**")
                for issue in stat_validation['issues']:
                    st.write(f"- {issue}")
            
            if stat_validation.get('suggestions'):
                st.info("**ê°œì„  ì œì•ˆ:**")
                for suggestion in stat_validation['suggestions']:
                    st.write(f"- {suggestion}")
        
        with val_tab2:
            practical_validation = validation_report.get('practical', {})
            
            # ì‹¤ìš©ì„± ë©”íŠ¸ë¦­
            st.markdown("**ì‹œê°„ ë¶„ì„**")
            col1, col2 = st.columns(2)
            
            with col1:
                estimated_time = design.get('estimates', {}).get('total_time_sequential', 0)
                available_time = st.session_state.project_info.get('timeline', 4) * 7 * 24
                
                time_ratio = estimated_time / available_time if available_time > 0 else 0
                
                st.progress(min(time_ratio, 1.0))
                st.caption(f"ì˜ˆìƒ: {estimated_time:.0f}ì‹œê°„ / ê°€ìš©: {available_time:.0f}ì‹œê°„")
            
            with col2:
                if practical_validation.get('optimized_sequence'):
                    st.success("âœ… ì‹¤í—˜ ìˆœì„œ ìµœì í™” ì™„ë£Œ")
                    if st.button("ìµœì í™”ëœ ìˆœì„œ ë³´ê¸°"):
                        st.dataframe(practical_validation['optimized_sequence'])
            
            # ì¥ë¹„ ì œì•½
            if practical_validation.get('issues'):
                st.warning("**ì œì•½ì‚¬í•­:**")
                for issue in practical_validation['issues']:
                    st.write(f"- {issue}")
        
        with val_tab3:
            safety_validation = validation_report.get('safety', {})
            
            # ìœ„í—˜ë„ ë ˆë²¨
            risk_level = safety_validation.get('risk_level', 'low')
            risk_colors = {'low': 'green', 'medium': 'orange', 'high': 'red'}
            
            st.markdown(f"**ì „ì²´ ìœ„í—˜ë„**: :{risk_colors[risk_level]}[{risk_level.upper()}]")
            
            # ì•ˆì „ ìš”êµ¬ì‚¬í•­
            if safety_validation.get('safety_requirements'):
                st.markdown("**í•„ìˆ˜ ì•ˆì „ ì¡°ì¹˜:**")
                
                requirements = safety_validation['safety_requirements']
                
                # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
                ppe_reqs = [r for r in requirements if 'ì°©ìš©' in r or 'ë³´í˜¸' in r]
                eng_controls = [r for r in requirements if 'í™˜ê¸°' in r or 'ì‹œìŠ¤í…œ' in r]
                procedures = [r for r in requirements if r not in ppe_reqs + eng_controls]
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.markdown("**ê°œì¸ë³´í˜¸êµ¬**")
                    for req in ppe_reqs:
                        st.write(f"â€¢ {req}")
                
                with col2:
                    st.markdown("**ê³µí•™ì  ì œì–´**")
                    for req in eng_controls:
                        st.write(f"â€¢ {req}")
                
                with col3:
                    st.markdown("**ì‘ì—… ì ˆì°¨**")
                    for req in procedures:
                        st.write(f"â€¢ {req}")
        
        # ìµœì í™” ì˜µì…˜
        st.markdown("#### ğŸ”§ ì„¤ê³„ ìµœì í™”")
        
        if overall_score < 0.9:
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.info(f"""
                í˜„ì¬ ì„¤ê³„ ì ìˆ˜ê°€ {overall_score*100:.1f}ì ì…ë‹ˆë‹¤. 
                90ì  ì´ìƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. AIê°€ ì„¤ê³„ë¥¼ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                """)
            
            with col2:
                if st.button("ğŸš€ AI ìµœì í™”", type="primary"):
                    with st.spinner("AIê°€ ì„¤ê³„ë¥¼ ìµœì í™”í•˜ëŠ” ì¤‘..."):
                        optimized_design = asyncio.run(
                            self._optimize_design_with_ai(design, validation_report)
                        )
                        
                        if optimized_design:
                            st.session_state.experiment_design = optimized_design
                            st.success("ì„¤ê³„ê°€ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.rerun()
    
    def _render_final_confirmation(self, user_level: UserLevel):
        """ìµœì¢… í™•ì¸ íƒ­"""
        st.markdown("### ìµœì¢… í™•ì¸")
        
        if 'experiment_design' not in st.session_state:
            st.warning("ì‹¤í—˜ ì„¤ê³„ë¥¼ ë¨¼ì € ì™„ì„±í•´ì£¼ì„¸ìš”.")
            return
        
        design = st.session_state.experiment_design
        
        # ì‹¤í—˜ ì„¤ê³„ ìš”ì•½
        st.markdown("#### ğŸ“Š ì‹¤í—˜ ì„¤ê³„ ìš”ì•½")
        
        summary_data = {
            "í•­ëª©": ["ì‹¤í—˜ ì œëª©", "ì„¤ê³„ ìœ í˜•", "ì´ ì‹¤í—˜ìˆ˜", "ìš”ì¸ ìˆ˜", 
                    "ë°˜ì‘ë³€ìˆ˜ ìˆ˜", "ì˜ˆìƒ ë¹„ìš©", "ì˜ˆìƒ ê¸°ê°„", "ì„¤ê³„ ì ìˆ˜"],
            "ë‚´ìš©": [
                design.get('experiment_title', 'N/A'),
                design.get('design_type', 'N/A'),
                f"{len(design.get('matrix', []))}íšŒ",
                f"{len(design.get('factors', []))}ê°œ",
                f"{len(design.get('responses', []))}ê°œ",
                f"{design.get('estimates', {}).get('total_cost', 0):.0f}ë§Œì›",
                f"{design.get('estimates', {}).get('total_time_sequential', 0)/24:.1f}ì¼",
                f"{design.get('validation_report', {}).get('overall_score', 0)*100:.1f}ì "
            ]
        }
        
        summary_df = pd.DataFrame(summary_data)
        st.dataframe(summary_df, hide_index=True, use_container_width=True)
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸
        st.markdown("#### âœ… ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸")
        
        checklist_items = [
            "ì‹¤í—˜ ëª©ì ì´ ëª…í™•íˆ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤",
            "ëª¨ë“  ì¤‘ìš” ìš”ì¸ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤",
            "ë°˜ì‘ë³€ìˆ˜ì™€ ëª©í‘œê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤",
            "ì¥ë¹„ì™€ ì¬ë£Œê°€ ì¤€ë¹„ ê°€ëŠ¥í•©ë‹ˆë‹¤",
            "ì•ˆì „ í”„ë¡œí† ì½œì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤",
            "ë°ì´í„° ìˆ˜ì§‘ ê³„íšì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤"
        ]
        
        all_checked = True
        for item in checklist_items:
            checked = st.checkbox(item, key=f"check_{item}")
            if not checked:
                all_checked = False
        
        # QR ì½”ë“œ ìƒì„±
        st.markdown("#### ğŸ”² ì‹¤í—˜ ë¼ë²¨ QR ì½”ë“œ")
        
        if st.button("QR ì½”ë“œ ìƒì„±"):
            qr_codes = self._generate_qr_codes(design)
            
            # QR ì½”ë“œ í‘œì‹œ
            cols = st.columns(4)
            for i, (exp_id, qr_img) in enumerate(qr_codes.items()):
                with cols[i % 4]:
                    st.image(qr_img, caption=f"ì‹¤í—˜ {exp_id}")
        
        # ìµœì¢… ìŠ¹ì¸
        st.markdown("#### ğŸ¯ ì‹¤í—˜ ì‹œì‘")
        
        if user_level == UserLevel.BEGINNER:
            st.info("""
            ğŸ’¡ **ì´ˆë³´ì íŒ**: ì‹¤í—˜ì„ ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
            1. ì‹¤í—˜ ë…¸íŠ¸ ì¤€ë¹„
            2. ë°ì´í„° ê¸°ë¡ ì–‘ì‹ ì¤€ë¹„  
            3. ì²« ì‹¤í—˜ì€ ì¤‘ì‹¬ì ìœ¼ë¡œ ì‹œì‘
            4. ëª¨ë“  ì¸¡ì •ê°’ì„ ì¦‰ì‹œ ê¸°ë¡
            """)
        
        col1, col2, col3 = st.columns([1, 2, 1])
        
        with col2:
            if all_checked:
                if st.button("ğŸš€ ì‹¤í—˜ ì‹œì‘í•˜ê¸°", type="primary", use_container_width=True):
                    # ì‹¤í—˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                    st.session_state.experiment_status = ExperimentStatus.IN_PROGRESS
                    st.session_state.experiment_start_time = datetime.now()
                    st.session_state.current_page = 'data_analysis'
                    
                    st.balloons()
                    st.success("ì‹¤í—˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤! í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸ€")
                    time.sleep(2)
                    st.rerun()
            else:
                st.warning("ëª¨ë“  ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    async def _get_ai_strategy_recommendation(self, project_info: Dict) -> str:
        """AI ì „ëµ ì¶”ì²œ"""
        # êµ¬í˜„ ìƒëµ (ì´ì „ íŒŒíŠ¸ ì°¸ì¡°)
        return "optimization"
    
    async def _generate_experiment_design(self, 
                                        project_info: Dict,
                                        strategy: str,
                                        user_level: UserLevel) -> Dict:
        """ì‹¤í—˜ ì„¤ê³„ ìƒì„±"""
        # êµ¬í˜„ ìƒëµ (ì´ì „ íŒŒíŠ¸ ì°¸ì¡°)
        return {
            'status': 'success',
            'design': {
                'experiment_title': f"{project_info['polymer_type']} ìµœì í™” ì‹¤í—˜",
                'design_type': strategy,
                'matrix': pd.DataFrame(),  # ì‹¤ì œ ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤
                'factors': [],
                'responses': [],
                'estimates': {
                    'total_cost': 450,
                    'total_time_sequential': 120
                },
                'validation_report': {
                    'overall_score': 0.85
                }
            }
        }
    
    def _generate_qr_codes(self, design: Dict) -> Dict[str, Image.Image]:
        """ì‹¤í—˜ë³„ QR ì½”ë“œ ìƒì„±"""
        import qrcode
        
        qr_codes = {}
        matrix = design.get('matrix', pd.DataFrame())
        
        for idx, row in matrix.iterrows():
            # QR ë°ì´í„° ìƒì„±
            qr_data = {
                'exp_id': f"EXP_{idx+1:03d}",
                'project': st.session_state.project_info['name'],
                'conditions': row.to_dict(),
                'date': datetime.now().isoformat()
            }
            
            # QR ì½”ë“œ ìƒì„±
            qr = qrcode.QRCode(
                version=1,
                error_correction=qrcode.constants.ERROR_CORRECT_L,
                box_size=10,
                border=4,
            )
            
            qr.add_data(json.dumps(qr_data))
            qr.make(fit=True)
            
            img = qr.make_image(fill_color="black", back_color="white")
            qr_codes[f"EXP_{idx+1:03d}"] = img
        
        return qr_codes


# Polymer-doe-platform - Part 12
# ==================== ë°ì´í„° ë¶„ì„ í˜ì´ì§€ ====================
class DataAnalysisPage:
    """ë°ì´í„° ë¶„ì„ í˜ì´ì§€"""
    
    def __init__(self):
        self.analyzer = AdvancedStatisticalAnalyzer()
        self.ml_analyzer = MachineLearningAnalyzer()
        
    def render(self, user_level: UserLevel):
        st.title("ğŸ“Š ë°ì´í„° ë¶„ì„")
        
        # ì‹¤í—˜ ìƒíƒœ í™•ì¸
        if 'experiment_design' not in st.session_state:
            st.warning("ë¨¼ì € ì‹¤í—˜ì„ ì„¤ê³„í•´ì£¼ì„¸ìš”.")
            return
        
        # íƒ­ êµ¬ì„±
        tabs = st.tabs([
            "ğŸ“¥ ë°ì´í„° ì…ë ¥",
            "ğŸ“ˆ ê¸°ë³¸ ë¶„ì„",
            "ğŸ”¬ ê³ ê¸‰ ë¶„ì„",
            "ğŸ¤– AI ì¸ì‚¬ì´íŠ¸",
            "ğŸ“‘ ë³´ê³ ì„œ"
        ])
        
        with tabs[0]:
            self._render_data_input(user_level)
        
        with tabs[1]:
            self._render_basic_analysis(user_level)
        
        with tabs[2]:
            self._render_advanced_analysis(user_level)
        
        with tabs[3]:
            self._render_ai_insights(user_level)
        
        with tabs[4]:
            self._render_report_generation()
    
    def _render_data_input(self, user_level: UserLevel):
        """ë°ì´í„° ì…ë ¥ íƒ­"""
        st.markdown("### ì‹¤í—˜ ë°ì´í„° ì…ë ¥")
        
        design = st.session_state.experiment_design
        
        # ì…ë ¥ ë°©ë²• ì„ íƒ
        input_method = st.radio(
            "ë°ì´í„° ì…ë ¥ ë°©ë²•",
            ["ì§ì ‘ ì…ë ¥", "íŒŒì¼ ì—…ë¡œë“œ", "QR ìŠ¤ìº”", "ì‹¤ì‹œê°„ ì—°ë™"],
            horizontal=True
        )
        
        if input_method == "ì§ì ‘ ì…ë ¥":
            # ì‹¤í—˜ë³„ ë°ì´í„° ì…ë ¥
            st.markdown("#### ì‹¤í—˜ë³„ ê²°ê³¼ ì…ë ¥")
            
            # ë°ì´í„° ì…ë ¥ í…Œì´ë¸”
            if 'experimental_data' not in st.session_state:
                # ì´ˆê¸° ë°ì´í„°í”„ë ˆì„ ìƒì„±
                matrix = design['matrix'].copy()
                for response in design['responses']:
                    matrix[response['name']] = np.nan
                st.session_state.experimental_data = matrix
            
            # ë°ì´í„° ì—ë””í„°
            edited_data = st.data_editor(
                st.session_state.experimental_data,
                use_container_width=True,
                num_rows="fixed",
                column_config={
                    col: st.column_config.NumberColumn(
                        col,
                        help=f"{col} ì¸¡ì •ê°’ì„ ì…ë ¥í•˜ì„¸ìš”",
                        format="%.3f"
                    )
                    for col in design['responses']
                }
            )
            
            st.session_state.experimental_data = edited_data
            
            # ì§„í–‰ë¥  í‘œì‹œ
            total_cells = len(design['matrix']) * len(design['responses'])
            filled_cells = edited_data[
                [r['name'] for r in design['responses']]
            ].notna().sum().sum()
            
            progress = filled_cells / total_cells if total_cells > 0 else 0
            
            st.progress(progress)
            st.caption(f"ì…ë ¥ ì™„ë£Œ: {filled_cells}/{total_cells} ({progress*100:.1f}%)")
            
        elif input_method == "íŒŒì¼ ì—…ë¡œë“œ":
            uploaded_file = st.file_uploader(
                "ë°ì´í„° íŒŒì¼ ì„ íƒ",
                type=['csv', 'xlsx', 'xls'],
                help="ì‹¤í—˜ ì„¤ê³„ì™€ ë™ì¼í•œ í˜•ì‹ì˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
            )
            
            if uploaded_file:
                try:
                    if uploaded_file.name.endswith('.csv'):
                        data = pd.read_csv(uploaded_file)
                    else:
                        data = pd.read_excel(uploaded_file)
                    
                    st.success("íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.dataframe(data, use_container_width=True)
                    
                    # ë°ì´í„° ê²€ì¦
                    validation_result = self._validate_uploaded_data(data, design)
                    if validation_result['valid']:
                        st.session_state.experimental_data = data
                    else:
                        st.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {validation_result['message']}")
                        
                except Exception as e:
                    st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        
        elif input_method == "QR ìŠ¤ìº”":
            st.info("QR ìŠ¤ìº” ê¸°ëŠ¥ì€ ëª¨ë°”ì¼ ì•±ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            
            # QR ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
            if st.button("QR ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜"):
                simulated_data = self._simulate_qr_data(design)
                st.session_state.experimental_data = simulated_data
                st.success("QR ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        elif input_method == "ì‹¤ì‹œê°„ ì—°ë™":
            st.markdown("#### ì‹¤ì‹œê°„ ë°ì´í„° ì—°ë™")
            
            equipment_options = st.session_state.project_info.get('equipment', [])
            selected_equipment = st.selectbox(
                "ì—°ë™í•  ì¥ë¹„",
                equipment_options,
                help="ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ë°›ì„ ì¥ë¹„ë¥¼ ì„ íƒí•˜ì„¸ìš”."
            )
            
            if st.button("ì—°ë™ ì‹œì‘"):
                st.info(f"{selected_equipment}ì™€ ì—°ë™ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                # ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜
                self._start_realtime_stream(selected_equipment)
        
        # ë°ì´í„° ì €ì¥
        if st.button("ğŸ’¾ ë°ì´í„° ì €ì¥", type="primary"):
            if hasattr(st.session_state, 'experimental_data'):
                # ë°ì´í„° ì €ì¥ ë¡œì§
                st.success("ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # ìë™ ë°±ì—…
                backup_path = self._create_data_backup(st.session_state.experimental_data)
                st.caption(f"ë°±ì—… ìƒì„±: {backup_path}")
    
    def _render_basic_analysis(self, user_level: UserLevel):
        """ê¸°ë³¸ ë¶„ì„ íƒ­"""
        st.markdown("### ê¸°ë³¸ í†µê³„ ë¶„ì„")
        
        if 'experimental_data' not in st.session_state:
            st.warning("ë¨¼ì € ì‹¤í—˜ ë°ì´í„°ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        data = st.session_state.experimental_data
        design = st.session_state.experiment_design
        responses = [r['name'] for r in design['responses']]
        
        # ë°˜ì‘ë³€ìˆ˜ ì„ íƒ
        selected_response = st.selectbox(
            "ë¶„ì„í•  ë°˜ì‘ë³€ìˆ˜",
            responses,
            help="ë¶„ì„í•˜ê³ ì í•˜ëŠ” ë°˜ì‘ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”."
        )
        
        if selected_response and selected_response in data.columns:
            response_data = data[selected_response].dropna()
            
            if len(response_data) > 0:
                # ê¸°ìˆ í†µê³„
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    st.markdown("#### ğŸ“Š ê¸°ìˆ í†µê³„")
                    
                    stats_data = {
                        "í†µê³„ëŸ‰": ["í‰ê· ", "í‘œì¤€í¸ì°¨", "ìµœì†Œê°’", "Q1", "ì¤‘ì•™ê°’", "Q3", "ìµœëŒ€ê°’", "CV(%)"],
                        "ê°’": [
                            f"{response_data.mean():.3f}",
                            f"{response_data.std():.3f}",
                            f"{response_data.min():.3f}",
                            f"{response_data.quantile(0.25):.3f}",
                            f"{response_data.median():.3f}",
                            f"{response_data.quantile(0.75):.3f}",
                            f"{response_data.max():.3f}",
                            f"{(response_data.std()/response_data.mean()*100):.1f}"
                        ]
                    }
                    
                    stats_df = pd.DataFrame(stats_data)
                    st.dataframe(stats_df, hide_index=True, use_container_width=True)
                
                with col2:
                    st.markdown("#### ğŸ“ˆ ë¶„í¬ ê·¸ë˜í”„")
                    
                    # íˆìŠ¤í† ê·¸ë¨ with ì •ê·œë¶„í¬
                    fig = go.Figure()
                    
                    # íˆìŠ¤í† ê·¸ë¨
                    fig.add_trace(go.Histogram(
                        x=response_data,
                        name='ì‹¤ì œ ë¶„í¬',
                        nbinsx=10,
                        opacity=0.7
                    ))
                    
                    # ì •ê·œë¶„í¬ ê³¡ì„ 
                    x_range = np.linspace(response_data.min(), response_data.max(), 100)
                    normal_dist = stats.norm(loc=response_data.mean(), scale=response_data.std())
                    
                    fig.add_trace(go.Scatter(
                        x=x_range,
                        y=normal_dist.pdf(x_range) * len(response_data) * (response_data.max() - response_data.min()) / 10,
                        mode='lines',
                        name='ì •ê·œë¶„í¬',
                        line=dict(color='red', width=2)
                    ))
                    
                    fig.update_layout(
                        title=f"{selected_response} ë¶„í¬",
                        xaxis_title=selected_response,
                        yaxis_title="ë¹ˆë„",
                        showlegend=True
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                # ì •ê·œì„± ê²€ì •
                st.markdown("#### ğŸ” ì •ê·œì„± ê²€ì •")
                
                # Shapiro-Wilk ê²€ì •
                stat, p_value = stats.shapiro(response_data)
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Shapiro-Wilk í†µê³„ëŸ‰", f"{stat:.4f}")
                
                with col2:
                    st.metric("p-value", f"{p_value:.4f}")
                
                with col3:
                    if p_value > 0.05:
                        st.success("ì •ê·œë¶„í¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤")
                    else:
                        st.warning("ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                
                # ì£¼íš¨ê³¼ ë¶„ì„
                st.markdown("#### ğŸ¯ ì£¼íš¨ê³¼ ë¶„ì„")
                
                factors = [f['name'] for f in design['factors']]
                
                # ê° ìš”ì¸ë³„ íš¨ê³¼ ê³„ì‚°
                effects = {}
                
                for factor in factors:
                    if factor in data.columns:
                        # ê³ ìˆ˜ì¤€ê³¼ ì €ìˆ˜ì¤€ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¦¬
                        factor_median = data[factor].median()
                        high_group = data[data[factor] > factor_median][selected_response].dropna()
                        low_group = data[data[factor] <= factor_median][selected_response].dropna()
                        
                        if len(high_group) > 0 and len(low_group) > 0:
                            effect = high_group.mean() - low_group.mean()
                            effects[factor] = effect
                
                if effects:
                    # íš¨ê³¼ ë§‰ëŒ€ ê·¸ë˜í”„
                    effect_df = pd.DataFrame(
                        list(effects.items()), 
                        columns=['ìš”ì¸', 'íš¨ê³¼']
                    )
                    effect_df = effect_df.sort_values('íš¨ê³¼', key=abs, ascending=False)
                    
                    fig = px.bar(
                        effect_df,
                        x='íš¨ê³¼',
                        y='ìš”ì¸',
                        orientation='h',
                        title='ìš”ì¸ë³„ ì£¼íš¨ê³¼',
                        color='íš¨ê³¼',
                        color_continuous_scale='RdBu_r',
                        color_continuous_midpoint=0
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # ìœ ì˜ì„± í…Œì´ë¸”
                    significance_data = []
                    
                    for factor in factors:
                        if factor in data.columns:
                            # t-test
                            factor_median = data[factor].median()
                            high_group = data[data[factor] > factor_median][selected_response].dropna()
                            low_group = data[data[factor] <= factor_median][selected_response].dropna()
                            
                            if len(high_group) > 1 and len(low_group) > 1:
                                t_stat, p_val = stats.ttest_ind(high_group, low_group)
                                
                                significance_data.append({
                                    'ìš”ì¸': factor,
                                    'íš¨ê³¼': effects.get(factor, 0),
                                    't-í†µê³„ëŸ‰': t_stat,
                                    'p-value': p_val,
                                    'ìœ ì˜ì„±': 'âœ“' if p_val < 0.05 else 'âœ—'
                                })
                    
                    if significance_data:
                        sig_df = pd.DataFrame(significance_data)
                        st.dataframe(
                            sig_df.style.format({
                                'íš¨ê³¼': '{:.3f}',
                                't-í†µê³„ëŸ‰': '{:.3f}',
                                'p-value': '{:.4f}'
                            }),
                            use_container_width=True
                        )
    
    def _render_advanced_analysis(self, user_level: UserLevel):
        """ê³ ê¸‰ ë¶„ì„ íƒ­"""
        st.markdown("### ê³ ê¸‰ í†µê³„ ë¶„ì„")
        
        if user_level == UserLevel.BEGINNER:
            st.info("""
            ğŸ’¡ **ì´ˆë³´ì ê°€ì´ë“œ**: ê³ ê¸‰ ë¶„ì„ì€ ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
            - ANOVA: ìš”ì¸ë“¤ì´ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
            - íšŒê·€ë¶„ì„: ìˆ˜í•™ì  ëª¨ë¸ êµ¬ì¶•
            - ìµœì í™”: ìµœìƒì˜ ì¡°ê±´ ì°¾ê¸°
            """)
        
        analysis_type = st.selectbox(
            "ë¶„ì„ ìœ í˜•",
            ["ANOVA", "íšŒê·€ë¶„ì„", "ë°˜ì‘í‘œë©´ë¶„ì„", "ìµœì í™”", "ê¸°ê³„í•™ìŠµ"]
        )
        
        if analysis_type == "ANOVA":
            self._render_anova_analysis()
        elif analysis_type == "íšŒê·€ë¶„ì„":
            self._render_regression_analysis()
        elif analysis_type == "ë°˜ì‘í‘œë©´ë¶„ì„":
            self._render_rsm_analysis()
        elif analysis_type == "ìµœì í™”":
            self._render_optimization()
        elif analysis_type == "ê¸°ê³„í•™ìŠµ":
            self._render_ml_analysis()

# Polymer-doe-platform - Part 13
# ==================== ê³ ê¸‰ ë¶„ì„ ë©”ì„œë“œ êµ¬í˜„ ====================
    def _render_anova_analysis(self):
        """ANOVA ë¶„ì„"""
        st.markdown("#### ğŸ“Š ë¶„ì‚°ë¶„ì„ (ANOVA)")
        
        if 'experimental_data' not in st.session_state:
            st.warning("ì‹¤í—˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        data = st.session_state.experimental_data
        design = st.session_state.experiment_design
        
        # ë°˜ì‘ë³€ìˆ˜ ì„ íƒ
        responses = [r['name'] for r in design['responses']]
        selected_response = st.selectbox(
            "ë¶„ì„í•  ë°˜ì‘ë³€ìˆ˜",
            responses,
            key="anova_response"
        )
        
        if selected_response:
            # ANOVA ìˆ˜í–‰
            factors = [f['name'] for f in design['factors']]
            
            # ëª¨ë¸ êµ¬ì„±
            model_formula = f"{selected_response} ~ " + " + ".join(factors)
            
            # ìƒí˜¸ì‘ìš© í¬í•¨ ì˜µì…˜
            include_interactions = st.checkbox("2ì°¨ ìƒí˜¸ì‘ìš© í¬í•¨", value=True)
            
            if include_interactions and len(factors) > 1:
                # 2ì°¨ ìƒí˜¸ì‘ìš© ì¶”ê°€
                import itertools
                interactions = [f"{f1}:{f2}" for f1, f2 in itertools.combinations(factors, 2)]
                model_formula += " + " + " + ".join(interactions)
            
            st.info(f"ëª¨ë¸: {model_formula}")
            
            try:
                # ANOVA í…Œì´ë¸” ìƒì„±
                import statsmodels.api as sm
                from statsmodels.formula.api import ols
                
                model = ols(model_formula, data=data).fit()
                anova_table = sm.stats.anova_lm(model, typ=2)
                
                # ANOVA í…Œì´ë¸” í‘œì‹œ
                st.dataframe(
                    anova_table.style.format({
                        'sum_sq': '{:.3f}',
                        'df': '{:.0f}',
                        'F': '{:.3f}',
                        'PR(>F)': '{:.4f}'
                    }).apply(
                        lambda x: ['background-color: yellow' if v < 0.05 else '' 
                                 for v in x], 
                        subset=['PR(>F)']
                    ),
                    use_container_width=True
                )
                
                # R-squared ê°’
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("RÂ²", f"{model.rsquared:.4f}")
                with col2:
                    st.metric("Adjusted RÂ²", f"{model.rsquared_adj:.4f}")
                with col3:
                    st.metric("F-statistic", f"{model.fvalue:.3f}")
                
                # ì”ì°¨ ë¶„ì„
                st.markdown("##### ì”ì°¨ ë¶„ì„")
                
                residuals = model.resid
                fitted = model.fittedvalues
                
                fig = make_subplots(
                    rows=2, cols=2,
                    subplot_titles=('ì”ì°¨ vs ì í•©ê°’', 'Q-Q Plot', 
                                  'ì”ì°¨ íˆìŠ¤í† ê·¸ë¨', 'Cook\'s Distance')
                )
                
                # ì”ì°¨ vs ì í•©ê°’
                fig.add_trace(
                    go.Scatter(x=fitted, y=residuals, mode='markers',
                             marker=dict(color='blue', size=8),
                             name='ì”ì°¨'),
                    row=1, col=1
                )
                fig.add_hline(y=0, line_dash="dash", line_color="red", row=1, col=1)
                
                # Q-Q Plot
                import scipy.stats as stats
                theoretical_quantiles = stats.norm.ppf(
                    (np.arange(len(residuals)) + 0.5) / len(residuals)
                )
                sorted_residuals = np.sort(residuals)
                
                fig.add_trace(
                    go.Scatter(x=theoretical_quantiles, y=sorted_residuals,
                             mode='markers', name='Q-Q'),
                    row=1, col=2
                )
                fig.add_trace(
                    go.Scatter(x=[-3, 3], y=[-3, 3], mode='lines',
                             line=dict(color='red', dash='dash'),
                             name='ì°¸ì¡°ì„ '),
                    row=1, col=2
                )
                
                # ì”ì°¨ íˆìŠ¤í† ê·¸ë¨
                fig.add_trace(
                    go.Histogram(x=residuals, nbinsx=20, name='ì”ì°¨'),
                    row=2, col=1
                )
                
                # Cook's Distance
                influence = model.get_influence()
                cooks_d = influence.cooks_distance[0]
                
                fig.add_trace(
                    go.Bar(y=cooks_d, name="Cook's D"),
                    row=2, col=2
                )
                fig.add_hline(y=4/len(data), line_dash="dash", 
                            line_color="red", row=2, col=2)
                
                fig.update_layout(height=800, showlegend=False)
                st.plotly_chart(fig, use_container_width=True)
                
            except Exception as e:
                st.error(f"ANOVA ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
    
    def _render_regression_analysis(self):
        """íšŒê·€ë¶„ì„"""
        st.markdown("#### ğŸ“ˆ íšŒê·€ë¶„ì„")
        
        data = st.session_state.experimental_data
        design = st.session_state.experiment_design
        
        # ëª¨ë¸ ìœ í˜• ì„ íƒ
        model_type = st.radio(
            "íšŒê·€ ëª¨ë¸ ìœ í˜•",
            ["ì„ í˜• íšŒê·€", "ë‹¤í•­ íšŒê·€", "Ridge íšŒê·€", "Lasso íšŒê·€"],
            horizontal=True
        )
        
        # ë°˜ì‘ë³€ìˆ˜ ì„ íƒ
        responses = [r['name'] for r in design['responses']]
        selected_response = st.selectbox(
            "ì¢…ì†ë³€ìˆ˜ (Y)",
            responses,
            key="reg_response"
        )
        
        # ë…ë¦½ë³€ìˆ˜ ì„ íƒ
        factors = [f['name'] for f in design['factors']]
        selected_factors = st.multiselect(
            "ë…ë¦½ë³€ìˆ˜ (X)",
            factors,
            default=factors[:3] if len(factors) >= 3 else factors
        )
        
        if selected_response and selected_factors:
            X = data[selected_factors]
            y = data[selected_response].dropna()
            
            # ê²°ì¸¡ì¹˜ ì œê±°
            valid_idx = y.index.intersection(X.dropna().index)
            X = X.loc[valid_idx]
            y = y.loc[valid_idx]
            
            if len(y) > len(selected_factors):
                if model_type == "ì„ í˜• íšŒê·€":
                    from sklearn.linear_model import LinearRegression
                    model = LinearRegression()
                    
                elif model_type == "ë‹¤í•­ íšŒê·€":
                    degree = st.slider("ë‹¤í•­ì‹ ì°¨ìˆ˜", 2, 4, 2)
                    from sklearn.preprocessing import PolynomialFeatures
                    from sklearn.linear_model import LinearRegression
                    
                    poly = PolynomialFeatures(degree=degree)
                    X_poly = poly.fit_transform(X)
                    model = LinearRegression()
                    X = X_poly
                    
                elif model_type == "Ridge íšŒê·€":
                    alpha = st.slider("ì •ê·œí™” ê°•ë„ (Î±)", 0.01, 10.0, 1.0)
                    from sklearn.linear_model import Ridge
                    model = Ridge(alpha=alpha)
                    
                elif model_type == "Lasso íšŒê·€":
                    alpha = st.slider("ì •ê·œí™” ê°•ë„ (Î±)", 0.01, 10.0, 1.0)
                    from sklearn.linear_model import Lasso
                    model = Lasso(alpha=alpha)
                
                # ëª¨ë¸ í•™ìŠµ
                model.fit(X, y)
                predictions = model.predict(X)
                
                # ëª¨ë¸ ì„±ëŠ¥
                from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("RÂ²", f"{r2_score(y, predictions):.4f}")
                with col2:
                    st.metric("RMSE", f"{np.sqrt(mean_squared_error(y, predictions)):.3f}")
                with col3:
                    st.metric("MAE", f"{mean_absolute_error(y, predictions):.3f}")
                with col4:
                    st.metric("MAPE", f"{np.mean(np.abs((y - predictions) / y)) * 100:.1f}%")
                
                # ê³„ìˆ˜ í‘œì‹œ
                if hasattr(model, 'coef_'):
                    st.markdown("##### íšŒê·€ ê³„ìˆ˜")
                    
                    if model_type == "ì„ í˜• íšŒê·€" or model_type in ["Ridge íšŒê·€", "Lasso íšŒê·€"]:
                        coef_df = pd.DataFrame({
                            'ë³€ìˆ˜': selected_factors,
                            'ê³„ìˆ˜': model.coef_,
                            'ì ˆëŒ€ê°’': np.abs(model.coef_)
                        }).sort_values('ì ˆëŒ€ê°’', ascending=False)
                        
                        # ê³„ìˆ˜ ë§‰ëŒ€ ê·¸ë˜í”„
                        fig = px.bar(
                            coef_df,
                            x='ê³„ìˆ˜',
                            y='ë³€ìˆ˜',
                            orientation='h',
                            color='ê³„ìˆ˜',
                            color_continuous_scale='RdBu_r',
                            color_continuous_midpoint=0,
                            title='íšŒê·€ ê³„ìˆ˜'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                
                # ì˜ˆì¸¡ vs ì‹¤ì œ ê·¸ë˜í”„
                fig = go.Figure()
                
                fig.add_trace(go.Scatter(
                    x=y,
                    y=predictions,
                    mode='markers',
                    name='ì˜ˆì¸¡ê°’',
                    marker=dict(color='blue', size=8)
                ))
                
                # ì™„ë²½í•œ ì˜ˆì¸¡ì„ 
                min_val = min(y.min(), predictions.min())
                max_val = max(y.max(), predictions.max())
                fig.add_trace(go.Scatter(
                    x=[min_val, max_val],
                    y=[min_val, max_val],
                    mode='lines',
                    name='ì™„ë²½í•œ ì˜ˆì¸¡',
                    line=dict(color='red', dash='dash')
                ))
                
                fig.update_layout(
                    title='ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’',
                    xaxis_title='ì‹¤ì œê°’',
                    yaxis_title='ì˜ˆì¸¡ê°’',
                    showlegend=True
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # íšŒê·€ì‹ í‘œì‹œ
                if model_type == "ì„ í˜• íšŒê·€":
                    equation = f"{selected_response} = {model.intercept_:.3f}"
                    for factor, coef in zip(selected_factors, model.coef_):
                        equation += f" + {coef:.3f} Ã— {factor}"
                    
                    st.success(f"íšŒê·€ì‹: {equation}")
    
    def _render_rsm_analysis(self):
        """ë°˜ì‘í‘œë©´ë¶„ì„"""
        st.markdown("#### ğŸ—ºï¸ ë°˜ì‘í‘œë©´ë¶„ì„ (RSM)")
        
        data = st.session_state.experimental_data
        design = st.session_state.experiment_design
        
        # ë¶„ì„ ì˜µì…˜
        col1, col2 = st.columns(2)
        
        with col1:
            response = st.selectbox(
                "ë°˜ì‘ë³€ìˆ˜",
                [r['name'] for r in design['responses']],
                key="rsm_response"
            )
        
        with col2:
            model_order = st.selectbox(
                "ëª¨ë¸ ì°¨ìˆ˜",
                ["1ì°¨ (ì„ í˜•)", "2ì°¨ (ì´ì°¨)"],
                index=1
            )
        
        if response:
            factors = [f['name'] for f in design['factors'] if not f.get('categorical', False)]
            
            if len(factors) >= 2:
                # 2ê°œ ìš”ì¸ ì„ íƒ (3D í‘œë©´ìš©)
                st.markdown("##### 3D ë°˜ì‘í‘œë©´ì„ ìœ„í•œ ìš”ì¸ ì„ íƒ (2ê°œ)")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    factor1 = st.selectbox("Xì¶• ìš”ì¸", factors)
                
                with col2:
                    factor2 = st.selectbox(
                        "Yì¶• ìš”ì¸", 
                        [f for f in factors if f != factor1]
                    )
                
                if factor1 and factor2:
                    # RSM ëª¨ë¸ êµ¬ì¶•
                    from sklearn.preprocessing import PolynomialFeatures
                    from sklearn.linear_model import LinearRegression
                    
                    # ë°ì´í„° ì¤€ë¹„
                    X = data[[factor1, factor2]].values
                    y = data[response].values
                    
                    # ê²°ì¸¡ì¹˜ ì œê±°
                    valid_mask = ~np.isnan(y)
                    X = X[valid_mask]
                    y = y[valid_mask]
                    
                    # ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„±
                    degree = 2 if "2ì°¨" in model_order else 1
                    poly = PolynomialFeatures(degree=degree)
                    X_poly = poly.fit_transform(X)
                    
                    # ëª¨ë¸ í•™ìŠµ
                    model = LinearRegression()
                    model.fit(X_poly, y)
                    
                    # 3D í‘œë©´ ìƒì„±
                    x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 50)
                    x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 50)
                    X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)
                    
                    X_grid = np.column_stack([X1_grid.ravel(), X2_grid.ravel()])
                    X_grid_poly = poly.transform(X_grid)
                    Z_grid = model.predict(X_grid_poly).reshape(X1_grid.shape)
                    
                    # 3D í‘œë©´ í”Œë¡¯
                    fig = go.Figure()
                    
                    # í‘œë©´
                    fig.add_trace(go.Surface(
                        x=x1_range,
                        y=x2_range,
                        z=Z_grid,
                        colorscale='Viridis',
                        name='ë°˜ì‘í‘œë©´'
                    ))
                    
                    # ì‹¤í—˜ì 
                    fig.add_trace(go.Scatter3d(
                        x=X[:, 0],
                        y=X[:, 1],
                        z=y,
                        mode='markers',
                        marker=dict(
                            size=8,
                            color='red',
                            symbol='circle'
                        ),
                        name='ì‹¤í—˜ ë°ì´í„°'
                    ))
                    
                    fig.update_layout(
                        title=f'{response} ë°˜ì‘í‘œë©´',
                        scene=dict(
                            xaxis_title=factor1,
                            yaxis_title=factor2,
                            zaxis_title=response
                        ),
                        height=600
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # ë“±ê³ ì„  í”Œë¡¯
                    st.markdown("##### ë“±ê³ ì„  í”Œë¡¯")
                    
                    fig2 = go.Figure()
                    
                    fig2.add_trace(go.Contour(
                        x=x1_range,
                        y=x2_range,
                        z=Z_grid,
                        colorscale='Viridis',
                        showscale=True,
                        contours=dict(
                            showlabels=True,
                            labelfont=dict(size=12, color='white')
                        )
                    ))
                    
                    # ì‹¤í—˜ì  ì¶”ê°€
                    fig2.add_trace(go.Scatter(
                        x=X[:, 0],
                        y=X[:, 1],
                        mode='markers',
                        marker=dict(
                            size=10,
                            color='red',
                            symbol='x'
                        ),
                        name='ì‹¤í—˜ì '
                    ))
                    
                    fig2.update_layout(
                        title=f'{response} ë“±ê³ ì„ ',
                        xaxis_title=factor1,
                        yaxis_title=factor2,
                        height=500
                    )
                    
                    st.plotly_chart(fig2, use_container_width=True)
                    
                    # ìµœì ì  ì°¾ê¸°
                    if st.button("ğŸ¯ ìµœì ì  ì°¾ê¸°"):
                        from scipy.optimize import minimize
                        
                        # ëª©ì í•¨ìˆ˜ (ìµœëŒ€í™”ë¥¼ ìœ„í•´ ìŒìˆ˜)
                        def objective(x):
                            x_poly = poly.transform(x.reshape(1, -1))
                            return -model.predict(x_poly)[0]
                        
                        # ì œì•½ì¡°ê±´
                        bounds = [(X[:, 0].min(), X[:, 0].max()),
                                (X[:, 1].min(), X[:, 1].max())]
                        
                        # ìµœì í™”
                        result = minimize(objective, X.mean(axis=0), 
                                        bounds=bounds, method='L-BFGS-B')
                        
                        if result.success:
                            optimal_point = result.x
                            optimal_value = -result.fun
                            
                            st.success(f"""
                            **ìµœì  ì¡°ê±´ ë°œê²¬!**
                            - {factor1}: {optimal_point[0]:.3f}
                            - {factor2}: {optimal_point[1]:.3f}
                            - ì˜ˆìƒ {response}: {optimal_value:.3f}
                            """)
                            
                            # ìµœì ì ì„ ê·¸ë˜í”„ì— ì¶”ê°€
                            fig2.add_trace(go.Scatter(
                                x=[optimal_point[0]],
                                y=[optimal_point[1]],
                                mode='markers',
                                marker=dict(
                                    size=20,
                                    color='yellow',
                                    symbol='star',
                                    line=dict(color='black', width=2)
                                ),
                                name='ìµœì ì '
                            ))
                            
                            st.plotly_chart(fig2, use_container_width=True)
    
    def _render_optimization(self):
        """ìµœì í™” ë¶„ì„"""
        st.markdown("#### ğŸ¯ ë‹¤ëª©ì  ìµœì í™”")
        
        data = st.session_state.experimental_data
        design = st.session_state.experiment_design
        
        # ìµœì í™” ëª©í‘œ ì„¤ì •
        st.markdown("##### ìµœì í™” ëª©í‘œ ì„¤ì •")
        
        responses = [r['name'] for r in design['responses']]
        optimization_goals = {}
        weights = {}
        
        for i, response in enumerate(responses):
            col1, col2, col3 = st.columns([3, 2, 2])
            
            with col1:
                include = st.checkbox(f"{response}", value=i==0, key=f"opt_inc_{i}")
            
            if include:
                with col2:
                    goal = st.selectbox(
                        "ëª©í‘œ",
                        ["ìµœëŒ€í™”", "ìµœì†Œí™”", "ëª©í‘œê°’"],
                        key=f"opt_goal_{i}"
                    )
                    optimization_goals[response] = goal
                
                with col3:
                    if goal == "ëª©í‘œê°’":
                        target = st.number_input(
                            "ëª©í‘œ",
                            key=f"opt_target_{i}"
                        )
                        optimization_goals[response] = ('target', target)
                    
                    weight = st.slider(
                        "ê°€ì¤‘ì¹˜",
                        0.0, 1.0, 0.5,
                        key=f"opt_weight_{i}"
                    )
                    weights[response] = weight
        
        if optimization_goals:
            # ìµœì í™” ë°©ë²• ì„ íƒ
            method = st.selectbox(
                "ìµœì í™” ë°©ë²•",
                ["Desirability Function", "Pareto Front", "Genetic Algorithm"]
            )
            
            if st.button("ğŸš€ ìµœì í™” ì‹¤í–‰", type="primary"):
                with st.spinner("ìµœì í™” ì¤‘..."):
                    if method == "Desirability Function":
                        optimal_conditions = self._optimize_desirability(
                            data, design, optimization_goals, weights
                        )
                    elif method == "Pareto Front":
                        optimal_conditions = self._optimize_pareto(
                            data, design, optimization_goals
                        )
                    else:  # Genetic Algorithm
                        optimal_conditions = self._optimize_genetic(
                            data, design, optimization_goals, weights
                        )
                    
                    # ê²°ê³¼ í‘œì‹œ
                    if optimal_conditions:
                        st.success("ìµœì  ì¡°ê±´ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                        
                        # ìµœì  ì¡°ê±´ í…Œì´ë¸”
                        st.markdown("##### ìµœì  ì¡°ê±´")
                        
                        conditions_df = pd.DataFrame([optimal_conditions['conditions']])
                        st.dataframe(conditions_df, use_container_width=True)
                        
                        # ì˜ˆìƒ ê²°ê³¼
                        st.markdown("##### ì˜ˆìƒ ê²°ê³¼")
                        
                        predictions_df = pd.DataFrame([optimal_conditions['predictions']])
                        st.dataframe(predictions_df, use_container_width=True)
                        
                        # ì‹ ë¢°êµ¬ê°„
                        if 'confidence_intervals' in optimal_conditions:
                            st.markdown("##### 95% ì‹ ë¢°êµ¬ê°„")
                            ci_df = pd.DataFrame(optimal_conditions['confidence_intervals'])
                            st.dataframe(ci_df, use_container_width=True)
    
    def _render_ml_analysis(self):
        """ê¸°ê³„í•™ìŠµ ë¶„ì„"""
        st.markdown("#### ğŸ¤– ê¸°ê³„í•™ìŠµ ëª¨ë¸ë§")
        
        ml_task = st.radio(
            "ì‘ì—… ìœ í˜•",
            ["ì˜ˆì¸¡ ëª¨ë¸", "íŠ¹ì„± ì¤‘ìš”ë„", "ì´ìƒì¹˜ íƒì§€", "í´ëŸ¬ìŠ¤í„°ë§"],
            horizontal=True
        )
        
        if ml_task == "ì˜ˆì¸¡ ëª¨ë¸":
            self._render_ml_prediction()
        elif ml_task == "íŠ¹ì„± ì¤‘ìš”ë„":
            self._render_feature_importance()
        elif ml_task == "ì´ìƒì¹˜ íƒì§€":
            self._render_anomaly_detection()
        else:  # í´ëŸ¬ìŠ¤í„°ë§
            self._render_clustering()
    
    def _render_ai_insights(self, user_level: UserLevel):
        """AI ì¸ì‚¬ì´íŠ¸ íƒ­"""
        st.markdown("### ğŸ¤– AI ì¸ì‚¬ì´íŠ¸")
        
        if 'experimental_data' not in st.session_state:
            st.warning("ì‹¤í—˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # AI ë¶„ì„ ìœ í˜•
        insight_type = st.selectbox(
            "ì¸ì‚¬ì´íŠ¸ ìœ í˜•",
            ["ì¢…í•© ë¶„ì„", "íŒ¨í„´ ë°œê²¬", "ì´ìƒ í˜„ìƒ", "ê°œì„  ì œì•ˆ", "ë‹¤ìŒ ì‹¤í—˜ ì¶”ì²œ"]
        )
        
        if st.button("ğŸ§  AI ë¶„ì„ ì‹¤í–‰", type="primary"):
            with st.spinner("AIê°€ ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                insights = asyncio.run(
                    self._generate_ai_insights(insight_type, user_level)
                )
                
                if insights:
                    # ì¸ì‚¬ì´íŠ¸ í‘œì‹œ
                    st.markdown("#### ğŸ’¡ AI ë¶„ì„ ê²°ê³¼")
                    
                    # ì£¼ìš” ë°œê²¬ì‚¬í•­
                    with st.expander("ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­", expanded=True):
                        st.markdown(insights.get('key_findings', ''))
                    
                    # ìƒì„¸ ë¶„ì„
                    with st.expander("ğŸ“Š ìƒì„¸ ë¶„ì„"):
                        st.markdown(insights.get('detailed_analysis', ''))
                    
                    # ì‹œê°í™”
                    if 'visualizations' in insights:
                        st.markdown("#### ğŸ“ˆ ì‹œê°í™”")
                        for viz in insights['visualizations']:
                            if viz['type'] == 'plotly':
                                st.plotly_chart(viz['figure'], use_container_width=True)
                            elif viz['type'] == 'dataframe':
                                st.dataframe(viz['data'], use_container_width=True)
                    
                    # ì¶”ì²œì‚¬í•­
                    if 'recommendations' in insights:
                        st.markdown("#### ğŸ’¡ ì¶”ì²œì‚¬í•­")
                        for i, rec in enumerate(insights['recommendations']):
                            st.info(f"{i+1}. {rec}")
    
    async def _generate_ai_insights(self, insight_type: str, user_level: UserLevel) -> Dict:
        """AI ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        if not hasattr(st.session_state, 'ai_orchestrator'):
            return None
        
        data = st.session_state.experimental_data
        design = st.session_state.experiment_design
        
        # ë°ì´í„° ìš”ì•½ ìƒì„±
        data_summary = self._create_data_summary(data, design)
        
        # ì¸ì‚¬ì´íŠ¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸
        prompts = {
            "ì¢…í•© ë¶„ì„": f"""
            ë‹¤ìŒ ê³ ë¶„ì ì‹¤í—˜ ë°ì´í„°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
            
            {data_summary}
            
            ë‹¤ìŒì„ í¬í•¨í•´ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ì „ë°˜ì ì¸ ì‹¤í—˜ í’ˆì§ˆ í‰ê°€
            2. ì£¼ìš” ìš”ì¸ì˜ ì˜í–¥ë ¥
            3. ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
            4. íŠ¹ì´ì‚¬í•­ ë° ì£¼ëª©í•  ì 
            """,
            
            "íŒ¨í„´ ë°œê²¬": f"""
            ë‹¤ìŒ ì‹¤í—˜ ë°ì´í„°ì—ì„œ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ì°¾ì•„ì£¼ì„¸ìš”:
            
            {data_summary}
            
            íŠ¹íˆ ë‹¤ìŒì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ìš”ì¸ ê°„ ìƒí˜¸ì‘ìš© íŒ¨í„´
            2. ë¹„ì„ í˜• ê´€ê³„
            3. ì˜ˆìƒì¹˜ ëª»í•œ ìƒê´€ê´€ê³„
            4. ì„ê³„ê°’ì´ë‚˜ ë³€ê³¡ì 
            """,
            
            "ì´ìƒ í˜„ìƒ": f"""
            ë‹¤ìŒ ì‹¤í—˜ ë°ì´í„°ì—ì„œ ì´ìƒ í˜„ìƒì„ íƒì§€í•˜ê³  ì„¤ëª…í•´ì£¼ì„¸ìš”:
            
            {data_summary}
            
            ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:
            1. í†µê³„ì  ì´ìƒì¹˜
            2. ë¬¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ê¸° ì–´ë ¤ìš´ ê²°ê³¼
            3. ì¬í˜„ì„± ë¬¸ì œ
            4. ê°€ëŠ¥í•œ ì›ì¸ ì¶”ì •
            """,
            
            "ê°œì„  ì œì•ˆ": f"""
            ë‹¤ìŒ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ë°©ì•ˆì„ ì œì•ˆí•´ì£¼ì„¸ìš”:
            
            {data_summary}
            
            ë‹¤ìŒì„ í¬í•¨í•´ì„œ ì œì•ˆí•´ì£¼ì„¸ìš”:
            1. ì‹¤í—˜ ì„¤ê³„ ê°œì„ ì 
            2. ì¶”ê°€ë¡œ íƒìƒ‰í•´ì•¼ í•  ì˜ì—­
            3. ì¸¡ì • ë°©ë²• ê°œì„ 
            4. ë¹„ìš©/ì‹œê°„ íš¨ìœ¨í™” ë°©ì•ˆ
            """,
            
            "ë‹¤ìŒ ì‹¤í—˜ ì¶”ì²œ": f"""
            í˜„ì¬ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì‹¤í—˜ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”:
            
            {data_summary}
            
            ë‹¤ìŒì„ ê³ ë ¤í•´ì„œ ì¶”ì²œí•´ì£¼ì„¸ìš”:
            1. í˜„ì¬ê¹Œì§€ì˜ ìµœì  ì¡°ê±´
            2. ë¯¸íƒìƒ‰ ì˜ì—­
            3. í™•ì¸ì´ í•„ìš”í•œ ê°€ì„¤
            4. êµ¬ì²´ì ì¸ ì‹¤í—˜ ì¡°ê±´ (5-10ê°œ)
            """
        }
        
        prompt = prompts.get(insight_type, prompts["ì¢…í•© ë¶„ì„"])
        
        # ë‹¤ì¤‘ AI ë¶„ì„
        response = await st.session_state.ai_orchestrator.query_multiple(
            prompt,
            strategy='consensus',
            engines=['gemini', 'deepseek', 'sambanova'],
            temperature=0.7,
            user_level=user_level
        )
        
        if response['status'] == 'success':
            # ì‘ë‹µ íŒŒì‹± ë° êµ¬ì¡°í™”
            insights = self._parse_ai_insights(response['response'], insight_type)
            
            # ì‹œê°í™” ì¶”ê°€
            insights['visualizations'] = self._create_insight_visualizations(
                data, design, insight_type
            )
            
            return insights
        
        return None
    
    def _create_data_summary(self, data: pd.DataFrame, design: Dict) -> str:
        """ë°ì´í„° ìš”ì•½ ìƒì„±"""
        summary = []
        
        # í”„ë¡œì íŠ¸ ì •ë³´
        project_info = st.session_state.get('project_info', {})
        summary.append(f"ê³ ë¶„ì: {project_info.get('polymer_type', 'N/A')}")
        summary.append(f"ëª©í‘œ íŠ¹ì„±: {', '.join(project_info.get('target_properties', []))}")
        
        # ì‹¤í—˜ ì„¤ê³„ ì •ë³´
        summary.append(f"\nì‹¤í—˜ ì„¤ê³„: {design.get('design_type', 'N/A')}")
        summary.append(f"ì´ ì‹¤í—˜ìˆ˜: {len(data)}")
        summary.append(f"ìš”ì¸: {', '.join([f['name'] for f in design['factors']])}")
        summary.append(f"ë°˜ì‘ë³€ìˆ˜: {', '.join([r['name'] for r in design['responses']])}")
        
        # ì£¼ìš” í†µê³„
        summary.append("\nì£¼ìš” ê²°ê³¼:")
        for response in design['responses']:
            if response['name'] in data.columns:
                values = data[response['name']].dropna()
                if len(values) > 0:
                    summary.append(
                        f"- {response['name']}: "
                        f"í‰ê· ={values.mean():.3f}, "
                        f"í‘œì¤€í¸ì°¨={values.std():.3f}, "
                        f"ìµœì†Œ={values.min():.3f}, "
                        f"ìµœëŒ€={values.max():.3f}"
                    )
        
        return "\n".join(summary)
    
    def _parse_ai_insights(self, response: str, insight_type: str) -> Dict:
        """AI ì‘ë‹µ íŒŒì‹±"""
        insights = {
            'key_findings': '',
            'detailed_analysis': '',
            'recommendations': []
        }
        
        # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
        sections = response.split('\n\n')
        
        if sections:
            insights['key_findings'] = sections[0]
            
        if len(sections) > 1:
            insights['detailed_analysis'] = '\n\n'.join(sections[1:-1])
            
        # ì¶”ì²œì‚¬í•­ ì¶”ì¶œ
        for section in sections:
            if 'ì¶”ì²œ' in section or 'recommend' in section.lower():
                lines = section.split('\n')
                for line in lines:
                    if line.strip() and (line.strip()[0].isdigit() or line.strip().startswith('-')):
                        insights['recommendations'].append(line.strip().lstrip('0123456789.- '))
        
        return insights

# ==================== í•™ìŠµ ì„¼í„° í˜ì´ì§€ ====================
class LearningCenterPage:
    """í•™ìŠµ ì„¼í„° í˜ì´ì§€"""
    
    def __init__(self):
        self.learning_modules = {
            'basics': {
                'title': 'ê¸°ì´ˆ ê°œë…',
                'topics': [
                    'ì‹¤í—˜ê³„íšë²•ì´ë€?',
                    'ìš”ì¸ê³¼ ë°˜ì‘ë³€ìˆ˜',
                    'ì£¼íš¨ê³¼ì™€ ìƒí˜¸ì‘ìš©',
                    'í†µê³„ì  ìœ ì˜ì„±'
                ]
            },
            'design_types': {
                'title': 'ì‹¤í—˜ ì„¤ê³„ ìœ í˜•',
                'topics': [
                    'ì™„ì „ìš”ì¸ì„¤ê³„',
                    'ë¶€ë¶„ìš”ì¸ì„¤ê³„',
                    'ë°˜ì‘í‘œë©´ì„¤ê³„',
                    'í˜¼í•©ë¬¼ì„¤ê³„'
                ]
            },
            'analysis': {
                'title': 'ë°ì´í„° ë¶„ì„',
                'topics': [
                    'ANOVA ì´í•´í•˜ê¸°',
                    'íšŒê·€ë¶„ì„ ê¸°ì´ˆ',
                    'ìµœì í™” ë°©ë²•',
                    'ê²°ê³¼ í•´ì„'
                ]
            },
            'polymer_specific': {
                'title': 'ê³ ë¶„ì íŠ¹í™”',
                'topics': [
                    'ê³ ë¶„ì íŠ¹ì„± í‰ê°€',
                    'ê°€ê³µ ì¡°ê±´ ìµœì í™”',
                    'êµ¬ì¡°-ë¬¼ì„± ê´€ê³„',
                    'í’ˆì§ˆ ê´€ë¦¬'
                ]
            }
        }
        
    def render(self, user_level: UserLevel):
        st.title("ğŸ“š í•™ìŠµ ì„¼í„°")
        
        # í•™ìŠµ ì§„ë„
        col1, col2, col3 = st.columns(3)
        
        with col1:
            progress = self._calculate_learning_progress()
            st.metric("í•™ìŠµ ì§„ë„", f"{progress}%")
        
        with col2:
            completed_modules = self._get_completed_modules()
            st.metric("ì™„ë£Œ ëª¨ë“ˆ", f"{completed_modules}/16")
        
        with col3:
            streak = self._get_learning_streak()
            st.metric("ì—°ì† í•™ìŠµ", f"{streak}ì¼")
        
        # ì¶”ì²œ í•™ìŠµ ê²½ë¡œ
        if user_level == UserLevel.BEGINNER:
            st.info("""
            ğŸ¯ **ì´ˆë³´ì ì¶”ì²œ í•™ìŠµ ê²½ë¡œ**
            1. ê¸°ì´ˆ ê°œë… â†’ 2. ì‹¤í—˜ ì„¤ê³„ ìœ í˜• â†’ 3. ë°ì´í„° ë¶„ì„ â†’ 4. ê³ ë¶„ì íŠ¹í™”
            
            ê° ëª¨ë“ˆì„ ìˆœì„œëŒ€ë¡œ í•™ìŠµí•˜ì‹œë©´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì— ë„ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
            """)
        
        # í•™ìŠµ ëª¨ë“ˆ íƒ­
        tabs = st.tabs(list(self.learning_modules.keys()))
        
        for i, (module_key, module_info) in enumerate(self.learning_modules.items()):
            with tabs[i]:
                st.markdown(f"### {module_info['title']}")
                
                for topic in module_info['topics']:
                    with st.expander(f"ğŸ“– {topic}"):
                        # í•™ìŠµ ì»¨í…ì¸  ë¡œë“œ
                        content = self._load_learning_content(module_key, topic, user_level)
                        st.markdown(content['text'])
                        
                        # ì¸í„°ë™í‹°ë¸Œ ìš”ì†Œ
                        if 'quiz' in content:
                            st.markdown("#### ğŸ¯ í™•ì¸ ë¬¸ì œ")
                            answer = st.radio(
                                content['quiz']['question'],
                                content['quiz']['options'],
                                key=f"quiz_{module_key}_{topic}"
                            )
                            
                            if st.button("ì •ë‹µ í™•ì¸", key=f"check_{module_key}_{topic}"):
                                if answer == content['quiz']['correct']:
                                    st.success("ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰")
                                else:
                                    st.error(f"í‹€ë ¸ìŠµë‹ˆë‹¤. ì •ë‹µì€ '{content['quiz']['correct']}'ì…ë‹ˆë‹¤.")
                        
                        # ì‹¤ìŠµ ì˜ˆì œ
                        if 'example' in content:
                            st.markdown("#### ğŸ’» ì‹¤ìŠµ ì˜ˆì œ")
                            st.code(content['example']['code'], language='python')
                            
                            if st.button("ì‹¤í–‰", key=f"run_{module_key}_{topic}"):
                                # ì˜ˆì œ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
                                st.pyplot(content['example']['result'])

# ==================== í•™ìŠµ ì»¨í…ì¸  ì‹œìŠ¤í…œ ====================
    def _load_learning_content(self, module: str, topic: str, user_level: UserLevel) -> Dict:
        """í•™ìŠµ ì»¨í…ì¸  ë¡œë“œ"""
        # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ íŒŒì¼ì—ì„œ ë¡œë“œ
        content_database = {
            'basics': {
                'ì‹¤í—˜ê³„íšë²•ì´ë€?': {
                    'text': """
                    ì‹¤í—˜ê³„íšë²•(Design of Experiments, DOE)ì€ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê³  
                    ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” í†µê³„ì  ë°©ë²•ì…ë‹ˆë‹¤.
                    
                    **ì£¼ìš” ì¥ì :**
                    - ìµœì†Œí•œì˜ ì‹¤í—˜ìœ¼ë¡œ ìµœëŒ€í•œì˜ ì •ë³´ íšë“
                    - ìš”ì¸ ê°„ ìƒí˜¸ì‘ìš© íŒŒì•… ê°€ëŠ¥
                    - í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê²°ë¡  ë„ì¶œ
                    - ìµœì  ì¡°ê±´ ì˜ˆì¸¡ ê°€ëŠ¥
                    
                    **ê³ ë¶„ì ì—°êµ¬ì—ì„œì˜ í™œìš©:**
                    - í•©ì„± ì¡°ê±´ ìµœì í™”
                    - ê°€ê³µ ì¡°ê±´ ì„¤ì •
                    - ë¬¼ì„± ê°œì„ 
                    - í’ˆì§ˆ ê´€ë¦¬
                    """,
                    'quiz': {
                        'question': "ì‹¤í—˜ê³„íšë²•ì˜ ê°€ì¥ í° ì¥ì ì€?",
                        'options': [
                            "ì‹¤í—˜ ë¹„ìš© ì ˆê°",
                            "ìµœì†Œ ì‹¤í—˜ìœ¼ë¡œ ìµœëŒ€ ì •ë³´ íšë“",
                            "ê°„ë‹¨í•œ ê³„ì‚°",
                            "ë¹ ë¥¸ ì‹¤í—˜"
                        ],
                        'correct': "ìµœì†Œ ì‹¤í—˜ìœ¼ë¡œ ìµœëŒ€ ì •ë³´ íšë“"
                    },
                    'example': {
                        'code': """
# 2^3 ì™„ì „ìš”ì¸ì„¤ê³„ ì˜ˆì œ
import numpy as np
import pandas as pd

# ìš”ì¸ ì„¤ì •
factors = {
    'ì˜¨ë„': [-1, 1],  # 150Â°C, 200Â°C
    'ì‹œê°„': [-1, 1],  # 30ë¶„, 60ë¶„
    'ì••ë ¥': [-1, 1]   # 1ê¸°ì••, 2ê¸°ì••
}

# ì„¤ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
from itertools import product
design = pd.DataFrame(
    list(product(*factors.values())),
    columns=factors.keys()
)
print(design)
                        """,
                        'result': None  # ì‹¤ì œë¡œëŠ” ê·¸ë˜í”„ë‚˜ ê²°ê³¼
                    }
                }
            }
        }
        
        # ê¸°ë³¸ ì»¨í…ì¸ 
        default_content = {
            'text': f"{topic}ì— ëŒ€í•œ í•™ìŠµ ìë£Œë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.",
            'quiz': None,
            'example': None
        }
        
        # ì‚¬ìš©ì ë ˆë²¨ì— ë”°ë¥¸ ì»¨í…ì¸  ì¡°ì •
        content = content_database.get(module, {}).get(topic, default_content)
        
        if user_level == UserLevel.BEGINNER:
            # ì´ˆë³´ììš© ì¶”ê°€ ì„¤ëª…
            content['text'] = "ğŸ’¡ **ì´ˆë³´ìë¥¼ ìœ„í•œ ì‰¬ìš´ ì„¤ëª…**\n\n" + content['text']
        elif user_level == UserLevel.EXPERT:
            # ì „ë¬¸ê°€ìš© ì‹¬í™” ë‚´ìš© ì¶”ê°€
            content['text'] += "\n\nğŸ“š **ì‹¬í™” í•™ìŠµ**\nê³ ê¸‰ í†µê³„ ì´ë¡ ê³¼ ìµœì‹  ì—°êµ¬ ë™í–¥..."
        
        return content
    
    def _calculate_learning_progress(self) -> int:
        """í•™ìŠµ ì§„ë„ ê³„ì‚°"""
        if 'learning_progress' not in st.session_state:
            st.session_state.learning_progress = {}
        
        total_topics = sum(len(module['topics']) for module in self.learning_modules.values())
        completed_topics = len(st.session_state.learning_progress)
        
        return int((completed_topics / total_topics) * 100)
    
    def _get_completed_modules(self) -> int:
        """ì™„ë£Œëœ ëª¨ë“ˆ ìˆ˜"""
        return len(st.session_state.get('completed_modules', []))
    
    def _get_learning_streak(self) -> int:
        """ì—°ì† í•™ìŠµ ì¼ìˆ˜"""
        return st.session_state.get('learning_streak', 0)

# Polymer-doe-platform - Part 14 (Final)
# ==================== ë³´ê³ ì„œ ìƒì„± ì‹œìŠ¤í…œ ====================
class ReportGenerator:
    """ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.template_engine = TemplateEngine()
        self.chart_generator = ChartGenerator()
        
    def generate_report(self, 
                       project_info: Dict,
                       design: Dict,
                       data: pd.DataFrame,
                       analysis_results: Dict,
                       format_type: str = "pdf") -> bytes:
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        
        # ë³´ê³ ì„œ êµ¬ì¡°
        report_sections = [
            self._create_title_page(project_info),
            self._create_executive_summary(project_info, analysis_results),
            self._create_introduction(project_info),
            self._create_experimental_section(design, data),
            self._create_results_section(data, analysis_results),
            self._create_discussion_section(analysis_results),
            self._create_conclusions(analysis_results),
            self._create_appendix(design, data)
        ]
        
        if format_type == "pdf":
            return self._generate_pdf(report_sections)
        elif format_type == "docx":
            return self._generate_docx(report_sections)
        elif format_type == "html":
            return self._generate_html(report_sections)
        else:
            return self._generate_markdown(report_sections)
    
    def _create_executive_summary(self, project_info: Dict, analysis_results: Dict) -> Dict:
        """ìš”ì•½ ì„¹ì…˜ ìƒì„±"""
        summary = {
            'title': 'ìš”ì•½',
            'content': f"""
## í”„ë¡œì íŠ¸ ê°œìš”
- **í”„ë¡œì íŠ¸ëª…**: {project_info.get('name')}
- **ê³ ë¶„ì**: {project_info.get('polymer_type')}
- **ëª©ì **: {project_info.get('objective')}

## ì£¼ìš” ë°œê²¬ì‚¬í•­
{self._summarize_key_findings(analysis_results)}

## ìµœì  ì¡°ê±´
{self._summarize_optimal_conditions(analysis_results)}

## ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
{self._summarize_recommendations(analysis_results)}
            """
        }
        
        return summary
    
    def _create_results_section(self, data: pd.DataFrame, analysis_results: Dict) -> Dict:
        """ê²°ê³¼ ì„¹ì…˜ ìƒì„±"""
        results = {
            'title': 'ì‹¤í—˜ ê²°ê³¼ ë° ë¶„ì„',
            'subsections': []
        }
        
        # ê¸°ìˆ í†µê³„
        results['subsections'].append({
            'title': 'ê¸°ìˆ í†µê³„',
            'content': self._format_descriptive_stats(data, analysis_results),
            'charts': self._create_descriptive_charts(data)
        })
        
        # ì£¼íš¨ê³¼ ë¶„ì„
        if 'effects' in analysis_results:
            results['subsections'].append({
                'title': 'ì£¼íš¨ê³¼ ë¶„ì„',
                'content': self._format_main_effects(analysis_results['effects']),
                'charts': self._create_effect_plots(analysis_results['effects'])
            })
        
        # ANOVA
        if 'anova' in analysis_results:
            results['subsections'].append({
                'title': 'ë¶„ì‚°ë¶„ì„ (ANOVA)',
                'content': self._format_anova_table(analysis_results['anova']),
                'charts': []
            })
        
        # íšŒê·€ë¶„ì„
        if 'regression' in analysis_results:
            results['subsections'].append({
                'title': 'íšŒê·€ë¶„ì„',
                'content': self._format_regression_results(analysis_results['regression']),
                'charts': self._create_regression_plots(analysis_results['regression'])
            })
        
        return results
    
    def _generate_pdf(self, sections: List[Dict]) -> bytes:
        """PDF ìƒì„±"""
        from reportlab.lib import colors
        from reportlab.lib.pagesizes import letter, A4
        from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image, PageBreak
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.units import inch
        
        # PDF ë²„í¼
        buffer = io.BytesIO()
        
        # ë¬¸ì„œ ìƒì„±
        doc = SimpleDocTemplate(
            buffer,
            pagesize=A4,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=18,
        )
        
        # ìŠ¤íƒ€ì¼
        styles = getSampleStyleSheet()
        styles.add(ParagraphStyle(
            name='KoreanStyle',
            fontName='Helvetica',
            fontSize=10,
            leading=12,
        ))
        
        # ìŠ¤í† ë¦¬ êµ¬ì„±
        story = []
        
        for section in sections:
            # ì œëª©
            story.append(Paragraph(section['title'], styles['Heading1']))
            story.append(Spacer(1, 12))
            
            # ë‚´ìš©
            if 'content' in section:
                # ë§ˆí¬ë‹¤ìš´ì„ ë‹¨ë½ìœ¼ë¡œ ë³€í™˜
                paragraphs = section['content'].split('\n\n')
                for para in paragraphs:
                    if para.strip():
                        story.append(Paragraph(para, styles['KoreanStyle']))
                        story.append(Spacer(1, 6))
            
            # í•˜ìœ„ ì„¹ì…˜
            if 'subsections' in section:
                for subsection in section['subsections']:
                    story.append(Paragraph(subsection['title'], styles['Heading2']))
                    story.append(Spacer(1, 6))
                    
                    if 'content' in subsection:
                        story.append(Paragraph(subsection['content'], styles['KoreanStyle']))
                        story.append(Spacer(1, 6))
                    
                    # ì°¨íŠ¸ ì¶”ê°€
                    if 'charts' in subsection:
                        for chart in subsection['charts']:
                            if isinstance(chart, bytes):
                                img = Image(io.BytesIO(chart), width=5*inch, height=3*inch)
                                story.append(img)
                                story.append(Spacer(1, 12))
            
            # í˜ì´ì§€ êµ¬ë¶„
            story.append(PageBreak())
        
        # PDF ë¹Œë“œ
        doc.build(story)
        
        # ë²„í¼ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        buffer.seek(0)
        return buffer.getvalue()
    
    def _generate_html(self, sections: List[Dict]) -> bytes:
        """HTML ë³´ê³ ì„œ ìƒì„±"""
        html_template = """
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ê³ ë¶„ì ì‹¤í—˜ ë³´ê³ ì„œ</title>
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
        .chart {
            margin: 20px 0;
            text-align: center;
        }
        .highlight {
            background-color: #fff3cd;
            padding: 10px;
            border-left: 4px solid #ffc107;
        }
    </style>
</head>
<body>
    {content}
</body>
</html>
        """
        
        content_parts = []
        
        for section in sections:
            section_html = f"<h1>{section['title']}</h1>\n"
            
            if 'content' in section:
                # ë§ˆí¬ë‹¤ìš´ì„ HTMLë¡œ ë³€í™˜
                import markdown
                section_html += markdown.markdown(section['content'])
            
            if 'subsections' in section:
                for subsection in section['subsections']:
                    section_html += f"<h2>{subsection['title']}</h2>\n"
                    
                    if 'content' in subsection:
                        section_html += markdown.markdown(subsection['content'])
                    
                    if 'charts' in subsection:
                        for i, chart in enumerate(subsection['charts']):
                            section_html += f'<div class="chart" id="chart_{i}"></div>\n'
            
            content_parts.append(section_html)
        
        html_content = html_template.format(content='\n'.join(content_parts))
        
        return html_content.encode('utf-8')

# ==================== ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ====================
class PolymerDOEApp:
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config = self._load_config()
        self.initialize_session_state()
        
    def _load_config(self) -> Dict:
        """ì„¤ì • ë¡œë“œ"""
        return {
            'app_name': 'ê³ ë¶„ì ì‹¤í—˜ ì„¤ê³„ í”Œë«í¼',
            'version': '4.0.0',
            'theme': {
                'primaryColor': '#1f77b4',
                'backgroundColor': '#ffffff',
                'secondaryBackgroundColor': '#f0f2f6',
                'textColor': '#262730'
            }
        }
    
    def initialize_session_state(self):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        if 'initialized' not in st.session_state:
            st.session_state.initialized = True
            st.session_state.current_page = 'home'
            st.session_state.user_level = UserLevel.BEGINNER
            st.session_state.project_info = None
            st.session_state.experiment_design = None
            st.session_state.experimental_data = None
            st.session_state.analysis_results = None
            st.session_state.learning_progress = {}
            st.session_state.recent_projects = []
        
            # AI ë° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
            with st.spinner("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
                # ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    loop.run_until_complete(self._initialize_systems())
                finally:
                    loop.close()
    
    async def _initialize_systems(self):
        """AI ë° ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
            st.session_state.ai_orchestrator = AIOrchestrator()
            await st.session_state.ai_orchestrator.initialize()
        
            # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
            st.session_state.db_manager = DatabaseIntegrationManager()
            await st.session_state.db_manager.initialize()
        
            # ì‹¤í—˜ ì„¤ê³„ ì—”ì§„ ì´ˆê¸°í™”
            st.session_state.design_engine = AdvancedExperimentDesignEngine(
                st.session_state.ai_orchestrator,
                st.session_state.db_manager
            )
        
            # í˜‘ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            st.session_state.collaboration_system = CollaborationSystem()
        
            # í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            st.session_state.learning_system = AILearningSystem()
            await st.session_state.learning_system.start_learning()
        
            st.success("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"ì´ˆê¸°í™” ì˜¤ë¥˜: {e}", exc_info=True)
    
    def run(self):
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰"""
        # í˜ì´ì§€ ì„¤ì •
        st.set_page_config(
            page_title=self.config['app_name'],
            page_icon="ğŸ§¬",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # CSS ìŠ¤íƒ€ì¼ ì ìš©
        self._apply_custom_css()
        
        # ì´ˆê¸°í™” ëŒ€ê¸°
        if not st.session_state.get('init_complete', False):
            with st.spinner("ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
                # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    loop.run_until_complete(self._initialize_systems())
                    st.session_state.init_complete = True
                except Exception as e:
                    st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                    return
                finally:
                    loop.close()
        
        # UI ì‹œìŠ¤í…œ ìƒì„± ë° ë Œë”ë§
        ui_system = UserInterfaceSystem()
        ui_system.render()
    
    def _apply_custom_css(self):
        """ì»¤ìŠ¤í…€ CSS ì ìš©"""
        st.markdown("""
        <style>
        /* ë©”ì¸ ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
        .main {
            padding-top: 1rem;
        }
        
        /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ */
        .css-1d391kg {
            padding-top: 1rem;
        }
        
        /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
        .stButton > button {
            width: 100%;
            border-radius: 5px;
            height: 3em;
            font-weight: bold;
        }
        
        /* ë©”íŠ¸ë¦­ ì¹´ë“œ ìŠ¤íƒ€ì¼ */
        [data-testid="metric-container"] {
            background-color: #f0f2f6;
            border-radius: 5px;
            padding: 10px;
            box-shadow: 2px 2px 5px rgba(0,0,0,0.1);
        }
        
        /* íƒ­ ìŠ¤íƒ€ì¼ */
        .stTabs [data-baseweb="tab-list"] {
            gap: 24px;
        }
        
        .stTabs [data-baseweb="tab"] {
            height: 50px;
            padding-left: 20px;
            padding-right: 20px;
            background-color: #f0f2f6;
            border-radius: 10px 10px 0 0;
        }
        
        /* ì„±ê³µ ë©”ì‹œì§€ ì• ë‹ˆë©”ì´ì…˜ */
        .element-container .stSuccess {
            animation: slideIn 0.5s ease-out;
        }
        
        @keyframes slideIn {
            from {
                transform: translateX(-100%);
                opacity: 0;
            }
            to {
                transform: translateX(0);
                opacity: 1;
            }
        }
        
        /* í”„ë¡œê·¸ë ˆìŠ¤ ë°” ìŠ¤íƒ€ì¼ */
        .stProgress > div > div > div > div {
            background-color: #1f77b4;
        }
        
        /* ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼ */
        .dataframe {
            font-size: 14px;
        }
        
        /* ì •ë³´ ë°•ìŠ¤ ìŠ¤íƒ€ì¼ */
        .stInfo {
            background-color: #e3f2fd;
            border-left: 4px solid #2196f3;
        }
        
        /* ë ˆë²¨ë³„ ìƒ‰ìƒ */
        .beginner-hint {
            background-color: #fff3cd;
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
        }
        
        .expert-section {
            background-color: #f3e5f5;
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
        }
        </style>
        """, unsafe_allow_html=True)

# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================
def format_dataframe_for_display(df: pd.DataFrame, 
                               precision: int = 3,
                               highlight_columns: List[str] = None) -> pd.DataFrame:
    """ë°ì´í„°í”„ë ˆì„ í‘œì‹œ í¬ë§·íŒ…"""
    styled_df = df.style.format(
        formatter={col: f"{{:.{precision}f}}" for col in df.select_dtypes(include=[np.number]).columns}
    )
    
    if highlight_columns:
        styled_df = styled_df.background_gradient(
            subset=highlight_columns,
            cmap='RdYlGn_r'
        )
    
    return styled_df

def create_download_link(data: Any, 
                        filename: str, 
                        file_format: str = 'csv') -> str:
    """ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±"""
    if file_format == 'csv':
        if isinstance(data, pd.DataFrame):
            data = data.to_csv(index=False, encoding='utf-8-sig')
        b64 = base64.b64encode(data.encode()).decode()
        mime = 'text/csv'
    elif file_format == 'json':
        if isinstance(data, dict):
            data = json.dumps(data, ensure_ascii=False, indent=2)
        b64 = base64.b64encode(data.encode()).decode()
        mime = 'application/json'
    elif file_format == 'xlsx':
        if isinstance(data, pd.DataFrame):
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                data.to_excel(writer, index=False)
            b64 = base64.b64encode(output.getvalue()).decode()
            mime = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    else:
        b64 = base64.b64encode(data).decode()
        mime = 'application/octet-stream'
    
    return f'<a href="data:{mime};base64,{b64}" download="{filename}">ë‹¤ìš´ë¡œë“œ</a>'

def validate_experiment_data(data: pd.DataFrame, 
                           design: Dict) -> Tuple[bool, List[str]]:
    """ì‹¤í—˜ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
    errors = []
    
    # í•„ìˆ˜ ì—´ í™•ì¸
    required_columns = [f['name'] for f in design['factors']]
    missing_columns = set(required_columns) - set(data.columns)
    
    if missing_columns:
        errors.append(f"ëˆ„ë½ëœ ìš”ì¸: {', '.join(missing_columns)}")
    
    # ë°ì´í„° íƒ€ì… í™•ì¸
    for factor in design['factors']:
        if factor['name'] in data.columns:
            if not factor.get('categorical', False):
                # ìˆ˜ì¹˜í˜• ë°ì´í„° í™•ì¸
                try:
                    pd.to_numeric(data[factor['name']])
                except:
                    errors.append(f"{factor['name']}ì€(ëŠ”) ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
    
    # ë²”ìœ„ í™•ì¸
    for factor in design['factors']:
        if factor['name'] in data.columns and not factor.get('categorical', False):
            values = pd.to_numeric(data[factor['name']], errors='coerce')
            min_val = factor.get('min_value', -np.inf)
            max_val = factor.get('max_value', np.inf)
            
            if values.min() < min_val or values.max() > max_val:
                errors.append(
                    f"{factor['name']} ë²”ìœ„ ë²—ì–´ë‚¨ "
                    f"(í—ˆìš©: {min_val}-{max_val})"
                )
    
    return len(errors) == 0, errors

def calculate_experiment_duration(design: Dict, 
                                experiment_time_per_run: float = 4.0) -> Dict[str, float]:
    """ì‹¤í—˜ ì†Œìš” ì‹œê°„ ê³„ì‚°"""
    n_experiments = len(design.get('matrix', []))
    
    # ìˆœì°¨ ì‹¤í–‰ ì‹œê°„
    sequential_time = n_experiments * experiment_time_per_run
    
    # ë³‘ë ¬ ì‹¤í–‰ ì‹œê°„ (ì¥ë¹„ ìˆ˜ì— ë”°ë¼)
    n_equipment = len(design.get('available_equipment', [1]))
    parallel_time = sequential_time / n_equipment
    
    # ì¤€ë¹„ ì‹œê°„ ì¶”ê°€
    setup_time = 8.0  # í•˜ë£¨
    
    return {
        'setup_time': setup_time,
        'sequential_time': sequential_time,
        'parallel_time': parallel_time,
        'total_sequential': setup_time + sequential_time,
        'total_parallel': setup_time + parallel_time,
        'time_saved': sequential_time - parallel_time
    }

# ==================== ì§„ì…ì  ====================
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    app = PolymerDOEApp()
    app.run()

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('polymer_doe.log'),
            logging.StreamHandler()
        ]
    )
    
    # ê²½ê³  í•„í„°
    warnings.filterwarnings('ignore')
    
    # ì•± ì‹¤í–‰
    main()

# ==================== END OF POLYMER DOE PLATFORM ====================
"""
ğŸ‰ ê³ ë¶„ì ì‹¤í—˜ ì„¤ê³„ í”Œë«í¼ v4.0.0 ì™„ì„±!

ì£¼ìš” ê¸°ëŠ¥:
1. ğŸ¤– 6ê°œ AI ì—”ì§„ í†µí•© (Gemini, Grok, SambaNova, DeepSeek, Groq, HuggingFace)
2. ğŸ—„ï¸ 9ê°œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
3. ğŸ“Š ê³ ê¸‰ í†µê³„ ë¶„ì„ ë° ê¸°ê³„í•™ìŠµ
4. ğŸ‘¥ ì‹¤ì‹œê°„ í˜‘ì—… ì‹œìŠ¤í…œ
5. ğŸ“š ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ
6. ğŸ¯ ë‹¤ëª©ì  ìµœì í™”
7. ğŸ“‘ ìë™ ë³´ê³ ì„œ ìƒì„±
8. ğŸ”¬ ê³ ë¶„ì íŠ¹í™” ê¸°ëŠ¥

ì´ ì½”ë“œ ë¼ì¸: ì•½ 14,000ì¤„
íŒŒì¼ êµ¬ì¡°:
- polymer-doe-platform.py (ë©”ì¸ íŒŒì¼)
- 14ê°œ íŒŒíŠ¸ë¡œ êµ¬ì„±ëœ ëª¨ë“ˆí˜• ì„¤ê³„

ê°œë°œíŒ€ì—ê²Œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤! ğŸ™
"""
